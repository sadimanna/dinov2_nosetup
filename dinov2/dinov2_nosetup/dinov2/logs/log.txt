I20240906 12:57:17 12012 dinov2 config.py:59] git:
  sha: d43546fae6805ceb22618af8cf78469b61d913c7, status: has uncommitted changes, branch: main

I20240906 12:57:17 12012 dinov2 config.py:60] config_file: configs/train/vitl16_short.yaml
eval: 
eval_only: False
no_resume: False
opts: ['train.dataset_path=ImageNet:split=TRAIN:root=C:\\Users\\ISI_UTS\\Siladittya\\iclr2023\\inexpts\\datasets\\ILSVRC\\Data\\CLS-LOC:extra=C:\\Users\\ISI_UTS\\Siladittya\\iclr2023\\inexpts\\datasets\\ILSVRC\\Data\\CLS-LOC', 'train.output_dir=C:\\Users\\ISI_UTS\\Siladittya\\iclr2023\\inexpts\\dinov2_nosetup\\dinov2\\dinov2_nosetup\\dinov2']
output_dir: C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\dinov2_nosetup\dinov2\dinov2_nosetup\dinov2
I20240906 12:57:17 12012 dinov2 config.py:26] sqrt scaling learning rate; base: 0.004, new: 0.001
I20240906 12:57:17 12012 dinov2 config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
dino:
  loss_weight: 1.0
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
  koleo_loss_weight: 0.1
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  separate_head: false
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
train:
  batch_size_per_gpu: 64
  dataset_path: ImageNet:split=TRAIN:root=C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\datasets\ILSVRC\Data\CLS-LOC:extra=C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\datasets\ILSVRC\Data\CLS-LOC
  output_dir: C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\dinov2_nosetup\dinov2\dinov2_nosetup\dinov2
  saveckp_freq: 20
  seed: 0
  num_workers: 10
  OFFICIAL_EPOCH_LENGTH: 1250
  cache_dataset: true
  centering: centering
student:
  arch: vit_large
  patch_size: 16
  drop_path_rate: 0.3
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: mlp
  block_chunks: 4
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
  num_register_tokens: 0
  interpolate_antialias: false
  interpolate_offset: 0.1
teacher:
  momentum_teacher: 0.992
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 30
optim:
  epochs: 100
  weight_decay: 0.04
  weight_decay_end: 0.4
  base_lr: 0.004
  lr: 0.001
  warmup_epochs: 10
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 0.9
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 8
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 224
  local_crops_size: 96
evaluation:
  eval_period_iterations: 12500

I20240906 12:57:17 12012 dinov2 image_net.py:194] loading labels from "labels.txt"
I20240906 12:57:35 12012 dinov2 image_net.py:240] creating entries: 0%
I20240906 12:57:35 12012 dinov2 image_net.py:240] creating entries: 1%
I20240906 12:57:35 12012 dinov2 image_net.py:240] creating entries: 2%
I20240906 12:57:36 12012 dinov2 image_net.py:240] creating entries: 3%
I20240906 12:57:36 12012 dinov2 image_net.py:240] creating entries: 4%
I20240906 12:57:37 12012 dinov2 image_net.py:240] creating entries: 5%
I20240906 12:57:37 12012 dinov2 image_net.py:240] creating entries: 6%
I20240906 12:57:37 12012 dinov2 image_net.py:240] creating entries: 7%
I20240906 12:57:38 12012 dinov2 image_net.py:240] creating entries: 8%
I20240906 12:57:38 12012 dinov2 image_net.py:240] creating entries: 9%
I20240906 12:57:39 12012 dinov2 image_net.py:240] creating entries: 10%
I20240906 12:57:39 12012 dinov2 image_net.py:240] creating entries: 11%
I20240906 12:57:39 12012 dinov2 image_net.py:240] creating entries: 12%
I20240906 12:57:40 12012 dinov2 image_net.py:240] creating entries: 13%
I20240906 12:57:40 12012 dinov2 image_net.py:240] creating entries: 14%
I20240906 12:57:40 12012 dinov2 image_net.py:240] creating entries: 15%
I20240906 12:57:41 12012 dinov2 image_net.py:240] creating entries: 16%
I20240906 12:57:41 12012 dinov2 image_net.py:240] creating entries: 17%
I20240906 12:57:42 12012 dinov2 image_net.py:240] creating entries: 18%
I20240906 12:57:42 12012 dinov2 image_net.py:240] creating entries: 19%
I20240906 12:57:42 12012 dinov2 image_net.py:240] creating entries: 20%
I20240906 12:57:43 12012 dinov2 image_net.py:240] creating entries: 21%
I20240906 12:57:43 12012 dinov2 image_net.py:240] creating entries: 22%
I20240906 12:57:43 12012 dinov2 image_net.py:240] creating entries: 23%
I20240906 12:57:44 12012 dinov2 image_net.py:240] creating entries: 24%
I20240906 12:57:44 12012 dinov2 image_net.py:240] creating entries: 25%
I20240906 12:57:45 12012 dinov2 image_net.py:240] creating entries: 26%
I20240906 12:57:45 12012 dinov2 image_net.py:240] creating entries: 27%
I20240906 12:57:45 12012 dinov2 image_net.py:240] creating entries: 28%
I20240906 12:57:46 12012 dinov2 image_net.py:240] creating entries: 29%
I20240906 12:57:46 12012 dinov2 image_net.py:240] creating entries: 30%
I20240906 12:57:46 12012 dinov2 image_net.py:240] creating entries: 31%
I20240906 12:57:47 12012 dinov2 image_net.py:240] creating entries: 32%
I20240906 12:57:47 12012 dinov2 image_net.py:240] creating entries: 33%
I20240906 12:57:48 12012 dinov2 image_net.py:240] creating entries: 34%
I20240906 12:57:48 12012 dinov2 image_net.py:240] creating entries: 35%
I20240906 12:57:48 12012 dinov2 image_net.py:240] creating entries: 36%
I20240906 12:57:49 12012 dinov2 image_net.py:240] creating entries: 37%
I20240906 12:57:49 12012 dinov2 image_net.py:240] creating entries: 38%
I20240906 12:57:49 12012 dinov2 image_net.py:240] creating entries: 39%
I20240906 12:57:50 12012 dinov2 image_net.py:240] creating entries: 40%
I20240906 12:57:50 12012 dinov2 image_net.py:240] creating entries: 41%
I20240906 12:57:51 12012 dinov2 image_net.py:240] creating entries: 42%
I20240906 12:57:51 12012 dinov2 image_net.py:240] creating entries: 43%
I20240906 12:57:51 12012 dinov2 image_net.py:240] creating entries: 44%
I20240906 12:57:52 12012 dinov2 image_net.py:240] creating entries: 45%
I20240906 12:57:52 12012 dinov2 image_net.py:240] creating entries: 46%
I20240906 12:57:52 12012 dinov2 image_net.py:240] creating entries: 47%
I20240906 12:57:53 12012 dinov2 image_net.py:240] creating entries: 48%
I20240906 12:57:53 12012 dinov2 image_net.py:240] creating entries: 49%
I20240906 12:57:54 12012 dinov2 image_net.py:240] creating entries: 50%
I20240906 12:57:54 12012 dinov2 image_net.py:240] creating entries: 51%
I20240906 12:57:54 12012 dinov2 image_net.py:240] creating entries: 52%
I20240906 12:57:55 12012 dinov2 image_net.py:240] creating entries: 53%
I20240906 12:57:55 12012 dinov2 image_net.py:240] creating entries: 54%
I20240906 12:57:55 12012 dinov2 image_net.py:240] creating entries: 55%
I20240906 12:57:56 12012 dinov2 image_net.py:240] creating entries: 56%
I20240906 12:57:56 12012 dinov2 image_net.py:240] creating entries: 57%
I20240906 12:57:57 12012 dinov2 image_net.py:240] creating entries: 58%
I20240906 12:57:57 12012 dinov2 image_net.py:240] creating entries: 59%
I20240906 12:57:57 12012 dinov2 image_net.py:240] creating entries: 60%
I20240906 12:57:58 12012 dinov2 image_net.py:240] creating entries: 61%
I20240906 12:57:58 12012 dinov2 image_net.py:240] creating entries: 62%
I20240906 12:57:58 12012 dinov2 image_net.py:240] creating entries: 63%
I20240906 12:57:59 12012 dinov2 image_net.py:240] creating entries: 64%
I20240906 12:57:59 12012 dinov2 image_net.py:240] creating entries: 65%
I20240906 12:58:00 12012 dinov2 image_net.py:240] creating entries: 66%
I20240906 12:58:00 12012 dinov2 image_net.py:240] creating entries: 67%
I20240906 12:58:00 12012 dinov2 image_net.py:240] creating entries: 68%
I20240906 12:58:01 12012 dinov2 image_net.py:240] creating entries: 69%
I20240906 12:58:01 12012 dinov2 image_net.py:240] creating entries: 70%
I20240906 12:58:01 12012 dinov2 image_net.py:240] creating entries: 71%
I20240906 12:58:02 12012 dinov2 image_net.py:240] creating entries: 72%
I20240906 12:58:02 12012 dinov2 image_net.py:240] creating entries: 73%
I20240906 12:58:03 12012 dinov2 image_net.py:240] creating entries: 74%
I20240906 12:58:03 12012 dinov2 image_net.py:240] creating entries: 75%
I20240906 12:58:03 12012 dinov2 image_net.py:240] creating entries: 76%
I20240906 12:58:04 12012 dinov2 image_net.py:240] creating entries: 77%
I20240906 12:58:04 12012 dinov2 image_net.py:240] creating entries: 78%
I20240906 12:58:04 12012 dinov2 image_net.py:240] creating entries: 79%
I20240906 12:58:05 12012 dinov2 image_net.py:240] creating entries: 80%
I20240906 12:58:05 12012 dinov2 image_net.py:240] creating entries: 81%
I20240906 12:58:06 12012 dinov2 image_net.py:240] creating entries: 82%
I20240906 12:58:06 12012 dinov2 image_net.py:240] creating entries: 83%
I20240906 12:58:06 12012 dinov2 image_net.py:240] creating entries: 84%
I20240906 12:58:07 12012 dinov2 image_net.py:240] creating entries: 85%
I20240906 12:58:07 12012 dinov2 image_net.py:240] creating entries: 86%
I20240906 12:58:07 12012 dinov2 image_net.py:240] creating entries: 87%
I20240906 12:58:08 12012 dinov2 image_net.py:240] creating entries: 88%
I20240906 12:58:08 12012 dinov2 image_net.py:240] creating entries: 89%
I20240906 12:58:09 12012 dinov2 image_net.py:240] creating entries: 90%
I20240906 12:58:09 12012 dinov2 image_net.py:240] creating entries: 91%
I20240906 12:58:09 12012 dinov2 image_net.py:240] creating entries: 92%
I20240906 12:58:10 12012 dinov2 image_net.py:240] creating entries: 93%
I20240906 12:58:10 12012 dinov2 image_net.py:240] creating entries: 94%
I20240906 12:58:10 12012 dinov2 image_net.py:240] creating entries: 95%
I20240906 12:58:11 12012 dinov2 image_net.py:240] creating entries: 96%
I20240906 12:58:11 12012 dinov2 image_net.py:240] creating entries: 97%
I20240906 12:58:12 12012 dinov2 image_net.py:240] creating entries: 98%
I20240906 12:58:12 12012 dinov2 image_net.py:240] creating entries: 99%
I20240906 12:58:12 12012 dinov2 image_net.py:240] creating entries: 100%
I20240906 12:58:12 12012 dinov2 image_net.py:249] saving entries to "entries-TRAIN.npy"
I20240906 12:58:18 12012 dinov2 image_net.py:282] saving class IDs to "class-ids-TRAIN.npy"
I20240906 12:58:18 12012 dinov2 image_net.py:285] saving class names to "class-names-TRAIN.npy"
I20240906 12:58:18 12012 dinov2 image_net.py:194] loading labels from "labels.txt"
I20240906 12:58:19 12012 dinov2 image_net.py:240] creating entries: 0%
I20240906 12:58:19 12012 dinov2 image_net.py:240] creating entries: 1%
I20240906 12:58:19 12012 dinov2 image_net.py:240] creating entries: 2%
I20240906 12:58:19 12012 dinov2 image_net.py:240] creating entries: 3%
I20240906 12:58:19 12012 dinov2 image_net.py:240] creating entries: 4%
I20240906 12:58:19 12012 dinov2 image_net.py:240] creating entries: 5%
I20240906 12:58:19 12012 dinov2 image_net.py:240] creating entries: 6%
I20240906 12:58:19 12012 dinov2 image_net.py:240] creating entries: 7%
I20240906 12:58:19 12012 dinov2 image_net.py:240] creating entries: 8%
I20240906 12:58:19 12012 dinov2 image_net.py:240] creating entries: 9%
I20240906 12:58:19 12012 dinov2 image_net.py:240] creating entries: 10%
I20240906 12:58:19 12012 dinov2 image_net.py:240] creating entries: 11%
I20240906 12:58:19 12012 dinov2 image_net.py:240] creating entries: 12%
I20240906 12:58:19 12012 dinov2 image_net.py:240] creating entries: 13%
I20240906 12:58:19 12012 dinov2 image_net.py:240] creating entries: 14%
I20240906 12:58:19 12012 dinov2 image_net.py:240] creating entries: 15%
I20240906 12:58:19 12012 dinov2 image_net.py:240] creating entries: 16%
I20240906 12:58:19 12012 dinov2 image_net.py:240] creating entries: 17%
I20240906 12:58:19 12012 dinov2 image_net.py:240] creating entries: 18%
I20240906 12:58:19 12012 dinov2 image_net.py:240] creating entries: 19%
I20240906 12:58:19 12012 dinov2 image_net.py:240] creating entries: 20%
I20240906 12:58:19 12012 dinov2 image_net.py:240] creating entries: 21%
I20240906 12:58:19 12012 dinov2 image_net.py:240] creating entries: 22%
I20240906 12:58:19 12012 dinov2 image_net.py:240] creating entries: 23%
I20240906 12:58:19 12012 dinov2 image_net.py:240] creating entries: 24%
I20240906 12:58:19 12012 dinov2 image_net.py:240] creating entries: 25%
I20240906 12:58:19 12012 dinov2 image_net.py:240] creating entries: 26%
I20240906 12:58:19 12012 dinov2 image_net.py:240] creating entries: 27%
I20240906 12:58:19 12012 dinov2 image_net.py:240] creating entries: 28%
I20240906 12:58:19 12012 dinov2 image_net.py:240] creating entries: 29%
I20240906 12:58:19 12012 dinov2 image_net.py:240] creating entries: 30%
I20240906 12:58:19 12012 dinov2 image_net.py:240] creating entries: 31%
I20240906 12:58:19 12012 dinov2 image_net.py:240] creating entries: 32%
I20240906 12:58:19 12012 dinov2 image_net.py:240] creating entries: 33%
I20240906 12:58:19 12012 dinov2 image_net.py:240] creating entries: 34%
I20240906 12:58:19 12012 dinov2 image_net.py:240] creating entries: 35%
I20240906 12:58:19 12012 dinov2 image_net.py:240] creating entries: 36%
I20240906 12:58:19 12012 dinov2 image_net.py:240] creating entries: 37%
I20240906 12:58:19 12012 dinov2 image_net.py:240] creating entries: 38%
I20240906 12:58:19 12012 dinov2 image_net.py:240] creating entries: 39%
I20240906 12:58:19 12012 dinov2 image_net.py:240] creating entries: 40%
I20240906 12:58:19 12012 dinov2 image_net.py:240] creating entries: 41%
I20240906 12:58:19 12012 dinov2 image_net.py:240] creating entries: 42%
I20240906 12:58:19 12012 dinov2 image_net.py:240] creating entries: 43%
I20240906 12:58:19 12012 dinov2 image_net.py:240] creating entries: 44%
I20240906 12:58:19 12012 dinov2 image_net.py:240] creating entries: 45%
I20240906 12:58:19 12012 dinov2 image_net.py:240] creating entries: 46%
I20240906 12:58:19 12012 dinov2 image_net.py:240] creating entries: 47%
I20240906 12:58:19 12012 dinov2 image_net.py:240] creating entries: 48%
I20240906 12:58:19 12012 dinov2 image_net.py:240] creating entries: 49%
I20240906 12:58:19 12012 dinov2 image_net.py:240] creating entries: 50%
I20240906 12:58:19 12012 dinov2 image_net.py:240] creating entries: 51%
I20240906 12:58:20 12012 dinov2 image_net.py:240] creating entries: 52%
I20240906 12:58:20 12012 dinov2 image_net.py:240] creating entries: 53%
I20240906 12:58:20 12012 dinov2 image_net.py:240] creating entries: 54%
I20240906 12:58:20 12012 dinov2 image_net.py:240] creating entries: 55%
I20240906 12:58:20 12012 dinov2 image_net.py:240] creating entries: 56%
I20240906 12:58:20 12012 dinov2 image_net.py:240] creating entries: 57%
I20240906 12:58:20 12012 dinov2 image_net.py:240] creating entries: 58%
I20240906 12:58:20 12012 dinov2 image_net.py:240] creating entries: 59%
I20240906 12:58:20 12012 dinov2 image_net.py:240] creating entries: 60%
I20240906 12:58:20 12012 dinov2 image_net.py:240] creating entries: 61%
I20240906 12:58:20 12012 dinov2 image_net.py:240] creating entries: 62%
I20240906 12:58:20 12012 dinov2 image_net.py:240] creating entries: 63%
I20240906 12:58:20 12012 dinov2 image_net.py:240] creating entries: 64%
I20240906 12:58:20 12012 dinov2 image_net.py:240] creating entries: 65%
I20240906 12:58:20 12012 dinov2 image_net.py:240] creating entries: 66%
I20240906 12:58:20 12012 dinov2 image_net.py:240] creating entries: 67%
I20240906 12:58:20 12012 dinov2 image_net.py:240] creating entries: 68%
I20240906 12:58:20 12012 dinov2 image_net.py:240] creating entries: 69%
I20240906 12:58:20 12012 dinov2 image_net.py:240] creating entries: 70%
I20240906 12:58:20 12012 dinov2 image_net.py:240] creating entries: 71%
I20240906 12:58:20 12012 dinov2 image_net.py:240] creating entries: 72%
I20240906 12:58:20 12012 dinov2 image_net.py:240] creating entries: 73%
I20240906 12:58:20 12012 dinov2 image_net.py:240] creating entries: 74%
I20240906 12:58:20 12012 dinov2 image_net.py:240] creating entries: 75%
I20240906 12:58:20 12012 dinov2 image_net.py:240] creating entries: 76%
I20240906 12:58:20 12012 dinov2 image_net.py:240] creating entries: 77%
I20240906 12:58:20 12012 dinov2 image_net.py:240] creating entries: 78%
I20240906 12:58:20 12012 dinov2 image_net.py:240] creating entries: 79%
I20240906 12:58:20 12012 dinov2 image_net.py:240] creating entries: 80%
I20240906 12:58:20 12012 dinov2 image_net.py:240] creating entries: 81%
I20240906 12:58:20 12012 dinov2 image_net.py:240] creating entries: 82%
I20240906 12:58:20 12012 dinov2 image_net.py:240] creating entries: 83%
I20240906 12:58:20 12012 dinov2 image_net.py:240] creating entries: 84%
I20240906 12:58:20 12012 dinov2 image_net.py:240] creating entries: 85%
I20240906 12:58:20 12012 dinov2 image_net.py:240] creating entries: 86%
I20240906 12:58:20 12012 dinov2 image_net.py:240] creating entries: 87%
I20240906 12:58:20 12012 dinov2 image_net.py:240] creating entries: 88%
I20240906 12:58:20 12012 dinov2 image_net.py:240] creating entries: 89%
I20240906 12:58:20 12012 dinov2 image_net.py:240] creating entries: 90%
I20240906 12:58:20 12012 dinov2 image_net.py:240] creating entries: 91%
I20240906 12:58:20 12012 dinov2 image_net.py:240] creating entries: 92%
I20240906 12:58:20 12012 dinov2 image_net.py:240] creating entries: 93%
I20240906 12:58:20 12012 dinov2 image_net.py:240] creating entries: 94%
I20240906 12:58:20 12012 dinov2 image_net.py:240] creating entries: 95%
I20240906 12:58:20 12012 dinov2 image_net.py:240] creating entries: 96%
I20240906 12:58:20 12012 dinov2 image_net.py:240] creating entries: 97%
I20240906 12:58:20 12012 dinov2 image_net.py:240] creating entries: 98%
I20240906 12:58:20 12012 dinov2 image_net.py:240] creating entries: 99%
I20240906 12:58:20 12012 dinov2 image_net.py:240] creating entries: 100%
I20240906 12:58:20 12012 dinov2 image_net.py:249] saving entries to "entries-VAL.npy"
I20240906 12:58:21 12012 dinov2 image_net.py:282] saving class IDs to "class-ids-VAL.npy"
I20240906 12:58:21 12012 dinov2 image_net.py:285] saving class names to "class-names-VAL.npy"
I20240906 12:58:21 12012 dinov2 image_net.py:225] creating entries: 0%
I20240906 12:58:21 12012 dinov2 image_net.py:225] creating entries: 1%
I20240906 12:58:21 12012 dinov2 image_net.py:225] creating entries: 2%
I20240906 12:58:21 12012 dinov2 image_net.py:225] creating entries: 3%
I20240906 12:58:21 12012 dinov2 image_net.py:225] creating entries: 4%
I20240906 12:58:21 12012 dinov2 image_net.py:225] creating entries: 5%
I20240906 12:58:21 12012 dinov2 image_net.py:225] creating entries: 6%
I20240906 12:58:21 12012 dinov2 image_net.py:225] creating entries: 7%
I20240906 12:58:21 12012 dinov2 image_net.py:225] creating entries: 8%
I20240906 12:58:21 12012 dinov2 image_net.py:225] creating entries: 9%
I20240906 12:58:21 12012 dinov2 image_net.py:225] creating entries: 10%
I20240906 12:58:21 12012 dinov2 image_net.py:225] creating entries: 11%
I20240906 12:58:21 12012 dinov2 image_net.py:225] creating entries: 12%
I20240906 12:58:21 12012 dinov2 image_net.py:225] creating entries: 13%
I20240906 12:58:21 12012 dinov2 image_net.py:225] creating entries: 14%
I20240906 12:58:21 12012 dinov2 image_net.py:225] creating entries: 15%
I20240906 12:58:21 12012 dinov2 image_net.py:225] creating entries: 16%
I20240906 12:58:21 12012 dinov2 image_net.py:225] creating entries: 17%
I20240906 12:58:21 12012 dinov2 image_net.py:225] creating entries: 18%
I20240906 12:58:21 12012 dinov2 image_net.py:225] creating entries: 19%
I20240906 12:58:21 12012 dinov2 image_net.py:225] creating entries: 20%
I20240906 12:58:21 12012 dinov2 image_net.py:225] creating entries: 21%
I20240906 12:58:21 12012 dinov2 image_net.py:225] creating entries: 22%
I20240906 12:58:21 12012 dinov2 image_net.py:225] creating entries: 23%
I20240906 12:58:21 12012 dinov2 image_net.py:225] creating entries: 24%
I20240906 12:58:21 12012 dinov2 image_net.py:225] creating entries: 25%
I20240906 12:58:21 12012 dinov2 image_net.py:225] creating entries: 26%
I20240906 12:58:21 12012 dinov2 image_net.py:225] creating entries: 27%
I20240906 12:58:21 12012 dinov2 image_net.py:225] creating entries: 28%
I20240906 12:58:21 12012 dinov2 image_net.py:225] creating entries: 29%
I20240906 12:58:21 12012 dinov2 image_net.py:225] creating entries: 30%
I20240906 12:58:21 12012 dinov2 image_net.py:225] creating entries: 31%
I20240906 12:58:21 12012 dinov2 image_net.py:225] creating entries: 32%
I20240906 12:58:21 12012 dinov2 image_net.py:225] creating entries: 33%
I20240906 12:58:21 12012 dinov2 image_net.py:225] creating entries: 34%
I20240906 12:58:21 12012 dinov2 image_net.py:225] creating entries: 35%
I20240906 12:58:21 12012 dinov2 image_net.py:225] creating entries: 36%
I20240906 12:58:21 12012 dinov2 image_net.py:225] creating entries: 37%
I20240906 12:58:21 12012 dinov2 image_net.py:225] creating entries: 38%
I20240906 12:58:21 12012 dinov2 image_net.py:225] creating entries: 39%
I20240906 12:58:21 12012 dinov2 image_net.py:225] creating entries: 40%
I20240906 12:58:21 12012 dinov2 image_net.py:225] creating entries: 41%
I20240906 12:58:21 12012 dinov2 image_net.py:225] creating entries: 42%
I20240906 12:58:21 12012 dinov2 image_net.py:225] creating entries: 43%
I20240906 12:58:21 12012 dinov2 image_net.py:225] creating entries: 44%
I20240906 12:58:21 12012 dinov2 image_net.py:225] creating entries: 45%
I20240906 12:58:21 12012 dinov2 image_net.py:225] creating entries: 46%
I20240906 12:58:21 12012 dinov2 image_net.py:225] creating entries: 47%
I20240906 12:58:21 12012 dinov2 image_net.py:225] creating entries: 48%
I20240906 12:58:21 12012 dinov2 image_net.py:225] creating entries: 49%
I20240906 12:58:21 12012 dinov2 image_net.py:225] creating entries: 50%
I20240906 12:58:21 12012 dinov2 image_net.py:225] creating entries: 51%
I20240906 12:58:21 12012 dinov2 image_net.py:225] creating entries: 52%
I20240906 12:58:21 12012 dinov2 image_net.py:225] creating entries: 53%
I20240906 12:58:21 12012 dinov2 image_net.py:225] creating entries: 54%
I20240906 12:58:21 12012 dinov2 image_net.py:225] creating entries: 55%
I20240906 12:58:21 12012 dinov2 image_net.py:225] creating entries: 56%
I20240906 12:58:21 12012 dinov2 image_net.py:225] creating entries: 57%
I20240906 12:58:21 12012 dinov2 image_net.py:225] creating entries: 58%
I20240906 12:58:21 12012 dinov2 image_net.py:225] creating entries: 59%
I20240906 12:58:21 12012 dinov2 image_net.py:225] creating entries: 60%
I20240906 12:58:21 12012 dinov2 image_net.py:225] creating entries: 61%
I20240906 12:58:21 12012 dinov2 image_net.py:225] creating entries: 62%
I20240906 12:58:21 12012 dinov2 image_net.py:225] creating entries: 63%
I20240906 12:58:21 12012 dinov2 image_net.py:225] creating entries: 64%
I20240906 12:58:21 12012 dinov2 image_net.py:225] creating entries: 65%
I20240906 12:58:21 12012 dinov2 image_net.py:225] creating entries: 66%
I20240906 12:58:21 12012 dinov2 image_net.py:225] creating entries: 67%
I20240906 12:58:21 12012 dinov2 image_net.py:225] creating entries: 68%
I20240906 12:58:21 12012 dinov2 image_net.py:225] creating entries: 69%
I20240906 12:58:21 12012 dinov2 image_net.py:225] creating entries: 70%
I20240906 12:58:21 12012 dinov2 image_net.py:225] creating entries: 71%
I20240906 12:58:21 12012 dinov2 image_net.py:225] creating entries: 72%
I20240906 12:58:21 12012 dinov2 image_net.py:225] creating entries: 73%
I20240906 12:58:21 12012 dinov2 image_net.py:225] creating entries: 74%
I20240906 12:58:21 12012 dinov2 image_net.py:225] creating entries: 75%
I20240906 12:58:21 12012 dinov2 image_net.py:225] creating entries: 76%
I20240906 12:58:21 12012 dinov2 image_net.py:225] creating entries: 77%
I20240906 12:58:21 12012 dinov2 image_net.py:225] creating entries: 78%
I20240906 12:58:21 12012 dinov2 image_net.py:225] creating entries: 79%
I20240906 12:58:21 12012 dinov2 image_net.py:225] creating entries: 80%
I20240906 12:58:21 12012 dinov2 image_net.py:225] creating entries: 81%
I20240906 12:58:21 12012 dinov2 image_net.py:225] creating entries: 82%
I20240906 12:58:21 12012 dinov2 image_net.py:225] creating entries: 83%
I20240906 12:58:21 12012 dinov2 image_net.py:225] creating entries: 84%
I20240906 12:58:21 12012 dinov2 image_net.py:225] creating entries: 85%
I20240906 12:58:21 12012 dinov2 image_net.py:225] creating entries: 86%
I20240906 12:58:21 12012 dinov2 image_net.py:225] creating entries: 87%
I20240906 12:58:21 12012 dinov2 image_net.py:225] creating entries: 88%
I20240906 12:58:21 12012 dinov2 image_net.py:225] creating entries: 89%
I20240906 12:58:21 12012 dinov2 image_net.py:225] creating entries: 90%
I20240906 12:58:21 12012 dinov2 image_net.py:225] creating entries: 91%
I20240906 12:58:21 12012 dinov2 image_net.py:225] creating entries: 92%
I20240906 12:58:21 12012 dinov2 image_net.py:225] creating entries: 93%
I20240906 12:58:21 12012 dinov2 image_net.py:225] creating entries: 94%
I20240906 12:58:21 12012 dinov2 image_net.py:225] creating entries: 95%
I20240906 12:58:21 12012 dinov2 image_net.py:225] creating entries: 96%
I20240906 12:58:21 12012 dinov2 image_net.py:225] creating entries: 97%
I20240906 12:58:21 12012 dinov2 image_net.py:225] creating entries: 98%
I20240906 12:58:21 12012 dinov2 image_net.py:225] creating entries: 99%
I20240906 12:58:21 12012 dinov2 image_net.py:225] creating entries: 100%
I20240906 12:58:21 12012 dinov2 image_net.py:249] saving entries to "entries-TEST.npy"
I20240906 13:05:34 19536 dinov2 config.py:59] git:
  sha: d43546fae6805ceb22618af8cf78469b61d913c7, status: has uncommitted changes, branch: main

I20240906 13:05:34 19536 dinov2 config.py:60] config_file: configs/train/vitl16_short.yaml
eval: 
eval_only: False
no_resume: False
opts: ['train.dataset_path=ImageNet:split=TRAIN:root=C:\\Users\\ISI_UTS\\Siladittya\\iclr2023\\inexpts\\datasets\\ILSVRC\\Data\\CLS-LOC:extra=C:\\Users\\ISI_UTS\\Siladittya\\iclr2023\\inexpts\\datasets\\ILSVRC\\Data\\CLS-LOC', 'train.output_dir=C:\\Users\\ISI_UTS\\Siladittya\\iclr2023\\inexpts\\dinov2_nosetup\\dinov2\\dinov2_nosetup\\dinov2']
output_dir: C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\dinov2_nosetup\dinov2\dinov2_nosetup\dinov2
I20240906 13:05:34 19536 dinov2 config.py:26] sqrt scaling learning rate; base: 0.004, new: 0.001
I20240906 13:05:34 19536 dinov2 config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
dino:
  loss_weight: 1.0
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
  koleo_loss_weight: 0.1
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  separate_head: false
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
train:
  batch_size_per_gpu: 64
  dataset_path: ImageNet:split=TRAIN:root=C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\datasets\ILSVRC\Data\CLS-LOC:extra=C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\datasets\ILSVRC\Data\CLS-LOC
  output_dir: C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\dinov2_nosetup\dinov2\dinov2_nosetup\dinov2
  saveckp_freq: 20
  seed: 0
  num_workers: 10
  OFFICIAL_EPOCH_LENGTH: 1250
  cache_dataset: true
  centering: centering
student:
  arch: vit_large
  patch_size: 16
  drop_path_rate: 0.3
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: mlp
  block_chunks: 4
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
  num_register_tokens: 0
  interpolate_antialias: false
  interpolate_offset: 0.1
teacher:
  momentum_teacher: 0.992
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 30
optim:
  epochs: 100
  weight_decay: 0.04
  weight_decay_end: 0.4
  base_lr: 0.004
  lr: 0.001
  warmup_epochs: 10
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 0.9
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 8
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 224
  local_crops_size: 96
evaluation:
  eval_period_iterations: 12500

I20240906 13:05:35 19536 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20240906 13:05:37 19536 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20240906 13:05:40 19536 dinov2 ssl_meta_arch.py:43] OPTIONS -- architecture : embed_dim: 1024
I20240906 13:05:40 19536 dinov2 ssl_meta_arch.py:58] OPTIONS -- DINO
I20240906 13:05:40 19536 dinov2 ssl_meta_arch.py:60] OPTIONS -- DINO -- loss_weight: 1.0
I20240906 13:05:40 19536 dinov2 ssl_meta_arch.py:61] OPTIONS -- DINO -- head_n_prototypes: 65536
I20240906 13:05:40 19536 dinov2 ssl_meta_arch.py:62] OPTIONS -- DINO -- head_bottleneck_dim: 256
I20240906 13:05:40 19536 dinov2 ssl_meta_arch.py:63] OPTIONS -- DINO -- head_hidden_dim: 2048
I20240906 13:05:40 19536 dinov2 ssl_meta_arch.py:75] OPTIONS -- DINO -- applying KOLEO regularization
I20240906 13:05:40 19536 dinov2 ssl_meta_arch.py:85] OPTIONS -- IBOT
I20240906 13:05:40 19536 dinov2 ssl_meta_arch.py:86] OPTIONS -- IBOT -- loss_weight: 1.0
I20240906 13:05:40 19536 dinov2 ssl_meta_arch.py:87] OPTIONS -- IBOT masking -- ibot_mask_ratio_tuple: [0.1, 0.5]
I20240906 13:05:40 19536 dinov2 ssl_meta_arch.py:88] OPTIONS -- IBOT masking -- ibot_mask_sample_probability: 0.5
I20240906 13:05:40 19536 dinov2 ssl_meta_arch.py:111] OPTIONS -- IBOT -- head shared with DINO
I20240906 13:05:40 19536 dinov2 ssl_meta_arch.py:121] Student and Teacher are built: they are both vit_large network.
I20240906 13:05:41 19536 dinov2 train.py:301] Model:
SSLMetaArch(
  (dino_loss): DINOLoss()
  (koleo_loss): KoLeoLoss(
    (pdist): PairwiseDistance()
  )
  (ibot_patch_loss): iBOTPatchLoss()
  (student): ModuleDict(
    (backbone): DinoVisionTransformer(
      (patch_embed): PatchEmbed(
        (proj): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))
        (norm): Identity()
      )
      (blocks): ModuleList(
        (0): BlockChunk(
          (0-5): 6 x NestedTensorBlock(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): DropPath()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): DropPath()
          )
        )
        (1): BlockChunk(
          (0-5): 6 x Identity()
          (6-11): 6 x NestedTensorBlock(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): DropPath()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): DropPath()
          )
        )
        (2): BlockChunk(
          (0-11): 12 x Identity()
          (12-17): 6 x NestedTensorBlock(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): DropPath()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): DropPath()
          )
        )
        (3): BlockChunk(
          (0-17): 18 x Identity()
          (18-23): 6 x NestedTensorBlock(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): DropPath()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): DropPath()
          )
        )
      )
      (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (head): Identity()
    )
    (dino_head): DINOHead(
      (mlp): Sequential(
        (0): Linear(in_features=1024, out_features=2048, bias=True)
        (1): GELU(approximate='none')
        (2): Linear(in_features=2048, out_features=2048, bias=True)
        (3): GELU(approximate='none')
        (4): Linear(in_features=2048, out_features=256, bias=True)
      )
      (last_layer): Linear(in_features=256, out_features=65536, bias=False)
    )
  )
  (teacher): ModuleDict(
    (backbone): DinoVisionTransformer(
      (patch_embed): PatchEmbed(
        (proj): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))
        (norm): Identity()
      )
      (blocks): ModuleList(
        (0): BlockChunk(
          (0-5): 6 x NestedTensorBlock(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): Identity()
          )
        )
        (1): BlockChunk(
          (0-5): 6 x Identity()
          (6-11): 6 x NestedTensorBlock(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): Identity()
          )
        )
        (2): BlockChunk(
          (0-11): 12 x Identity()
          (12-17): 6 x NestedTensorBlock(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): Identity()
          )
        )
        (3): BlockChunk(
          (0-17): 18 x Identity()
          (18-23): 6 x NestedTensorBlock(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): Identity()
          )
        )
      )
      (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (head): Identity()
    )
    (dino_head): DINOHead(
      (mlp): Sequential(
        (0): Linear(in_features=1024, out_features=2048, bias=True)
        (1): GELU(approximate='none')
        (2): Linear(in_features=2048, out_features=2048, bias=True)
        (3): GELU(approximate='none')
        (4): Linear(in_features=2048, out_features=256, bias=True)
      )
      (last_layer): Linear(in_features=256, out_features=65536, bias=False)
    )
  )
)
I20240906 13:05:41 19536 dinov2 param_groups.py:54] chunked fsdp
I20240906 13:05:41 19536 dinov2 param_groups.py:87] cls_token: lr_multiplier: 0.0717897987691853, wd_multiplier: 1.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] pos_embed: lr_multiplier: 0.0717897987691853, wd_multiplier: 1.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] mask_token: lr_multiplier: 0.0717897987691853, wd_multiplier: 1.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] patch_embed.proj.weight: lr_multiplier: 0.01435795975383706, wd_multiplier: 1.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] patch_embed.proj.bias: lr_multiplier: 0.01435795975383706, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.0.0.norm1.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.0.0.norm1.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.0.0.attn.qkv.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 1.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.0.0.attn.qkv.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.0.0.attn.proj.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 1.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.0.0.attn.proj.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.0.0.ls1.gamma: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.0.0.norm2.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.0.0.norm2.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.0.0.mlp.fc1.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 1.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.0.0.mlp.fc1.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.0.0.mlp.fc2.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 1.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.0.0.mlp.fc2.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.0.0.ls2.gamma: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.0.1.norm1.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.0.1.norm1.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.0.1.attn.qkv.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 1.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.0.1.attn.qkv.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.0.1.attn.proj.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 1.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.0.1.attn.proj.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.0.1.ls1.gamma: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.0.1.norm2.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.0.1.norm2.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.0.1.mlp.fc1.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 1.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.0.1.mlp.fc1.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.0.1.mlp.fc2.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 1.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.0.1.mlp.fc2.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.0.1.ls2.gamma: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.0.2.norm1.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.0.2.norm1.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.0.2.attn.qkv.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 1.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.0.2.attn.qkv.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.0.2.attn.proj.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 1.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.0.2.attn.proj.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.0.2.ls1.gamma: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.0.2.norm2.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.0.2.norm2.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.0.2.mlp.fc1.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 1.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.0.2.mlp.fc1.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.0.2.mlp.fc2.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 1.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.0.2.mlp.fc2.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.0.2.ls2.gamma: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.0.3.norm1.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.0.3.norm1.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.0.3.attn.qkv.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 1.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.0.3.attn.qkv.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.0.3.attn.proj.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 1.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.0.3.attn.proj.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.0.3.ls1.gamma: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.0.3.norm2.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.0.3.norm2.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.0.3.mlp.fc1.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 1.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.0.3.mlp.fc1.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.0.3.mlp.fc2.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 1.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.0.3.mlp.fc2.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.0.3.ls2.gamma: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.0.4.norm1.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.0.4.norm1.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.0.4.attn.qkv.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 1.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.0.4.attn.qkv.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.0.4.attn.proj.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 1.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.0.4.attn.proj.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.0.4.ls1.gamma: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.0.4.norm2.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.0.4.norm2.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.0.4.mlp.fc1.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 1.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.0.4.mlp.fc1.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.0.4.mlp.fc2.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 1.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.0.4.mlp.fc2.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.0.4.ls2.gamma: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.0.5.norm1.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.0.5.norm1.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.0.5.attn.qkv.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 1.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.0.5.attn.qkv.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.0.5.attn.proj.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 1.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.0.5.attn.proj.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.0.5.ls1.gamma: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.0.5.norm2.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.0.5.norm2.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.0.5.mlp.fc1.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 1.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.0.5.mlp.fc1.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.0.5.mlp.fc2.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 1.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.0.5.mlp.fc2.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.0.5.ls2.gamma: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.1.6.norm1.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.1.6.norm1.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.1.6.attn.qkv.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 1.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.1.6.attn.qkv.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.1.6.attn.proj.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 1.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.1.6.attn.proj.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.1.6.ls1.gamma: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.1.6.norm2.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.1.6.norm2.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.1.6.mlp.fc1.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 1.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.1.6.mlp.fc1.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.1.6.mlp.fc2.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 1.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.1.6.mlp.fc2.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.1.6.ls2.gamma: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.1.7.norm1.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.1.7.norm1.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.1.7.attn.qkv.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 1.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.1.7.attn.qkv.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.1.7.attn.proj.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 1.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.1.7.attn.proj.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.1.7.ls1.gamma: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.1.7.norm2.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.1.7.norm2.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.1.7.mlp.fc1.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 1.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.1.7.mlp.fc1.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.1.7.mlp.fc2.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 1.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.1.7.mlp.fc2.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.1.7.ls2.gamma: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.1.8.norm1.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.1.8.norm1.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.1.8.attn.qkv.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 1.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.1.8.attn.qkv.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.1.8.attn.proj.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 1.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.1.8.attn.proj.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.1.8.ls1.gamma: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.1.8.norm2.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.1.8.norm2.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.1.8.mlp.fc1.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 1.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.1.8.mlp.fc1.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.1.8.mlp.fc2.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 1.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.1.8.mlp.fc2.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.1.8.ls2.gamma: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.1.9.norm1.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.1.9.norm1.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.1.9.attn.qkv.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 1.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.1.9.attn.qkv.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.1.9.attn.proj.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 1.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.1.9.attn.proj.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.1.9.ls1.gamma: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.1.9.norm2.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.1.9.norm2.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.1.9.mlp.fc1.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 1.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.1.9.mlp.fc1.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.1.9.mlp.fc2.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 1.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.1.9.mlp.fc2.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.1.9.ls2.gamma: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.1.10.norm1.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.1.10.norm1.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.1.10.attn.qkv.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 1.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.1.10.attn.qkv.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.1.10.attn.proj.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 1.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.1.10.attn.proj.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.1.10.ls1.gamma: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.1.10.norm2.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.1.10.norm2.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.1.10.mlp.fc1.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 1.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.1.10.mlp.fc1.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.1.10.mlp.fc2.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 1.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.1.10.mlp.fc2.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.1.10.ls2.gamma: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.1.11.norm1.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.1.11.norm1.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.1.11.attn.qkv.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 1.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.1.11.attn.qkv.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.1.11.attn.proj.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 1.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.1.11.attn.proj.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.1.11.ls1.gamma: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.1.11.norm2.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.1.11.norm2.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.1.11.mlp.fc1.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 1.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.1.11.mlp.fc1.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.1.11.mlp.fc2.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 1.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.1.11.mlp.fc2.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.1.11.ls2.gamma: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.2.12.norm1.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.2.12.norm1.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.2.12.attn.qkv.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 1.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.2.12.attn.qkv.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.2.12.attn.proj.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 1.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.2.12.attn.proj.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.2.12.ls1.gamma: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.2.12.norm2.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.2.12.norm2.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.2.12.mlp.fc1.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 1.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.2.12.mlp.fc1.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.2.12.mlp.fc2.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 1.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.2.12.mlp.fc2.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.2.12.ls2.gamma: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.2.13.norm1.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.2.13.norm1.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.2.13.attn.qkv.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 1.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.2.13.attn.qkv.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.2.13.attn.proj.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 1.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.2.13.attn.proj.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.2.13.ls1.gamma: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.2.13.norm2.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.2.13.norm2.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.2.13.mlp.fc1.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 1.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.2.13.mlp.fc1.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.2.13.mlp.fc2.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 1.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.2.13.mlp.fc2.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.2.13.ls2.gamma: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.2.14.norm1.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.2.14.norm1.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.2.14.attn.qkv.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 1.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.2.14.attn.qkv.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.2.14.attn.proj.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 1.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.2.14.attn.proj.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.2.14.ls1.gamma: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.2.14.norm2.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.2.14.norm2.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.2.14.mlp.fc1.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 1.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.2.14.mlp.fc1.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.2.14.mlp.fc2.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 1.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.2.14.mlp.fc2.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.2.14.ls2.gamma: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.2.15.norm1.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.2.15.norm1.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.2.15.attn.qkv.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 1.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.2.15.attn.qkv.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.2.15.attn.proj.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 1.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.2.15.attn.proj.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.2.15.ls1.gamma: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.2.15.norm2.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.2.15.norm2.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.2.15.mlp.fc1.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 1.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.2.15.mlp.fc1.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.2.15.mlp.fc2.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 1.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.2.15.mlp.fc2.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.2.15.ls2.gamma: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.2.16.norm1.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.2.16.norm1.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.2.16.attn.qkv.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 1.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.2.16.attn.qkv.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.2.16.attn.proj.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 1.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.2.16.attn.proj.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.2.16.ls1.gamma: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.2.16.norm2.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.2.16.norm2.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.2.16.mlp.fc1.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 1.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.2.16.mlp.fc1.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.2.16.mlp.fc2.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 1.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.2.16.mlp.fc2.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.2.16.ls2.gamma: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.2.17.norm1.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.2.17.norm1.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.2.17.attn.qkv.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 1.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.2.17.attn.qkv.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.2.17.attn.proj.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 1.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.2.17.attn.proj.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.2.17.ls1.gamma: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.2.17.norm2.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.2.17.norm2.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.2.17.mlp.fc1.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 1.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.2.17.mlp.fc1.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.2.17.mlp.fc2.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 1.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.2.17.mlp.fc2.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.2.17.ls2.gamma: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.3.18.norm1.weight: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.3.18.norm1.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.3.18.attn.qkv.weight: lr_multiplier: 0.531441, wd_multiplier: 1.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.3.18.attn.qkv.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.3.18.attn.proj.weight: lr_multiplier: 0.531441, wd_multiplier: 1.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.3.18.attn.proj.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.3.18.ls1.gamma: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.3.18.norm2.weight: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.3.18.norm2.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.3.18.mlp.fc1.weight: lr_multiplier: 0.531441, wd_multiplier: 1.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.3.18.mlp.fc1.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.3.18.mlp.fc2.weight: lr_multiplier: 0.531441, wd_multiplier: 1.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.3.18.mlp.fc2.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.3.18.ls2.gamma: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.3.19.norm1.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.3.19.norm1.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.3.19.attn.qkv.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 1.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.3.19.attn.qkv.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.3.19.attn.proj.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 1.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.3.19.attn.proj.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.3.19.ls1.gamma: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.3.19.norm2.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.3.19.norm2.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.3.19.mlp.fc1.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 1.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.3.19.mlp.fc1.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.3.19.mlp.fc2.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 1.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.3.19.mlp.fc2.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.3.19.ls2.gamma: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.3.20.norm1.weight: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.3.20.norm1.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.3.20.attn.qkv.weight: lr_multiplier: 0.6561, wd_multiplier: 1.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.3.20.attn.qkv.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.3.20.attn.proj.weight: lr_multiplier: 0.6561, wd_multiplier: 1.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.3.20.attn.proj.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.3.20.ls1.gamma: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.3.20.norm2.weight: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.3.20.norm2.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.3.20.mlp.fc1.weight: lr_multiplier: 0.6561, wd_multiplier: 1.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.3.20.mlp.fc1.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.3.20.mlp.fc2.weight: lr_multiplier: 0.6561, wd_multiplier: 1.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.3.20.mlp.fc2.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.3.20.ls2.gamma: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.3.21.norm1.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.3.21.norm1.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.3.21.attn.qkv.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 1.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.3.21.attn.qkv.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.3.21.attn.proj.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 1.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.3.21.attn.proj.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.3.21.ls1.gamma: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.3.21.norm2.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.3.21.norm2.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.3.21.mlp.fc1.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 1.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.3.21.mlp.fc1.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.3.21.mlp.fc2.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 1.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.3.21.mlp.fc2.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.3.21.ls2.gamma: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.3.22.norm1.weight: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.3.22.norm1.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.3.22.attn.qkv.weight: lr_multiplier: 0.81, wd_multiplier: 1.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.3.22.attn.qkv.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.3.22.attn.proj.weight: lr_multiplier: 0.81, wd_multiplier: 1.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.3.22.attn.proj.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.3.22.ls1.gamma: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.3.22.norm2.weight: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.3.22.norm2.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.3.22.mlp.fc1.weight: lr_multiplier: 0.81, wd_multiplier: 1.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.3.22.mlp.fc1.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.3.22.mlp.fc2.weight: lr_multiplier: 0.81, wd_multiplier: 1.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.3.22.mlp.fc2.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.3.22.ls2.gamma: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.3.23.norm1.weight: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.3.23.norm1.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.3.23.attn.qkv.weight: lr_multiplier: 0.9, wd_multiplier: 1.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.3.23.attn.qkv.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.3.23.attn.proj.weight: lr_multiplier: 0.9, wd_multiplier: 1.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.3.23.attn.proj.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.3.23.ls1.gamma: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.3.23.norm2.weight: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.3.23.norm2.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.3.23.mlp.fc1.weight: lr_multiplier: 0.9, wd_multiplier: 1.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.3.23.mlp.fc1.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.3.23.mlp.fc2.weight: lr_multiplier: 0.9, wd_multiplier: 1.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.3.23.mlp.fc2.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] blocks.3.23.ls2.gamma: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] norm.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] norm.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 ssl_meta_arch.py:378] fusing param groups
I20240906 13:05:41 19536 dinov2 param_groups.py:64] else code branch
I20240906 13:05:41 19536 dinov2 param_groups.py:87] mlp.0.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] mlp.0.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] mlp.2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] mlp.2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] mlp.4.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] mlp.4.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] last_layer.weight_g: lr_multiplier: 1.0, wd_multiplier: 1.0
I20240906 13:05:41 19536 dinov2 param_groups.py:87] last_layer.weight_v: lr_multiplier: 1.0, wd_multiplier: 1.0
I20240906 13:05:41 19536 dinov2 ssl_meta_arch.py:378] fusing param groups
I20240906 13:05:41 19536 dinov2 train.py:98] Schedulers ready.
I20240906 13:05:41 19536 dinov2 augmentations.py:34] ###################################
I20240906 13:05:41 19536 dinov2 augmentations.py:35] Using data augmentation parameters:
I20240906 13:05:41 19536 dinov2 augmentations.py:36] global_crops_scale: [0.32, 1.0]
I20240906 13:05:41 19536 dinov2 augmentations.py:37] local_crops_scale: [0.05, 0.32]
I20240906 13:05:41 19536 dinov2 augmentations.py:38] local_crops_number: 8
I20240906 13:05:41 19536 dinov2 augmentations.py:39] global_crops_size: 224
I20240906 13:05:41 19536 dinov2 augmentations.py:40] local_crops_size: 96
I20240906 13:05:41 19536 dinov2 augmentations.py:41] ###################################
I20240906 13:05:41 19536 dinov2 loaders.py:84] using dataset: "ImageNet:split=TRAIN:root=C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\datasets\ILSVRC\Data\CLS-LOC:extra=C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\datasets\ILSVRC\Data\CLS-LOC"
I20240906 13:06:52 14292 dinov2 config.py:59] git:
  sha: d43546fae6805ceb22618af8cf78469b61d913c7, status: has uncommitted changes, branch: main

I20240906 13:06:52 14292 dinov2 config.py:60] config_file: configs/train/vitl16_short.yaml
eval: 
eval_only: False
no_resume: False
opts: ['train.dataset_path=ImageNet:split=TRAIN:root=C:\\Users\\ISI_UTS\\Siladittya\\iclr2023\\inexpts\\datasets\\ILSVRC\\Data\\CLS-LOC:extra=C:\\Users\\ISI_UTS\\Siladittya\\iclr2023\\inexpts\\datasets\\ILSVRC\\Data\\CLS-LOC', 'train.output_dir=C:\\Users\\ISI_UTS\\Siladittya\\iclr2023\\inexpts\\dinov2_nosetup\\dinov2\\dinov2_nosetup\\dinov2']
output_dir: C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\dinov2_nosetup\dinov2\dinov2_nosetup\dinov2
I20240906 13:06:52 14292 dinov2 config.py:26] sqrt scaling learning rate; base: 0.004, new: 0.001
I20240906 13:06:52 14292 dinov2 config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
dino:
  loss_weight: 1.0
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
  koleo_loss_weight: 0.1
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  separate_head: false
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
train:
  batch_size_per_gpu: 64
  dataset_path: ImageNet:split=TRAIN:root=C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\datasets\ILSVRC\Data\CLS-LOC:extra=C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\datasets\ILSVRC\Data\CLS-LOC
  output_dir: C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\dinov2_nosetup\dinov2\dinov2_nosetup\dinov2
  saveckp_freq: 20
  seed: 0
  num_workers: 10
  OFFICIAL_EPOCH_LENGTH: 1250
  cache_dataset: true
  centering: centering
student:
  arch: vit_large
  patch_size: 16
  drop_path_rate: 0.3
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: mlp
  block_chunks: 4
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
  num_register_tokens: 0
  interpolate_antialias: false
  interpolate_offset: 0.1
teacher:
  momentum_teacher: 0.992
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 30
optim:
  epochs: 100
  weight_decay: 0.04
  weight_decay_end: 0.4
  base_lr: 0.004
  lr: 0.001
  warmup_epochs: 10
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 0.9
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 8
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 224
  local_crops_size: 96
evaluation:
  eval_period_iterations: 12500

I20240906 13:06:52 14292 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20240906 13:06:55 14292 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20240906 13:06:57 14292 dinov2 ssl_meta_arch.py:43] OPTIONS -- architecture : embed_dim: 1024
I20240906 13:06:57 14292 dinov2 ssl_meta_arch.py:58] OPTIONS -- DINO
I20240906 13:06:57 14292 dinov2 ssl_meta_arch.py:60] OPTIONS -- DINO -- loss_weight: 1.0
I20240906 13:06:57 14292 dinov2 ssl_meta_arch.py:61] OPTIONS -- DINO -- head_n_prototypes: 65536
I20240906 13:06:57 14292 dinov2 ssl_meta_arch.py:62] OPTIONS -- DINO -- head_bottleneck_dim: 256
I20240906 13:06:57 14292 dinov2 ssl_meta_arch.py:63] OPTIONS -- DINO -- head_hidden_dim: 2048
I20240906 13:06:57 14292 dinov2 ssl_meta_arch.py:75] OPTIONS -- DINO -- applying KOLEO regularization
I20240906 13:06:58 14292 dinov2 ssl_meta_arch.py:85] OPTIONS -- IBOT
I20240906 13:06:58 14292 dinov2 ssl_meta_arch.py:86] OPTIONS -- IBOT -- loss_weight: 1.0
I20240906 13:06:58 14292 dinov2 ssl_meta_arch.py:87] OPTIONS -- IBOT masking -- ibot_mask_ratio_tuple: [0.1, 0.5]
I20240906 13:06:58 14292 dinov2 ssl_meta_arch.py:88] OPTIONS -- IBOT masking -- ibot_mask_sample_probability: 0.5
I20240906 13:06:58 14292 dinov2 ssl_meta_arch.py:111] OPTIONS -- IBOT -- head shared with DINO
I20240906 13:06:58 14292 dinov2 ssl_meta_arch.py:121] Student and Teacher are built: they are both vit_large network.
I20240906 13:06:59 14292 dinov2 train.py:301] Model:
SSLMetaArch(
  (dino_loss): DINOLoss()
  (koleo_loss): KoLeoLoss(
    (pdist): PairwiseDistance()
  )
  (ibot_patch_loss): iBOTPatchLoss()
  (student): ModuleDict(
    (backbone): DinoVisionTransformer(
      (patch_embed): PatchEmbed(
        (proj): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))
        (norm): Identity()
      )
      (blocks): ModuleList(
        (0): BlockChunk(
          (0-5): 6 x NestedTensorBlock(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): DropPath()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): DropPath()
          )
        )
        (1): BlockChunk(
          (0-5): 6 x Identity()
          (6-11): 6 x NestedTensorBlock(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): DropPath()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): DropPath()
          )
        )
        (2): BlockChunk(
          (0-11): 12 x Identity()
          (12-17): 6 x NestedTensorBlock(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): DropPath()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): DropPath()
          )
        )
        (3): BlockChunk(
          (0-17): 18 x Identity()
          (18-23): 6 x NestedTensorBlock(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): DropPath()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): DropPath()
          )
        )
      )
      (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (head): Identity()
    )
    (dino_head): DINOHead(
      (mlp): Sequential(
        (0): Linear(in_features=1024, out_features=2048, bias=True)
        (1): GELU(approximate='none')
        (2): Linear(in_features=2048, out_features=2048, bias=True)
        (3): GELU(approximate='none')
        (4): Linear(in_features=2048, out_features=256, bias=True)
      )
      (last_layer): Linear(in_features=256, out_features=65536, bias=False)
    )
  )
  (teacher): ModuleDict(
    (backbone): DinoVisionTransformer(
      (patch_embed): PatchEmbed(
        (proj): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))
        (norm): Identity()
      )
      (blocks): ModuleList(
        (0): BlockChunk(
          (0-5): 6 x NestedTensorBlock(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): Identity()
          )
        )
        (1): BlockChunk(
          (0-5): 6 x Identity()
          (6-11): 6 x NestedTensorBlock(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): Identity()
          )
        )
        (2): BlockChunk(
          (0-11): 12 x Identity()
          (12-17): 6 x NestedTensorBlock(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): Identity()
          )
        )
        (3): BlockChunk(
          (0-17): 18 x Identity()
          (18-23): 6 x NestedTensorBlock(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): Identity()
          )
        )
      )
      (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (head): Identity()
    )
    (dino_head): DINOHead(
      (mlp): Sequential(
        (0): Linear(in_features=1024, out_features=2048, bias=True)
        (1): GELU(approximate='none')
        (2): Linear(in_features=2048, out_features=2048, bias=True)
        (3): GELU(approximate='none')
        (4): Linear(in_features=2048, out_features=256, bias=True)
      )
      (last_layer): Linear(in_features=256, out_features=65536, bias=False)
    )
  )
)
I20240906 13:06:59 14292 dinov2 param_groups.py:54] chunked fsdp
I20240906 13:06:59 14292 dinov2 param_groups.py:87] cls_token: lr_multiplier: 0.0717897987691853, wd_multiplier: 1.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] pos_embed: lr_multiplier: 0.0717897987691853, wd_multiplier: 1.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] mask_token: lr_multiplier: 0.0717897987691853, wd_multiplier: 1.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] patch_embed.proj.weight: lr_multiplier: 0.01435795975383706, wd_multiplier: 1.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] patch_embed.proj.bias: lr_multiplier: 0.01435795975383706, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.0.0.norm1.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.0.0.norm1.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.0.0.attn.qkv.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 1.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.0.0.attn.qkv.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.0.0.attn.proj.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 1.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.0.0.attn.proj.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.0.0.ls1.gamma: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.0.0.norm2.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.0.0.norm2.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.0.0.mlp.fc1.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 1.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.0.0.mlp.fc1.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.0.0.mlp.fc2.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 1.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.0.0.mlp.fc2.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.0.0.ls2.gamma: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.0.1.norm1.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.0.1.norm1.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.0.1.attn.qkv.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 1.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.0.1.attn.qkv.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.0.1.attn.proj.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 1.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.0.1.attn.proj.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.0.1.ls1.gamma: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.0.1.norm2.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.0.1.norm2.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.0.1.mlp.fc1.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 1.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.0.1.mlp.fc1.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.0.1.mlp.fc2.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 1.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.0.1.mlp.fc2.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.0.1.ls2.gamma: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.0.2.norm1.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.0.2.norm1.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.0.2.attn.qkv.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 1.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.0.2.attn.qkv.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.0.2.attn.proj.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 1.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.0.2.attn.proj.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.0.2.ls1.gamma: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.0.2.norm2.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.0.2.norm2.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.0.2.mlp.fc1.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 1.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.0.2.mlp.fc1.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.0.2.mlp.fc2.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 1.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.0.2.mlp.fc2.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.0.2.ls2.gamma: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.0.3.norm1.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.0.3.norm1.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.0.3.attn.qkv.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 1.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.0.3.attn.qkv.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.0.3.attn.proj.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 1.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.0.3.attn.proj.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.0.3.ls1.gamma: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.0.3.norm2.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.0.3.norm2.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.0.3.mlp.fc1.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 1.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.0.3.mlp.fc1.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.0.3.mlp.fc2.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 1.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.0.3.mlp.fc2.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.0.3.ls2.gamma: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.0.4.norm1.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.0.4.norm1.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.0.4.attn.qkv.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 1.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.0.4.attn.qkv.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.0.4.attn.proj.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 1.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.0.4.attn.proj.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.0.4.ls1.gamma: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.0.4.norm2.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.0.4.norm2.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.0.4.mlp.fc1.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 1.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.0.4.mlp.fc1.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.0.4.mlp.fc2.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 1.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.0.4.mlp.fc2.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.0.4.ls2.gamma: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.0.5.norm1.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.0.5.norm1.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.0.5.attn.qkv.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 1.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.0.5.attn.qkv.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.0.5.attn.proj.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 1.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.0.5.attn.proj.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.0.5.ls1.gamma: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.0.5.norm2.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.0.5.norm2.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.0.5.mlp.fc1.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 1.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.0.5.mlp.fc1.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.0.5.mlp.fc2.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 1.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.0.5.mlp.fc2.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.0.5.ls2.gamma: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.1.6.norm1.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.1.6.norm1.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.1.6.attn.qkv.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 1.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.1.6.attn.qkv.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.1.6.attn.proj.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 1.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.1.6.attn.proj.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.1.6.ls1.gamma: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.1.6.norm2.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.1.6.norm2.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.1.6.mlp.fc1.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 1.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.1.6.mlp.fc1.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.1.6.mlp.fc2.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 1.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.1.6.mlp.fc2.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.1.6.ls2.gamma: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.1.7.norm1.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.1.7.norm1.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.1.7.attn.qkv.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 1.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.1.7.attn.qkv.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.1.7.attn.proj.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 1.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.1.7.attn.proj.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.1.7.ls1.gamma: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.1.7.norm2.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.1.7.norm2.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.1.7.mlp.fc1.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 1.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.1.7.mlp.fc1.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.1.7.mlp.fc2.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 1.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.1.7.mlp.fc2.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.1.7.ls2.gamma: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.1.8.norm1.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.1.8.norm1.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.1.8.attn.qkv.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 1.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.1.8.attn.qkv.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.1.8.attn.proj.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 1.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.1.8.attn.proj.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.1.8.ls1.gamma: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.1.8.norm2.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.1.8.norm2.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.1.8.mlp.fc1.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 1.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.1.8.mlp.fc1.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.1.8.mlp.fc2.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 1.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.1.8.mlp.fc2.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.1.8.ls2.gamma: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.1.9.norm1.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.1.9.norm1.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.1.9.attn.qkv.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 1.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.1.9.attn.qkv.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.1.9.attn.proj.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 1.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.1.9.attn.proj.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.1.9.ls1.gamma: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.1.9.norm2.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.1.9.norm2.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.1.9.mlp.fc1.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 1.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.1.9.mlp.fc1.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.1.9.mlp.fc2.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 1.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.1.9.mlp.fc2.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.1.9.ls2.gamma: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.1.10.norm1.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.1.10.norm1.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.1.10.attn.qkv.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 1.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.1.10.attn.qkv.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.1.10.attn.proj.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 1.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.1.10.attn.proj.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.1.10.ls1.gamma: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.1.10.norm2.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.1.10.norm2.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.1.10.mlp.fc1.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 1.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.1.10.mlp.fc1.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.1.10.mlp.fc2.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 1.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.1.10.mlp.fc2.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.1.10.ls2.gamma: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.1.11.norm1.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.1.11.norm1.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.1.11.attn.qkv.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 1.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.1.11.attn.qkv.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.1.11.attn.proj.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 1.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.1.11.attn.proj.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.1.11.ls1.gamma: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.1.11.norm2.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.1.11.norm2.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.1.11.mlp.fc1.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 1.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.1.11.mlp.fc1.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.1.11.mlp.fc2.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 1.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.1.11.mlp.fc2.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.1.11.ls2.gamma: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.2.12.norm1.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.2.12.norm1.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.2.12.attn.qkv.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 1.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.2.12.attn.qkv.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.2.12.attn.proj.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 1.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.2.12.attn.proj.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.2.12.ls1.gamma: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.2.12.norm2.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.2.12.norm2.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.2.12.mlp.fc1.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 1.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.2.12.mlp.fc1.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.2.12.mlp.fc2.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 1.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.2.12.mlp.fc2.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.2.12.ls2.gamma: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.2.13.norm1.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.2.13.norm1.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.2.13.attn.qkv.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 1.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.2.13.attn.qkv.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.2.13.attn.proj.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 1.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.2.13.attn.proj.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.2.13.ls1.gamma: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.2.13.norm2.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.2.13.norm2.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.2.13.mlp.fc1.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 1.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.2.13.mlp.fc1.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.2.13.mlp.fc2.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 1.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.2.13.mlp.fc2.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.2.13.ls2.gamma: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.2.14.norm1.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.2.14.norm1.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.2.14.attn.qkv.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 1.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.2.14.attn.qkv.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.2.14.attn.proj.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 1.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.2.14.attn.proj.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.2.14.ls1.gamma: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.2.14.norm2.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.2.14.norm2.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.2.14.mlp.fc1.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 1.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.2.14.mlp.fc1.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.2.14.mlp.fc2.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 1.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.2.14.mlp.fc2.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.2.14.ls2.gamma: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.2.15.norm1.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.2.15.norm1.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.2.15.attn.qkv.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 1.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.2.15.attn.qkv.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.2.15.attn.proj.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 1.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.2.15.attn.proj.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.2.15.ls1.gamma: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.2.15.norm2.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.2.15.norm2.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.2.15.mlp.fc1.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 1.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.2.15.mlp.fc1.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.2.15.mlp.fc2.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 1.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.2.15.mlp.fc2.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.2.15.ls2.gamma: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.2.16.norm1.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.2.16.norm1.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.2.16.attn.qkv.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 1.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.2.16.attn.qkv.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.2.16.attn.proj.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 1.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.2.16.attn.proj.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.2.16.ls1.gamma: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.2.16.norm2.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.2.16.norm2.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.2.16.mlp.fc1.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 1.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.2.16.mlp.fc1.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.2.16.mlp.fc2.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 1.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.2.16.mlp.fc2.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.2.16.ls2.gamma: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.2.17.norm1.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.2.17.norm1.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.2.17.attn.qkv.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 1.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.2.17.attn.qkv.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.2.17.attn.proj.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 1.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.2.17.attn.proj.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.2.17.ls1.gamma: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.2.17.norm2.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.2.17.norm2.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.2.17.mlp.fc1.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 1.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.2.17.mlp.fc1.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.2.17.mlp.fc2.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 1.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.2.17.mlp.fc2.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.2.17.ls2.gamma: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.3.18.norm1.weight: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.3.18.norm1.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.3.18.attn.qkv.weight: lr_multiplier: 0.531441, wd_multiplier: 1.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.3.18.attn.qkv.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.3.18.attn.proj.weight: lr_multiplier: 0.531441, wd_multiplier: 1.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.3.18.attn.proj.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.3.18.ls1.gamma: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.3.18.norm2.weight: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.3.18.norm2.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.3.18.mlp.fc1.weight: lr_multiplier: 0.531441, wd_multiplier: 1.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.3.18.mlp.fc1.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.3.18.mlp.fc2.weight: lr_multiplier: 0.531441, wd_multiplier: 1.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.3.18.mlp.fc2.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.3.18.ls2.gamma: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.3.19.norm1.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.3.19.norm1.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.3.19.attn.qkv.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 1.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.3.19.attn.qkv.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.3.19.attn.proj.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 1.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.3.19.attn.proj.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.3.19.ls1.gamma: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.3.19.norm2.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.3.19.norm2.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.3.19.mlp.fc1.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 1.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.3.19.mlp.fc1.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.3.19.mlp.fc2.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 1.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.3.19.mlp.fc2.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.3.19.ls2.gamma: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.3.20.norm1.weight: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.3.20.norm1.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.3.20.attn.qkv.weight: lr_multiplier: 0.6561, wd_multiplier: 1.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.3.20.attn.qkv.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.3.20.attn.proj.weight: lr_multiplier: 0.6561, wd_multiplier: 1.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.3.20.attn.proj.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.3.20.ls1.gamma: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.3.20.norm2.weight: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.3.20.norm2.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.3.20.mlp.fc1.weight: lr_multiplier: 0.6561, wd_multiplier: 1.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.3.20.mlp.fc1.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.3.20.mlp.fc2.weight: lr_multiplier: 0.6561, wd_multiplier: 1.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.3.20.mlp.fc2.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.3.20.ls2.gamma: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.3.21.norm1.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.3.21.norm1.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.3.21.attn.qkv.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 1.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.3.21.attn.qkv.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.3.21.attn.proj.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 1.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.3.21.attn.proj.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.3.21.ls1.gamma: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.3.21.norm2.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.3.21.norm2.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.3.21.mlp.fc1.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 1.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.3.21.mlp.fc1.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.3.21.mlp.fc2.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 1.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.3.21.mlp.fc2.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.3.21.ls2.gamma: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.3.22.norm1.weight: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.3.22.norm1.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.3.22.attn.qkv.weight: lr_multiplier: 0.81, wd_multiplier: 1.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.3.22.attn.qkv.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.3.22.attn.proj.weight: lr_multiplier: 0.81, wd_multiplier: 1.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.3.22.attn.proj.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.3.22.ls1.gamma: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.3.22.norm2.weight: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.3.22.norm2.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.3.22.mlp.fc1.weight: lr_multiplier: 0.81, wd_multiplier: 1.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.3.22.mlp.fc1.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.3.22.mlp.fc2.weight: lr_multiplier: 0.81, wd_multiplier: 1.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.3.22.mlp.fc2.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.3.22.ls2.gamma: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.3.23.norm1.weight: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.3.23.norm1.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.3.23.attn.qkv.weight: lr_multiplier: 0.9, wd_multiplier: 1.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.3.23.attn.qkv.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.3.23.attn.proj.weight: lr_multiplier: 0.9, wd_multiplier: 1.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.3.23.attn.proj.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.3.23.ls1.gamma: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.3.23.norm2.weight: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.3.23.norm2.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.3.23.mlp.fc1.weight: lr_multiplier: 0.9, wd_multiplier: 1.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.3.23.mlp.fc1.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.3.23.mlp.fc2.weight: lr_multiplier: 0.9, wd_multiplier: 1.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.3.23.mlp.fc2.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] blocks.3.23.ls2.gamma: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] norm.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] norm.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 ssl_meta_arch.py:378] fusing param groups
I20240906 13:06:59 14292 dinov2 param_groups.py:64] else code branch
I20240906 13:06:59 14292 dinov2 param_groups.py:87] mlp.0.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] mlp.0.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] mlp.2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] mlp.2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] mlp.4.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] mlp.4.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] last_layer.weight_g: lr_multiplier: 1.0, wd_multiplier: 1.0
I20240906 13:06:59 14292 dinov2 param_groups.py:87] last_layer.weight_v: lr_multiplier: 1.0, wd_multiplier: 1.0
I20240906 13:06:59 14292 dinov2 ssl_meta_arch.py:378] fusing param groups
I20240906 13:06:59 14292 dinov2 train.py:98] Schedulers ready.
I20240906 13:06:59 14292 dinov2 augmentations.py:34] ###################################
I20240906 13:06:59 14292 dinov2 augmentations.py:35] Using data augmentation parameters:
I20240906 13:06:59 14292 dinov2 augmentations.py:36] global_crops_scale: [0.32, 1.0]
I20240906 13:06:59 14292 dinov2 augmentations.py:37] local_crops_scale: [0.05, 0.32]
I20240906 13:06:59 14292 dinov2 augmentations.py:38] local_crops_number: 8
I20240906 13:06:59 14292 dinov2 augmentations.py:39] global_crops_size: 224
I20240906 13:06:59 14292 dinov2 augmentations.py:40] local_crops_size: 96
I20240906 13:06:59 14292 dinov2 augmentations.py:41] ###################################
I20240906 13:06:59 14292 dinov2 loaders.py:89] using dataset: "ImageNet:split=TRAIN:root=C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\datasets\ILSVRC\Data\CLS-LOC:extra=C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\datasets\ILSVRC\Data\CLS-LOC"
I20240906 13:06:59 14292 dinov2 loaders.py:94] # of dataset samples: 1,281,167
I20240906 13:06:59 14292 dinov2 loaders.py:117] sampler: infinite
I20240906 13:06:59 14292 dinov2 loaders.py:211] using PyTorch data loader
I20240906 13:06:59 14292 dinov2 loaders.py:226] infinite data loader
I20240906 13:06:59 14292 dinov2 train.py:215] Starting training from iteration 0
I20240906 13:09:17 18172 dinov2 config.py:59] git:
  sha: d43546fae6805ceb22618af8cf78469b61d913c7, status: has uncommitted changes, branch: main

I20240906 13:09:17 18172 dinov2 config.py:60] config_file: configs/train/vitl16_short.yaml
eval: 
eval_only: False
no_resume: False
opts: ['train.dataset_path=ImageNet:split=TRAIN:root=C:\\Users\\ISI_UTS\\Siladittya\\iclr2023\\inexpts\\datasets\\ILSVRC\\Data\\CLS-LOC:extra=C:\\Users\\ISI_UTS\\Siladittya\\iclr2023\\inexpts\\datasets\\ILSVRC\\Data\\CLS-LOC', 'train.output_dir=C:\\Users\\ISI_UTS\\Siladittya\\iclr2023\\inexpts\\dinov2_nosetup\\dinov2\\dinov2_nosetup\\dinov2']
output_dir: C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\dinov2_nosetup\dinov2\dinov2_nosetup\dinov2
I20240906 13:09:17 18172 dinov2 config.py:26] sqrt scaling learning rate; base: 0.004, new: 0.001
I20240906 13:09:17 18172 dinov2 config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
dino:
  loss_weight: 1.0
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
  koleo_loss_weight: 0.1
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  separate_head: false
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
train:
  batch_size_per_gpu: 64
  dataset_path: ImageNet:split=TRAIN:root=C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\datasets\ILSVRC\Data\CLS-LOC:extra=C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\datasets\ILSVRC\Data\CLS-LOC
  output_dir: C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\dinov2_nosetup\dinov2\dinov2_nosetup\dinov2
  saveckp_freq: 20
  seed: 0
  num_workers: 4
  OFFICIAL_EPOCH_LENGTH: 1250
  cache_dataset: true
  centering: centering
student:
  arch: vit_large
  patch_size: 16
  drop_path_rate: 0.3
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: mlp
  block_chunks: 4
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
  num_register_tokens: 0
  interpolate_antialias: false
  interpolate_offset: 0.1
teacher:
  momentum_teacher: 0.992
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 30
optim:
  epochs: 100
  weight_decay: 0.04
  weight_decay_end: 0.4
  base_lr: 0.004
  lr: 0.001
  warmup_epochs: 10
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 0.9
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 8
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 224
  local_crops_size: 96
evaluation:
  eval_period_iterations: 12500

I20240906 13:09:17 18172 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20240906 13:09:20 18172 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20240906 13:09:22 18172 dinov2 ssl_meta_arch.py:43] OPTIONS -- architecture : embed_dim: 1024
I20240906 13:09:22 18172 dinov2 ssl_meta_arch.py:58] OPTIONS -- DINO
I20240906 13:09:22 18172 dinov2 ssl_meta_arch.py:60] OPTIONS -- DINO -- loss_weight: 1.0
I20240906 13:09:22 18172 dinov2 ssl_meta_arch.py:61] OPTIONS -- DINO -- head_n_prototypes: 65536
I20240906 13:09:22 18172 dinov2 ssl_meta_arch.py:62] OPTIONS -- DINO -- head_bottleneck_dim: 256
I20240906 13:09:22 18172 dinov2 ssl_meta_arch.py:63] OPTIONS -- DINO -- head_hidden_dim: 2048
I20240906 13:09:22 18172 dinov2 ssl_meta_arch.py:75] OPTIONS -- DINO -- applying KOLEO regularization
I20240906 13:09:23 18172 dinov2 ssl_meta_arch.py:85] OPTIONS -- IBOT
I20240906 13:09:23 18172 dinov2 ssl_meta_arch.py:86] OPTIONS -- IBOT -- loss_weight: 1.0
I20240906 13:09:23 18172 dinov2 ssl_meta_arch.py:87] OPTIONS -- IBOT masking -- ibot_mask_ratio_tuple: [0.1, 0.5]
I20240906 13:09:23 18172 dinov2 ssl_meta_arch.py:88] OPTIONS -- IBOT masking -- ibot_mask_sample_probability: 0.5
I20240906 13:09:23 18172 dinov2 ssl_meta_arch.py:111] OPTIONS -- IBOT -- head shared with DINO
I20240906 13:09:23 18172 dinov2 ssl_meta_arch.py:121] Student and Teacher are built: they are both vit_large network.
I20240906 13:09:24 18172 dinov2 train.py:301] Model:
SSLMetaArch(
  (dino_loss): DINOLoss()
  (koleo_loss): KoLeoLoss(
    (pdist): PairwiseDistance()
  )
  (ibot_patch_loss): iBOTPatchLoss()
  (student): ModuleDict(
    (backbone): DinoVisionTransformer(
      (patch_embed): PatchEmbed(
        (proj): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))
        (norm): Identity()
      )
      (blocks): ModuleList(
        (0): BlockChunk(
          (0-5): 6 x NestedTensorBlock(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): DropPath()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): DropPath()
          )
        )
        (1): BlockChunk(
          (0-5): 6 x Identity()
          (6-11): 6 x NestedTensorBlock(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): DropPath()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): DropPath()
          )
        )
        (2): BlockChunk(
          (0-11): 12 x Identity()
          (12-17): 6 x NestedTensorBlock(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): DropPath()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): DropPath()
          )
        )
        (3): BlockChunk(
          (0-17): 18 x Identity()
          (18-23): 6 x NestedTensorBlock(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): DropPath()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): DropPath()
          )
        )
      )
      (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (head): Identity()
    )
    (dino_head): DINOHead(
      (mlp): Sequential(
        (0): Linear(in_features=1024, out_features=2048, bias=True)
        (1): GELU(approximate='none')
        (2): Linear(in_features=2048, out_features=2048, bias=True)
        (3): GELU(approximate='none')
        (4): Linear(in_features=2048, out_features=256, bias=True)
      )
      (last_layer): Linear(in_features=256, out_features=65536, bias=False)
    )
  )
  (teacher): ModuleDict(
    (backbone): DinoVisionTransformer(
      (patch_embed): PatchEmbed(
        (proj): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))
        (norm): Identity()
      )
      (blocks): ModuleList(
        (0): BlockChunk(
          (0-5): 6 x NestedTensorBlock(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): Identity()
          )
        )
        (1): BlockChunk(
          (0-5): 6 x Identity()
          (6-11): 6 x NestedTensorBlock(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): Identity()
          )
        )
        (2): BlockChunk(
          (0-11): 12 x Identity()
          (12-17): 6 x NestedTensorBlock(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): Identity()
          )
        )
        (3): BlockChunk(
          (0-17): 18 x Identity()
          (18-23): 6 x NestedTensorBlock(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): Identity()
          )
        )
      )
      (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (head): Identity()
    )
    (dino_head): DINOHead(
      (mlp): Sequential(
        (0): Linear(in_features=1024, out_features=2048, bias=True)
        (1): GELU(approximate='none')
        (2): Linear(in_features=2048, out_features=2048, bias=True)
        (3): GELU(approximate='none')
        (4): Linear(in_features=2048, out_features=256, bias=True)
      )
      (last_layer): Linear(in_features=256, out_features=65536, bias=False)
    )
  )
)
I20240906 13:09:24 18172 dinov2 param_groups.py:54] chunked fsdp
I20240906 13:09:24 18172 dinov2 param_groups.py:87] cls_token: lr_multiplier: 0.0717897987691853, wd_multiplier: 1.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] pos_embed: lr_multiplier: 0.0717897987691853, wd_multiplier: 1.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] mask_token: lr_multiplier: 0.0717897987691853, wd_multiplier: 1.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] patch_embed.proj.weight: lr_multiplier: 0.01435795975383706, wd_multiplier: 1.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] patch_embed.proj.bias: lr_multiplier: 0.01435795975383706, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.0.0.norm1.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.0.0.norm1.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.0.0.attn.qkv.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 1.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.0.0.attn.qkv.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.0.0.attn.proj.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 1.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.0.0.attn.proj.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.0.0.ls1.gamma: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.0.0.norm2.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.0.0.norm2.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.0.0.mlp.fc1.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 1.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.0.0.mlp.fc1.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.0.0.mlp.fc2.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 1.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.0.0.mlp.fc2.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.0.0.ls2.gamma: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.0.1.norm1.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.0.1.norm1.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.0.1.attn.qkv.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 1.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.0.1.attn.qkv.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.0.1.attn.proj.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 1.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.0.1.attn.proj.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.0.1.ls1.gamma: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.0.1.norm2.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.0.1.norm2.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.0.1.mlp.fc1.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 1.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.0.1.mlp.fc1.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.0.1.mlp.fc2.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 1.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.0.1.mlp.fc2.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.0.1.ls2.gamma: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.0.2.norm1.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.0.2.norm1.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.0.2.attn.qkv.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 1.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.0.2.attn.qkv.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.0.2.attn.proj.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 1.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.0.2.attn.proj.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.0.2.ls1.gamma: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.0.2.norm2.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.0.2.norm2.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.0.2.mlp.fc1.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 1.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.0.2.mlp.fc1.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.0.2.mlp.fc2.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 1.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.0.2.mlp.fc2.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.0.2.ls2.gamma: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.0.3.norm1.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.0.3.norm1.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.0.3.attn.qkv.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 1.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.0.3.attn.qkv.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.0.3.attn.proj.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 1.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.0.3.attn.proj.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.0.3.ls1.gamma: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.0.3.norm2.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.0.3.norm2.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.0.3.mlp.fc1.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 1.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.0.3.mlp.fc1.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.0.3.mlp.fc2.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 1.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.0.3.mlp.fc2.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.0.3.ls2.gamma: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.0.4.norm1.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.0.4.norm1.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.0.4.attn.qkv.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 1.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.0.4.attn.qkv.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.0.4.attn.proj.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 1.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.0.4.attn.proj.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.0.4.ls1.gamma: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.0.4.norm2.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.0.4.norm2.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.0.4.mlp.fc1.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 1.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.0.4.mlp.fc1.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.0.4.mlp.fc2.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 1.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.0.4.mlp.fc2.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.0.4.ls2.gamma: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.0.5.norm1.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.0.5.norm1.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.0.5.attn.qkv.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 1.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.0.5.attn.qkv.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.0.5.attn.proj.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 1.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.0.5.attn.proj.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.0.5.ls1.gamma: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.0.5.norm2.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.0.5.norm2.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.0.5.mlp.fc1.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 1.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.0.5.mlp.fc1.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.0.5.mlp.fc2.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 1.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.0.5.mlp.fc2.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.0.5.ls2.gamma: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.1.6.norm1.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.1.6.norm1.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.1.6.attn.qkv.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 1.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.1.6.attn.qkv.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.1.6.attn.proj.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 1.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.1.6.attn.proj.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.1.6.ls1.gamma: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.1.6.norm2.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.1.6.norm2.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.1.6.mlp.fc1.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 1.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.1.6.mlp.fc1.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.1.6.mlp.fc2.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 1.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.1.6.mlp.fc2.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.1.6.ls2.gamma: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.1.7.norm1.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.1.7.norm1.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.1.7.attn.qkv.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 1.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.1.7.attn.qkv.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.1.7.attn.proj.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 1.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.1.7.attn.proj.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.1.7.ls1.gamma: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.1.7.norm2.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.1.7.norm2.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.1.7.mlp.fc1.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 1.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.1.7.mlp.fc1.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.1.7.mlp.fc2.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 1.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.1.7.mlp.fc2.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.1.7.ls2.gamma: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.1.8.norm1.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.1.8.norm1.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.1.8.attn.qkv.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 1.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.1.8.attn.qkv.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.1.8.attn.proj.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 1.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.1.8.attn.proj.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.1.8.ls1.gamma: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.1.8.norm2.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.1.8.norm2.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.1.8.mlp.fc1.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 1.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.1.8.mlp.fc1.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.1.8.mlp.fc2.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 1.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.1.8.mlp.fc2.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.1.8.ls2.gamma: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.1.9.norm1.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.1.9.norm1.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.1.9.attn.qkv.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 1.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.1.9.attn.qkv.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.1.9.attn.proj.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 1.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.1.9.attn.proj.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.1.9.ls1.gamma: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.1.9.norm2.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.1.9.norm2.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.1.9.mlp.fc1.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 1.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.1.9.mlp.fc1.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.1.9.mlp.fc2.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 1.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.1.9.mlp.fc2.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.1.9.ls2.gamma: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.1.10.norm1.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.1.10.norm1.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.1.10.attn.qkv.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 1.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.1.10.attn.qkv.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.1.10.attn.proj.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 1.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.1.10.attn.proj.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.1.10.ls1.gamma: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.1.10.norm2.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.1.10.norm2.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.1.10.mlp.fc1.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 1.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.1.10.mlp.fc1.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.1.10.mlp.fc2.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 1.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.1.10.mlp.fc2.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.1.10.ls2.gamma: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.1.11.norm1.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.1.11.norm1.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.1.11.attn.qkv.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 1.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.1.11.attn.qkv.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.1.11.attn.proj.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 1.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.1.11.attn.proj.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.1.11.ls1.gamma: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.1.11.norm2.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.1.11.norm2.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.1.11.mlp.fc1.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 1.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.1.11.mlp.fc1.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.1.11.mlp.fc2.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 1.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.1.11.mlp.fc2.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.1.11.ls2.gamma: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.2.12.norm1.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.2.12.norm1.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.2.12.attn.qkv.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 1.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.2.12.attn.qkv.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.2.12.attn.proj.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 1.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.2.12.attn.proj.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.2.12.ls1.gamma: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.2.12.norm2.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.2.12.norm2.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.2.12.mlp.fc1.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 1.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.2.12.mlp.fc1.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.2.12.mlp.fc2.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 1.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.2.12.mlp.fc2.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.2.12.ls2.gamma: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.2.13.norm1.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.2.13.norm1.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.2.13.attn.qkv.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 1.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.2.13.attn.qkv.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.2.13.attn.proj.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 1.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.2.13.attn.proj.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.2.13.ls1.gamma: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.2.13.norm2.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.2.13.norm2.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.2.13.mlp.fc1.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 1.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.2.13.mlp.fc1.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.2.13.mlp.fc2.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 1.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.2.13.mlp.fc2.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.2.13.ls2.gamma: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.2.14.norm1.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.2.14.norm1.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.2.14.attn.qkv.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 1.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.2.14.attn.qkv.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.2.14.attn.proj.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 1.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.2.14.attn.proj.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.2.14.ls1.gamma: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.2.14.norm2.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.2.14.norm2.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.2.14.mlp.fc1.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 1.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.2.14.mlp.fc1.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.2.14.mlp.fc2.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 1.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.2.14.mlp.fc2.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.2.14.ls2.gamma: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.2.15.norm1.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.2.15.norm1.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.2.15.attn.qkv.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 1.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.2.15.attn.qkv.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.2.15.attn.proj.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 1.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.2.15.attn.proj.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.2.15.ls1.gamma: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.2.15.norm2.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.2.15.norm2.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.2.15.mlp.fc1.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 1.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.2.15.mlp.fc1.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.2.15.mlp.fc2.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 1.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.2.15.mlp.fc2.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.2.15.ls2.gamma: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.2.16.norm1.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.2.16.norm1.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.2.16.attn.qkv.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 1.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.2.16.attn.qkv.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.2.16.attn.proj.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 1.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.2.16.attn.proj.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.2.16.ls1.gamma: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.2.16.norm2.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.2.16.norm2.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.2.16.mlp.fc1.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 1.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.2.16.mlp.fc1.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.2.16.mlp.fc2.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 1.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.2.16.mlp.fc2.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.2.16.ls2.gamma: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.2.17.norm1.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.2.17.norm1.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.2.17.attn.qkv.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 1.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.2.17.attn.qkv.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.2.17.attn.proj.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 1.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.2.17.attn.proj.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.2.17.ls1.gamma: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.2.17.norm2.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.2.17.norm2.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.2.17.mlp.fc1.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 1.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.2.17.mlp.fc1.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.2.17.mlp.fc2.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 1.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.2.17.mlp.fc2.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.2.17.ls2.gamma: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.3.18.norm1.weight: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.3.18.norm1.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.3.18.attn.qkv.weight: lr_multiplier: 0.531441, wd_multiplier: 1.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.3.18.attn.qkv.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.3.18.attn.proj.weight: lr_multiplier: 0.531441, wd_multiplier: 1.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.3.18.attn.proj.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.3.18.ls1.gamma: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.3.18.norm2.weight: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.3.18.norm2.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.3.18.mlp.fc1.weight: lr_multiplier: 0.531441, wd_multiplier: 1.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.3.18.mlp.fc1.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.3.18.mlp.fc2.weight: lr_multiplier: 0.531441, wd_multiplier: 1.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.3.18.mlp.fc2.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.3.18.ls2.gamma: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.3.19.norm1.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.3.19.norm1.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.3.19.attn.qkv.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 1.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.3.19.attn.qkv.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.3.19.attn.proj.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 1.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.3.19.attn.proj.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.3.19.ls1.gamma: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.3.19.norm2.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.3.19.norm2.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.3.19.mlp.fc1.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 1.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.3.19.mlp.fc1.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.3.19.mlp.fc2.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 1.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.3.19.mlp.fc2.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.3.19.ls2.gamma: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.3.20.norm1.weight: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.3.20.norm1.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.3.20.attn.qkv.weight: lr_multiplier: 0.6561, wd_multiplier: 1.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.3.20.attn.qkv.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.3.20.attn.proj.weight: lr_multiplier: 0.6561, wd_multiplier: 1.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.3.20.attn.proj.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.3.20.ls1.gamma: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.3.20.norm2.weight: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.3.20.norm2.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.3.20.mlp.fc1.weight: lr_multiplier: 0.6561, wd_multiplier: 1.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.3.20.mlp.fc1.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.3.20.mlp.fc2.weight: lr_multiplier: 0.6561, wd_multiplier: 1.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.3.20.mlp.fc2.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.3.20.ls2.gamma: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.3.21.norm1.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.3.21.norm1.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.3.21.attn.qkv.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 1.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.3.21.attn.qkv.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.3.21.attn.proj.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 1.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.3.21.attn.proj.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.3.21.ls1.gamma: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.3.21.norm2.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.3.21.norm2.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.3.21.mlp.fc1.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 1.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.3.21.mlp.fc1.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.3.21.mlp.fc2.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 1.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.3.21.mlp.fc2.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.3.21.ls2.gamma: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.3.22.norm1.weight: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.3.22.norm1.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.3.22.attn.qkv.weight: lr_multiplier: 0.81, wd_multiplier: 1.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.3.22.attn.qkv.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.3.22.attn.proj.weight: lr_multiplier: 0.81, wd_multiplier: 1.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.3.22.attn.proj.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.3.22.ls1.gamma: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.3.22.norm2.weight: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.3.22.norm2.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.3.22.mlp.fc1.weight: lr_multiplier: 0.81, wd_multiplier: 1.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.3.22.mlp.fc1.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.3.22.mlp.fc2.weight: lr_multiplier: 0.81, wd_multiplier: 1.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.3.22.mlp.fc2.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.3.22.ls2.gamma: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.3.23.norm1.weight: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.3.23.norm1.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.3.23.attn.qkv.weight: lr_multiplier: 0.9, wd_multiplier: 1.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.3.23.attn.qkv.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.3.23.attn.proj.weight: lr_multiplier: 0.9, wd_multiplier: 1.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.3.23.attn.proj.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.3.23.ls1.gamma: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.3.23.norm2.weight: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.3.23.norm2.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.3.23.mlp.fc1.weight: lr_multiplier: 0.9, wd_multiplier: 1.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.3.23.mlp.fc1.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.3.23.mlp.fc2.weight: lr_multiplier: 0.9, wd_multiplier: 1.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.3.23.mlp.fc2.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] blocks.3.23.ls2.gamma: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] norm.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] norm.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 ssl_meta_arch.py:378] fusing param groups
I20240906 13:09:24 18172 dinov2 param_groups.py:64] else code branch
I20240906 13:09:24 18172 dinov2 param_groups.py:87] mlp.0.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] mlp.0.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] mlp.2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] mlp.2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] mlp.4.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] mlp.4.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] last_layer.weight_g: lr_multiplier: 1.0, wd_multiplier: 1.0
I20240906 13:09:24 18172 dinov2 param_groups.py:87] last_layer.weight_v: lr_multiplier: 1.0, wd_multiplier: 1.0
I20240906 13:09:24 18172 dinov2 ssl_meta_arch.py:378] fusing param groups
I20240906 13:09:24 18172 dinov2 train.py:98] Schedulers ready.
I20240906 13:09:24 18172 dinov2 augmentations.py:34] ###################################
I20240906 13:09:24 18172 dinov2 augmentations.py:35] Using data augmentation parameters:
I20240906 13:09:24 18172 dinov2 augmentations.py:36] global_crops_scale: [0.32, 1.0]
I20240906 13:09:24 18172 dinov2 augmentations.py:37] local_crops_scale: [0.05, 0.32]
I20240906 13:09:24 18172 dinov2 augmentations.py:38] local_crops_number: 8
I20240906 13:09:24 18172 dinov2 augmentations.py:39] global_crops_size: 224
I20240906 13:09:24 18172 dinov2 augmentations.py:40] local_crops_size: 96
I20240906 13:09:24 18172 dinov2 augmentations.py:41] ###################################
I20240906 13:09:24 18172 dinov2 loaders.py:89] using dataset: "ImageNet:split=TRAIN:root=C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\datasets\ILSVRC\Data\CLS-LOC:extra=C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\datasets\ILSVRC\Data\CLS-LOC"
I20240906 13:09:24 18172 dinov2 loaders.py:94] # of dataset samples: 1,281,167
I20240906 13:09:24 18172 dinov2 loaders.py:117] sampler: infinite
I20240906 13:09:24 18172 dinov2 loaders.py:211] using PyTorch data loader
I20240906 13:09:24 18172 dinov2 loaders.py:226] infinite data loader
I20240906 13:09:24 18172 dinov2 train.py:215] Starting training from iteration 0
I20240906 13:09:39 15640 dinov2 config.py:59] git:
  sha: d43546fae6805ceb22618af8cf78469b61d913c7, status: has uncommitted changes, branch: main

I20240906 13:09:39 15640 dinov2 config.py:60] config_file: configs/train/vitl16_short.yaml
eval: 
eval_only: False
no_resume: False
opts: ['train.dataset_path=ImageNet:split=TRAIN:root=C:\\Users\\ISI_UTS\\Siladittya\\iclr2023\\inexpts\\datasets\\ILSVRC\\Data\\CLS-LOC:extra=C:\\Users\\ISI_UTS\\Siladittya\\iclr2023\\inexpts\\datasets\\ILSVRC\\Data\\CLS-LOC', 'train.output_dir=C:\\Users\\ISI_UTS\\Siladittya\\iclr2023\\inexpts\\dinov2_nosetup\\dinov2\\dinov2_nosetup\\dinov2']
output_dir: C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\dinov2_nosetup\dinov2\dinov2_nosetup\dinov2
I20240906 13:09:39 15640 dinov2 config.py:26] sqrt scaling learning rate; base: 0.004, new: 0.001
I20240906 13:09:39 15640 dinov2 config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
dino:
  loss_weight: 1.0
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
  koleo_loss_weight: 0.1
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  separate_head: false
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
train:
  batch_size_per_gpu: 64
  dataset_path: ImageNet:split=TRAIN:root=C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\datasets\ILSVRC\Data\CLS-LOC:extra=C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\datasets\ILSVRC\Data\CLS-LOC
  output_dir: C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\dinov2_nosetup\dinov2\dinov2_nosetup\dinov2
  saveckp_freq: 20
  seed: 0
  num_workers: 0
  OFFICIAL_EPOCH_LENGTH: 1250
  cache_dataset: true
  centering: centering
student:
  arch: vit_large
  patch_size: 16
  drop_path_rate: 0.3
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: mlp
  block_chunks: 4
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
  num_register_tokens: 0
  interpolate_antialias: false
  interpolate_offset: 0.1
teacher:
  momentum_teacher: 0.992
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 30
optim:
  epochs: 100
  weight_decay: 0.04
  weight_decay_end: 0.4
  base_lr: 0.004
  lr: 0.001
  warmup_epochs: 10
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 0.9
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 8
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 224
  local_crops_size: 96
evaluation:
  eval_period_iterations: 12500

I20240906 13:09:39 15640 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20240906 13:09:42 15640 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20240906 13:09:44 15640 dinov2 ssl_meta_arch.py:43] OPTIONS -- architecture : embed_dim: 1024
I20240906 13:09:44 15640 dinov2 ssl_meta_arch.py:58] OPTIONS -- DINO
I20240906 13:09:44 15640 dinov2 ssl_meta_arch.py:60] OPTIONS -- DINO -- loss_weight: 1.0
I20240906 13:09:44 15640 dinov2 ssl_meta_arch.py:61] OPTIONS -- DINO -- head_n_prototypes: 65536
I20240906 13:09:44 15640 dinov2 ssl_meta_arch.py:62] OPTIONS -- DINO -- head_bottleneck_dim: 256
I20240906 13:09:44 15640 dinov2 ssl_meta_arch.py:63] OPTIONS -- DINO -- head_hidden_dim: 2048
I20240906 13:09:44 15640 dinov2 ssl_meta_arch.py:75] OPTIONS -- DINO -- applying KOLEO regularization
I20240906 13:09:45 15640 dinov2 ssl_meta_arch.py:85] OPTIONS -- IBOT
I20240906 13:09:45 15640 dinov2 ssl_meta_arch.py:86] OPTIONS -- IBOT -- loss_weight: 1.0
I20240906 13:09:45 15640 dinov2 ssl_meta_arch.py:87] OPTIONS -- IBOT masking -- ibot_mask_ratio_tuple: [0.1, 0.5]
I20240906 13:09:45 15640 dinov2 ssl_meta_arch.py:88] OPTIONS -- IBOT masking -- ibot_mask_sample_probability: 0.5
I20240906 13:09:45 15640 dinov2 ssl_meta_arch.py:111] OPTIONS -- IBOT -- head shared with DINO
I20240906 13:09:45 15640 dinov2 ssl_meta_arch.py:121] Student and Teacher are built: they are both vit_large network.
I20240906 13:09:46 15640 dinov2 train.py:301] Model:
SSLMetaArch(
  (dino_loss): DINOLoss()
  (koleo_loss): KoLeoLoss(
    (pdist): PairwiseDistance()
  )
  (ibot_patch_loss): iBOTPatchLoss()
  (student): ModuleDict(
    (backbone): DinoVisionTransformer(
      (patch_embed): PatchEmbed(
        (proj): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))
        (norm): Identity()
      )
      (blocks): ModuleList(
        (0): BlockChunk(
          (0-5): 6 x NestedTensorBlock(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): DropPath()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): DropPath()
          )
        )
        (1): BlockChunk(
          (0-5): 6 x Identity()
          (6-11): 6 x NestedTensorBlock(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): DropPath()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): DropPath()
          )
        )
        (2): BlockChunk(
          (0-11): 12 x Identity()
          (12-17): 6 x NestedTensorBlock(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): DropPath()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): DropPath()
          )
        )
        (3): BlockChunk(
          (0-17): 18 x Identity()
          (18-23): 6 x NestedTensorBlock(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): DropPath()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): DropPath()
          )
        )
      )
      (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (head): Identity()
    )
    (dino_head): DINOHead(
      (mlp): Sequential(
        (0): Linear(in_features=1024, out_features=2048, bias=True)
        (1): GELU(approximate='none')
        (2): Linear(in_features=2048, out_features=2048, bias=True)
        (3): GELU(approximate='none')
        (4): Linear(in_features=2048, out_features=256, bias=True)
      )
      (last_layer): Linear(in_features=256, out_features=65536, bias=False)
    )
  )
  (teacher): ModuleDict(
    (backbone): DinoVisionTransformer(
      (patch_embed): PatchEmbed(
        (proj): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))
        (norm): Identity()
      )
      (blocks): ModuleList(
        (0): BlockChunk(
          (0-5): 6 x NestedTensorBlock(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): Identity()
          )
        )
        (1): BlockChunk(
          (0-5): 6 x Identity()
          (6-11): 6 x NestedTensorBlock(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): Identity()
          )
        )
        (2): BlockChunk(
          (0-11): 12 x Identity()
          (12-17): 6 x NestedTensorBlock(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): Identity()
          )
        )
        (3): BlockChunk(
          (0-17): 18 x Identity()
          (18-23): 6 x NestedTensorBlock(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): Identity()
          )
        )
      )
      (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (head): Identity()
    )
    (dino_head): DINOHead(
      (mlp): Sequential(
        (0): Linear(in_features=1024, out_features=2048, bias=True)
        (1): GELU(approximate='none')
        (2): Linear(in_features=2048, out_features=2048, bias=True)
        (3): GELU(approximate='none')
        (4): Linear(in_features=2048, out_features=256, bias=True)
      )
      (last_layer): Linear(in_features=256, out_features=65536, bias=False)
    )
  )
)
I20240906 13:09:46 15640 dinov2 param_groups.py:54] chunked fsdp
I20240906 13:09:46 15640 dinov2 param_groups.py:87] cls_token: lr_multiplier: 0.0717897987691853, wd_multiplier: 1.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] pos_embed: lr_multiplier: 0.0717897987691853, wd_multiplier: 1.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] mask_token: lr_multiplier: 0.0717897987691853, wd_multiplier: 1.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] patch_embed.proj.weight: lr_multiplier: 0.01435795975383706, wd_multiplier: 1.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] patch_embed.proj.bias: lr_multiplier: 0.01435795975383706, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.0.0.norm1.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.0.0.norm1.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.0.0.attn.qkv.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 1.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.0.0.attn.qkv.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.0.0.attn.proj.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 1.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.0.0.attn.proj.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.0.0.ls1.gamma: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.0.0.norm2.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.0.0.norm2.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.0.0.mlp.fc1.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 1.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.0.0.mlp.fc1.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.0.0.mlp.fc2.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 1.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.0.0.mlp.fc2.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.0.0.ls2.gamma: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.0.1.norm1.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.0.1.norm1.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.0.1.attn.qkv.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 1.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.0.1.attn.qkv.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.0.1.attn.proj.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 1.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.0.1.attn.proj.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.0.1.ls1.gamma: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.0.1.norm2.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.0.1.norm2.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.0.1.mlp.fc1.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 1.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.0.1.mlp.fc1.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.0.1.mlp.fc2.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 1.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.0.1.mlp.fc2.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.0.1.ls2.gamma: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.0.2.norm1.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.0.2.norm1.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.0.2.attn.qkv.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 1.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.0.2.attn.qkv.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.0.2.attn.proj.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 1.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.0.2.attn.proj.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.0.2.ls1.gamma: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.0.2.norm2.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.0.2.norm2.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.0.2.mlp.fc1.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 1.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.0.2.mlp.fc1.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.0.2.mlp.fc2.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 1.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.0.2.mlp.fc2.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.0.2.ls2.gamma: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.0.3.norm1.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.0.3.norm1.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.0.3.attn.qkv.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 1.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.0.3.attn.qkv.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.0.3.attn.proj.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 1.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.0.3.attn.proj.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.0.3.ls1.gamma: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.0.3.norm2.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.0.3.norm2.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.0.3.mlp.fc1.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 1.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.0.3.mlp.fc1.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.0.3.mlp.fc2.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 1.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.0.3.mlp.fc2.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.0.3.ls2.gamma: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.0.4.norm1.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.0.4.norm1.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.0.4.attn.qkv.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 1.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.0.4.attn.qkv.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.0.4.attn.proj.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 1.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.0.4.attn.proj.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.0.4.ls1.gamma: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.0.4.norm2.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.0.4.norm2.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.0.4.mlp.fc1.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 1.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.0.4.mlp.fc1.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.0.4.mlp.fc2.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 1.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.0.4.mlp.fc2.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.0.4.ls2.gamma: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.0.5.norm1.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.0.5.norm1.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.0.5.attn.qkv.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 1.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.0.5.attn.qkv.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.0.5.attn.proj.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 1.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.0.5.attn.proj.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.0.5.ls1.gamma: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.0.5.norm2.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.0.5.norm2.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.0.5.mlp.fc1.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 1.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.0.5.mlp.fc1.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.0.5.mlp.fc2.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 1.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.0.5.mlp.fc2.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.0.5.ls2.gamma: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.1.6.norm1.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.1.6.norm1.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.1.6.attn.qkv.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 1.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.1.6.attn.qkv.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.1.6.attn.proj.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 1.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.1.6.attn.proj.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.1.6.ls1.gamma: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.1.6.norm2.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.1.6.norm2.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.1.6.mlp.fc1.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 1.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.1.6.mlp.fc1.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.1.6.mlp.fc2.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 1.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.1.6.mlp.fc2.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.1.6.ls2.gamma: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.1.7.norm1.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.1.7.norm1.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.1.7.attn.qkv.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 1.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.1.7.attn.qkv.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.1.7.attn.proj.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 1.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.1.7.attn.proj.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.1.7.ls1.gamma: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.1.7.norm2.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.1.7.norm2.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.1.7.mlp.fc1.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 1.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.1.7.mlp.fc1.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.1.7.mlp.fc2.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 1.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.1.7.mlp.fc2.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.1.7.ls2.gamma: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.1.8.norm1.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.1.8.norm1.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.1.8.attn.qkv.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 1.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.1.8.attn.qkv.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.1.8.attn.proj.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 1.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.1.8.attn.proj.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.1.8.ls1.gamma: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.1.8.norm2.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.1.8.norm2.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.1.8.mlp.fc1.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 1.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.1.8.mlp.fc1.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.1.8.mlp.fc2.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 1.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.1.8.mlp.fc2.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.1.8.ls2.gamma: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.1.9.norm1.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.1.9.norm1.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.1.9.attn.qkv.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 1.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.1.9.attn.qkv.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.1.9.attn.proj.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 1.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.1.9.attn.proj.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.1.9.ls1.gamma: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.1.9.norm2.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.1.9.norm2.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.1.9.mlp.fc1.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 1.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.1.9.mlp.fc1.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.1.9.mlp.fc2.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 1.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.1.9.mlp.fc2.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.1.9.ls2.gamma: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.1.10.norm1.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.1.10.norm1.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.1.10.attn.qkv.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 1.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.1.10.attn.qkv.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.1.10.attn.proj.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 1.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.1.10.attn.proj.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.1.10.ls1.gamma: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.1.10.norm2.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.1.10.norm2.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.1.10.mlp.fc1.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 1.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.1.10.mlp.fc1.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.1.10.mlp.fc2.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 1.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.1.10.mlp.fc2.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.1.10.ls2.gamma: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.1.11.norm1.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.1.11.norm1.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.1.11.attn.qkv.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 1.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.1.11.attn.qkv.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.1.11.attn.proj.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 1.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.1.11.attn.proj.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.1.11.ls1.gamma: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.1.11.norm2.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.1.11.norm2.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.1.11.mlp.fc1.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 1.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.1.11.mlp.fc1.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.1.11.mlp.fc2.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 1.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.1.11.mlp.fc2.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.1.11.ls2.gamma: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.2.12.norm1.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.2.12.norm1.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.2.12.attn.qkv.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 1.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.2.12.attn.qkv.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.2.12.attn.proj.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 1.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.2.12.attn.proj.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.2.12.ls1.gamma: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.2.12.norm2.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.2.12.norm2.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.2.12.mlp.fc1.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 1.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.2.12.mlp.fc1.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.2.12.mlp.fc2.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 1.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.2.12.mlp.fc2.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.2.12.ls2.gamma: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.2.13.norm1.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.2.13.norm1.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.2.13.attn.qkv.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 1.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.2.13.attn.qkv.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.2.13.attn.proj.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 1.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.2.13.attn.proj.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.2.13.ls1.gamma: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.2.13.norm2.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.2.13.norm2.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.2.13.mlp.fc1.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 1.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.2.13.mlp.fc1.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.2.13.mlp.fc2.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 1.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.2.13.mlp.fc2.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.2.13.ls2.gamma: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.2.14.norm1.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.2.14.norm1.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.2.14.attn.qkv.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 1.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.2.14.attn.qkv.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.2.14.attn.proj.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 1.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.2.14.attn.proj.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.2.14.ls1.gamma: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.2.14.norm2.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.2.14.norm2.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.2.14.mlp.fc1.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 1.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.2.14.mlp.fc1.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.2.14.mlp.fc2.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 1.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.2.14.mlp.fc2.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.2.14.ls2.gamma: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.2.15.norm1.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.2.15.norm1.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.2.15.attn.qkv.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 1.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.2.15.attn.qkv.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.2.15.attn.proj.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 1.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.2.15.attn.proj.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.2.15.ls1.gamma: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.2.15.norm2.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.2.15.norm2.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.2.15.mlp.fc1.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 1.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.2.15.mlp.fc1.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.2.15.mlp.fc2.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 1.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.2.15.mlp.fc2.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.2.15.ls2.gamma: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.2.16.norm1.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.2.16.norm1.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.2.16.attn.qkv.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 1.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.2.16.attn.qkv.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.2.16.attn.proj.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 1.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.2.16.attn.proj.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.2.16.ls1.gamma: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.2.16.norm2.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.2.16.norm2.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.2.16.mlp.fc1.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 1.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.2.16.mlp.fc1.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.2.16.mlp.fc2.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 1.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.2.16.mlp.fc2.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.2.16.ls2.gamma: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.2.17.norm1.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.2.17.norm1.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.2.17.attn.qkv.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 1.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.2.17.attn.qkv.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.2.17.attn.proj.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 1.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.2.17.attn.proj.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.2.17.ls1.gamma: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.2.17.norm2.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.2.17.norm2.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.2.17.mlp.fc1.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 1.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.2.17.mlp.fc1.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.2.17.mlp.fc2.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 1.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.2.17.mlp.fc2.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.2.17.ls2.gamma: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.3.18.norm1.weight: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.3.18.norm1.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.3.18.attn.qkv.weight: lr_multiplier: 0.531441, wd_multiplier: 1.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.3.18.attn.qkv.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.3.18.attn.proj.weight: lr_multiplier: 0.531441, wd_multiplier: 1.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.3.18.attn.proj.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.3.18.ls1.gamma: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.3.18.norm2.weight: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.3.18.norm2.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.3.18.mlp.fc1.weight: lr_multiplier: 0.531441, wd_multiplier: 1.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.3.18.mlp.fc1.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.3.18.mlp.fc2.weight: lr_multiplier: 0.531441, wd_multiplier: 1.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.3.18.mlp.fc2.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.3.18.ls2.gamma: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.3.19.norm1.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.3.19.norm1.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.3.19.attn.qkv.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 1.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.3.19.attn.qkv.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.3.19.attn.proj.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 1.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.3.19.attn.proj.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.3.19.ls1.gamma: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.3.19.norm2.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.3.19.norm2.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.3.19.mlp.fc1.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 1.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.3.19.mlp.fc1.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.3.19.mlp.fc2.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 1.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.3.19.mlp.fc2.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.3.19.ls2.gamma: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.3.20.norm1.weight: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.3.20.norm1.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.3.20.attn.qkv.weight: lr_multiplier: 0.6561, wd_multiplier: 1.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.3.20.attn.qkv.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.3.20.attn.proj.weight: lr_multiplier: 0.6561, wd_multiplier: 1.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.3.20.attn.proj.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.3.20.ls1.gamma: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.3.20.norm2.weight: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.3.20.norm2.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.3.20.mlp.fc1.weight: lr_multiplier: 0.6561, wd_multiplier: 1.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.3.20.mlp.fc1.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.3.20.mlp.fc2.weight: lr_multiplier: 0.6561, wd_multiplier: 1.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.3.20.mlp.fc2.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.3.20.ls2.gamma: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.3.21.norm1.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.3.21.norm1.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.3.21.attn.qkv.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 1.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.3.21.attn.qkv.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.3.21.attn.proj.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 1.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.3.21.attn.proj.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.3.21.ls1.gamma: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.3.21.norm2.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.3.21.norm2.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.3.21.mlp.fc1.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 1.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.3.21.mlp.fc1.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.3.21.mlp.fc2.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 1.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.3.21.mlp.fc2.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.3.21.ls2.gamma: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.3.22.norm1.weight: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.3.22.norm1.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.3.22.attn.qkv.weight: lr_multiplier: 0.81, wd_multiplier: 1.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.3.22.attn.qkv.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.3.22.attn.proj.weight: lr_multiplier: 0.81, wd_multiplier: 1.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.3.22.attn.proj.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.3.22.ls1.gamma: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.3.22.norm2.weight: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.3.22.norm2.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.3.22.mlp.fc1.weight: lr_multiplier: 0.81, wd_multiplier: 1.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.3.22.mlp.fc1.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.3.22.mlp.fc2.weight: lr_multiplier: 0.81, wd_multiplier: 1.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.3.22.mlp.fc2.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.3.22.ls2.gamma: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.3.23.norm1.weight: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.3.23.norm1.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.3.23.attn.qkv.weight: lr_multiplier: 0.9, wd_multiplier: 1.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.3.23.attn.qkv.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.3.23.attn.proj.weight: lr_multiplier: 0.9, wd_multiplier: 1.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.3.23.attn.proj.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.3.23.ls1.gamma: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.3.23.norm2.weight: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.3.23.norm2.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.3.23.mlp.fc1.weight: lr_multiplier: 0.9, wd_multiplier: 1.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.3.23.mlp.fc1.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.3.23.mlp.fc2.weight: lr_multiplier: 0.9, wd_multiplier: 1.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.3.23.mlp.fc2.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] blocks.3.23.ls2.gamma: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] norm.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] norm.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 ssl_meta_arch.py:378] fusing param groups
I20240906 13:09:46 15640 dinov2 param_groups.py:64] else code branch
I20240906 13:09:46 15640 dinov2 param_groups.py:87] mlp.0.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] mlp.0.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] mlp.2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] mlp.2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] mlp.4.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] mlp.4.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] last_layer.weight_g: lr_multiplier: 1.0, wd_multiplier: 1.0
I20240906 13:09:46 15640 dinov2 param_groups.py:87] last_layer.weight_v: lr_multiplier: 1.0, wd_multiplier: 1.0
I20240906 13:09:46 15640 dinov2 ssl_meta_arch.py:378] fusing param groups
I20240906 13:09:46 15640 dinov2 train.py:98] Schedulers ready.
I20240906 13:09:46 15640 dinov2 augmentations.py:34] ###################################
I20240906 13:09:46 15640 dinov2 augmentations.py:35] Using data augmentation parameters:
I20240906 13:09:46 15640 dinov2 augmentations.py:36] global_crops_scale: [0.32, 1.0]
I20240906 13:09:46 15640 dinov2 augmentations.py:37] local_crops_scale: [0.05, 0.32]
I20240906 13:09:46 15640 dinov2 augmentations.py:38] local_crops_number: 8
I20240906 13:09:46 15640 dinov2 augmentations.py:39] global_crops_size: 224
I20240906 13:09:46 15640 dinov2 augmentations.py:40] local_crops_size: 96
I20240906 13:09:46 15640 dinov2 augmentations.py:41] ###################################
I20240906 13:09:46 15640 dinov2 loaders.py:89] using dataset: "ImageNet:split=TRAIN:root=C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\datasets\ILSVRC\Data\CLS-LOC:extra=C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\datasets\ILSVRC\Data\CLS-LOC"
I20240906 13:09:46 15640 dinov2 loaders.py:94] # of dataset samples: 1,281,167
I20240906 13:09:46 15640 dinov2 loaders.py:117] sampler: infinite
I20240906 13:09:46 15640 dinov2 loaders.py:211] using PyTorch data loader
I20240906 13:09:46 15640 dinov2 loaders.py:226] infinite data loader
I20240906 13:09:46 15640 dinov2 train.py:215] Starting training from iteration 0
I20240906 13:14:26 10544 dinov2 config.py:59] git:
  sha: d43546fae6805ceb22618af8cf78469b61d913c7, status: has uncommitted changes, branch: main

I20240906 13:14:26 10544 dinov2 config.py:60] config_file: configs/train/vitl16_short.yaml
eval: 
eval_only: False
no_resume: False
opts: ['train.dataset_path=ImageNet:split=TRAIN:root=C:\\Users\\ISI_UTS\\Siladittya\\iclr2023\\inexpts\\datasets\\ILSVRC\\Data\\CLS-LOC:extra=C:\\Users\\ISI_UTS\\Siladittya\\iclr2023\\inexpts\\datasets\\ILSVRC\\Data\\CLS-LOC', 'train.output_dir=C:\\Users\\ISI_UTS\\Siladittya\\iclr2023\\inexpts\\dinov2_nosetup\\dinov2\\dinov2_nosetup\\dinov2']
output_dir: C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\dinov2_nosetup\dinov2\dinov2_nosetup\dinov2
I20240906 13:14:26 10544 dinov2 config.py:26] sqrt scaling learning rate; base: 0.004, new: 0.001
I20240906 13:14:26 10544 dinov2 config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
dino:
  loss_weight: 1.0
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
  koleo_loss_weight: 0.1
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  separate_head: false
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
train:
  batch_size_per_gpu: 64
  dataset_path: ImageNet:split=TRAIN:root=C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\datasets\ILSVRC\Data\CLS-LOC:extra=C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\datasets\ILSVRC\Data\CLS-LOC
  output_dir: C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\dinov2_nosetup\dinov2\dinov2_nosetup\dinov2
  saveckp_freq: 20
  seed: 0
  num_workers: 0
  OFFICIAL_EPOCH_LENGTH: 1250
  cache_dataset: true
  centering: centering
student:
  arch: vit_large
  patch_size: 16
  drop_path_rate: 0.3
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: mlp
  block_chunks: 4
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
  num_register_tokens: 0
  interpolate_antialias: false
  interpolate_offset: 0.1
teacher:
  momentum_teacher: 0.992
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 30
optim:
  epochs: 100
  weight_decay: 0.04
  weight_decay_end: 0.4
  base_lr: 0.004
  lr: 0.001
  warmup_epochs: 10
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 0.9
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 8
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 224
  local_crops_size: 96
evaluation:
  eval_period_iterations: 12500

I20240906 13:14:26 10544 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20240906 13:14:29 10544 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20240906 13:14:31 10544 dinov2 ssl_meta_arch.py:43] OPTIONS -- architecture : embed_dim: 1024
I20240906 13:14:31 10544 dinov2 ssl_meta_arch.py:58] OPTIONS -- DINO
I20240906 13:14:31 10544 dinov2 ssl_meta_arch.py:60] OPTIONS -- DINO -- loss_weight: 1.0
I20240906 13:14:31 10544 dinov2 ssl_meta_arch.py:61] OPTIONS -- DINO -- head_n_prototypes: 65536
I20240906 13:14:31 10544 dinov2 ssl_meta_arch.py:62] OPTIONS -- DINO -- head_bottleneck_dim: 256
I20240906 13:14:31 10544 dinov2 ssl_meta_arch.py:63] OPTIONS -- DINO -- head_hidden_dim: 2048
I20240906 13:14:31 10544 dinov2 ssl_meta_arch.py:75] OPTIONS -- DINO -- applying KOLEO regularization
I20240906 13:14:32 10544 dinov2 ssl_meta_arch.py:85] OPTIONS -- IBOT
I20240906 13:14:32 10544 dinov2 ssl_meta_arch.py:86] OPTIONS -- IBOT -- loss_weight: 1.0
I20240906 13:14:32 10544 dinov2 ssl_meta_arch.py:87] OPTIONS -- IBOT masking -- ibot_mask_ratio_tuple: [0.1, 0.5]
I20240906 13:14:32 10544 dinov2 ssl_meta_arch.py:88] OPTIONS -- IBOT masking -- ibot_mask_sample_probability: 0.5
I20240906 13:14:32 10544 dinov2 ssl_meta_arch.py:111] OPTIONS -- IBOT -- head shared with DINO
I20240906 13:14:32 10544 dinov2 ssl_meta_arch.py:121] Student and Teacher are built: they are both vit_large network.
I20240906 13:14:32 10544 dinov2 train.py:301] Model:
SSLMetaArch(
  (dino_loss): DINOLoss()
  (koleo_loss): KoLeoLoss(
    (pdist): PairwiseDistance()
  )
  (ibot_patch_loss): iBOTPatchLoss()
  (student): ModuleDict(
    (backbone): DinoVisionTransformer(
      (patch_embed): PatchEmbed(
        (proj): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))
        (norm): Identity()
      )
      (blocks): ModuleList(
        (0): BlockChunk(
          (0-5): 6 x NestedTensorBlock(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): DropPath()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): DropPath()
          )
        )
        (1): BlockChunk(
          (0-5): 6 x Identity()
          (6-11): 6 x NestedTensorBlock(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): DropPath()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): DropPath()
          )
        )
        (2): BlockChunk(
          (0-11): 12 x Identity()
          (12-17): 6 x NestedTensorBlock(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): DropPath()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): DropPath()
          )
        )
        (3): BlockChunk(
          (0-17): 18 x Identity()
          (18-23): 6 x NestedTensorBlock(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): DropPath()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): DropPath()
          )
        )
      )
      (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (head): Identity()
    )
    (dino_head): DINOHead(
      (mlp): Sequential(
        (0): Linear(in_features=1024, out_features=2048, bias=True)
        (1): GELU(approximate='none')
        (2): Linear(in_features=2048, out_features=2048, bias=True)
        (3): GELU(approximate='none')
        (4): Linear(in_features=2048, out_features=256, bias=True)
      )
      (last_layer): Linear(in_features=256, out_features=65536, bias=False)
    )
  )
  (teacher): ModuleDict(
    (backbone): DinoVisionTransformer(
      (patch_embed): PatchEmbed(
        (proj): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))
        (norm): Identity()
      )
      (blocks): ModuleList(
        (0): BlockChunk(
          (0-5): 6 x NestedTensorBlock(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): Identity()
          )
        )
        (1): BlockChunk(
          (0-5): 6 x Identity()
          (6-11): 6 x NestedTensorBlock(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): Identity()
          )
        )
        (2): BlockChunk(
          (0-11): 12 x Identity()
          (12-17): 6 x NestedTensorBlock(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): Identity()
          )
        )
        (3): BlockChunk(
          (0-17): 18 x Identity()
          (18-23): 6 x NestedTensorBlock(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): Identity()
          )
        )
      )
      (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (head): Identity()
    )
    (dino_head): DINOHead(
      (mlp): Sequential(
        (0): Linear(in_features=1024, out_features=2048, bias=True)
        (1): GELU(approximate='none')
        (2): Linear(in_features=2048, out_features=2048, bias=True)
        (3): GELU(approximate='none')
        (4): Linear(in_features=2048, out_features=256, bias=True)
      )
      (last_layer): Linear(in_features=256, out_features=65536, bias=False)
    )
  )
)
I20240906 13:14:32 10544 dinov2 param_groups.py:54] chunked fsdp
I20240906 13:14:32 10544 dinov2 param_groups.py:87] cls_token: lr_multiplier: 0.0717897987691853, wd_multiplier: 1.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] pos_embed: lr_multiplier: 0.0717897987691853, wd_multiplier: 1.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] mask_token: lr_multiplier: 0.0717897987691853, wd_multiplier: 1.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] patch_embed.proj.weight: lr_multiplier: 0.01435795975383706, wd_multiplier: 1.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] patch_embed.proj.bias: lr_multiplier: 0.01435795975383706, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.0.0.norm1.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.0.0.norm1.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.0.0.attn.qkv.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 1.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.0.0.attn.qkv.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.0.0.attn.proj.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 1.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.0.0.attn.proj.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.0.0.ls1.gamma: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.0.0.norm2.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.0.0.norm2.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.0.0.mlp.fc1.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 1.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.0.0.mlp.fc1.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.0.0.mlp.fc2.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 1.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.0.0.mlp.fc2.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.0.0.ls2.gamma: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.0.1.norm1.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.0.1.norm1.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.0.1.attn.qkv.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 1.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.0.1.attn.qkv.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.0.1.attn.proj.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 1.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.0.1.attn.proj.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.0.1.ls1.gamma: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.0.1.norm2.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.0.1.norm2.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.0.1.mlp.fc1.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 1.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.0.1.mlp.fc1.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.0.1.mlp.fc2.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 1.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.0.1.mlp.fc2.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.0.1.ls2.gamma: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.0.2.norm1.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.0.2.norm1.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.0.2.attn.qkv.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 1.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.0.2.attn.qkv.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.0.2.attn.proj.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 1.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.0.2.attn.proj.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.0.2.ls1.gamma: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.0.2.norm2.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.0.2.norm2.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.0.2.mlp.fc1.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 1.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.0.2.mlp.fc1.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.0.2.mlp.fc2.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 1.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.0.2.mlp.fc2.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.0.2.ls2.gamma: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.0.3.norm1.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.0.3.norm1.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.0.3.attn.qkv.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 1.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.0.3.attn.qkv.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.0.3.attn.proj.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 1.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.0.3.attn.proj.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.0.3.ls1.gamma: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.0.3.norm2.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.0.3.norm2.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.0.3.mlp.fc1.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 1.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.0.3.mlp.fc1.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.0.3.mlp.fc2.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 1.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.0.3.mlp.fc2.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.0.3.ls2.gamma: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.0.4.norm1.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.0.4.norm1.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.0.4.attn.qkv.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 1.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.0.4.attn.qkv.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.0.4.attn.proj.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 1.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.0.4.attn.proj.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.0.4.ls1.gamma: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.0.4.norm2.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.0.4.norm2.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.0.4.mlp.fc1.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 1.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.0.4.mlp.fc1.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.0.4.mlp.fc2.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 1.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.0.4.mlp.fc2.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.0.4.ls2.gamma: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.0.5.norm1.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.0.5.norm1.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.0.5.attn.qkv.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 1.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.0.5.attn.qkv.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.0.5.attn.proj.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 1.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.0.5.attn.proj.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.0.5.ls1.gamma: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.0.5.norm2.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.0.5.norm2.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.0.5.mlp.fc1.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 1.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.0.5.mlp.fc1.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.0.5.mlp.fc2.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 1.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.0.5.mlp.fc2.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.0.5.ls2.gamma: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.1.6.norm1.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.1.6.norm1.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.1.6.attn.qkv.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 1.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.1.6.attn.qkv.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.1.6.attn.proj.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 1.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.1.6.attn.proj.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.1.6.ls1.gamma: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.1.6.norm2.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.1.6.norm2.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.1.6.mlp.fc1.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 1.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.1.6.mlp.fc1.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.1.6.mlp.fc2.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 1.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.1.6.mlp.fc2.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.1.6.ls2.gamma: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.1.7.norm1.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.1.7.norm1.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.1.7.attn.qkv.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 1.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.1.7.attn.qkv.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.1.7.attn.proj.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 1.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.1.7.attn.proj.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.1.7.ls1.gamma: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.1.7.norm2.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.1.7.norm2.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.1.7.mlp.fc1.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 1.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.1.7.mlp.fc1.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.1.7.mlp.fc2.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 1.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.1.7.mlp.fc2.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.1.7.ls2.gamma: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.1.8.norm1.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.1.8.norm1.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.1.8.attn.qkv.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 1.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.1.8.attn.qkv.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.1.8.attn.proj.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 1.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.1.8.attn.proj.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.1.8.ls1.gamma: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.1.8.norm2.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.1.8.norm2.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.1.8.mlp.fc1.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 1.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.1.8.mlp.fc1.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.1.8.mlp.fc2.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 1.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.1.8.mlp.fc2.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.1.8.ls2.gamma: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.1.9.norm1.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.1.9.norm1.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.1.9.attn.qkv.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 1.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.1.9.attn.qkv.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.1.9.attn.proj.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 1.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.1.9.attn.proj.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.1.9.ls1.gamma: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.1.9.norm2.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.1.9.norm2.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.1.9.mlp.fc1.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 1.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.1.9.mlp.fc1.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.1.9.mlp.fc2.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 1.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.1.9.mlp.fc2.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.1.9.ls2.gamma: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.1.10.norm1.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.1.10.norm1.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.1.10.attn.qkv.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 1.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.1.10.attn.qkv.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.1.10.attn.proj.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 1.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.1.10.attn.proj.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.1.10.ls1.gamma: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.1.10.norm2.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.1.10.norm2.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.1.10.mlp.fc1.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 1.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.1.10.mlp.fc1.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.1.10.mlp.fc2.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 1.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.1.10.mlp.fc2.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.1.10.ls2.gamma: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.1.11.norm1.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.1.11.norm1.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.1.11.attn.qkv.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 1.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.1.11.attn.qkv.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.1.11.attn.proj.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 1.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.1.11.attn.proj.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.1.11.ls1.gamma: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.1.11.norm2.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.1.11.norm2.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.1.11.mlp.fc1.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 1.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.1.11.mlp.fc1.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.1.11.mlp.fc2.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 1.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.1.11.mlp.fc2.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.1.11.ls2.gamma: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.2.12.norm1.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.2.12.norm1.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.2.12.attn.qkv.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 1.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.2.12.attn.qkv.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.2.12.attn.proj.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 1.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.2.12.attn.proj.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.2.12.ls1.gamma: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.2.12.norm2.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.2.12.norm2.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.2.12.mlp.fc1.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 1.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.2.12.mlp.fc1.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.2.12.mlp.fc2.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 1.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.2.12.mlp.fc2.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.2.12.ls2.gamma: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.2.13.norm1.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.2.13.norm1.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.2.13.attn.qkv.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 1.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.2.13.attn.qkv.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.2.13.attn.proj.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 1.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.2.13.attn.proj.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.2.13.ls1.gamma: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.2.13.norm2.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.2.13.norm2.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.2.13.mlp.fc1.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 1.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.2.13.mlp.fc1.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.2.13.mlp.fc2.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 1.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.2.13.mlp.fc2.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.2.13.ls2.gamma: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.2.14.norm1.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.2.14.norm1.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.2.14.attn.qkv.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 1.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.2.14.attn.qkv.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.2.14.attn.proj.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 1.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.2.14.attn.proj.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.2.14.ls1.gamma: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.2.14.norm2.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.2.14.norm2.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.2.14.mlp.fc1.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 1.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.2.14.mlp.fc1.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.2.14.mlp.fc2.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 1.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.2.14.mlp.fc2.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.2.14.ls2.gamma: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.2.15.norm1.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.2.15.norm1.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.2.15.attn.qkv.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 1.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.2.15.attn.qkv.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.2.15.attn.proj.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 1.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.2.15.attn.proj.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.2.15.ls1.gamma: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.2.15.norm2.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.2.15.norm2.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.2.15.mlp.fc1.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 1.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.2.15.mlp.fc1.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.2.15.mlp.fc2.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 1.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.2.15.mlp.fc2.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.2.15.ls2.gamma: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.2.16.norm1.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.2.16.norm1.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.2.16.attn.qkv.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 1.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.2.16.attn.qkv.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.2.16.attn.proj.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 1.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.2.16.attn.proj.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.2.16.ls1.gamma: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.2.16.norm2.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.2.16.norm2.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.2.16.mlp.fc1.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 1.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.2.16.mlp.fc1.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.2.16.mlp.fc2.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 1.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.2.16.mlp.fc2.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.2.16.ls2.gamma: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.2.17.norm1.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.2.17.norm1.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.2.17.attn.qkv.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 1.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.2.17.attn.qkv.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.2.17.attn.proj.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 1.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.2.17.attn.proj.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.2.17.ls1.gamma: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.2.17.norm2.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.2.17.norm2.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.2.17.mlp.fc1.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 1.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.2.17.mlp.fc1.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.2.17.mlp.fc2.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 1.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.2.17.mlp.fc2.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.2.17.ls2.gamma: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.3.18.norm1.weight: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.3.18.norm1.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.3.18.attn.qkv.weight: lr_multiplier: 0.531441, wd_multiplier: 1.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.3.18.attn.qkv.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.3.18.attn.proj.weight: lr_multiplier: 0.531441, wd_multiplier: 1.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.3.18.attn.proj.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.3.18.ls1.gamma: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.3.18.norm2.weight: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.3.18.norm2.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.3.18.mlp.fc1.weight: lr_multiplier: 0.531441, wd_multiplier: 1.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.3.18.mlp.fc1.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.3.18.mlp.fc2.weight: lr_multiplier: 0.531441, wd_multiplier: 1.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.3.18.mlp.fc2.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.3.18.ls2.gamma: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.3.19.norm1.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.3.19.norm1.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.3.19.attn.qkv.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 1.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.3.19.attn.qkv.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.3.19.attn.proj.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 1.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.3.19.attn.proj.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.3.19.ls1.gamma: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.3.19.norm2.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.3.19.norm2.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.3.19.mlp.fc1.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 1.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.3.19.mlp.fc1.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.3.19.mlp.fc2.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 1.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.3.19.mlp.fc2.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.3.19.ls2.gamma: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.3.20.norm1.weight: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.3.20.norm1.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.3.20.attn.qkv.weight: lr_multiplier: 0.6561, wd_multiplier: 1.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.3.20.attn.qkv.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.3.20.attn.proj.weight: lr_multiplier: 0.6561, wd_multiplier: 1.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.3.20.attn.proj.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.3.20.ls1.gamma: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.3.20.norm2.weight: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.3.20.norm2.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.3.20.mlp.fc1.weight: lr_multiplier: 0.6561, wd_multiplier: 1.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.3.20.mlp.fc1.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.3.20.mlp.fc2.weight: lr_multiplier: 0.6561, wd_multiplier: 1.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.3.20.mlp.fc2.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.3.20.ls2.gamma: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.3.21.norm1.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.3.21.norm1.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.3.21.attn.qkv.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 1.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.3.21.attn.qkv.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.3.21.attn.proj.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 1.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.3.21.attn.proj.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.3.21.ls1.gamma: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.3.21.norm2.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.3.21.norm2.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.3.21.mlp.fc1.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 1.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.3.21.mlp.fc1.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.3.21.mlp.fc2.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 1.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.3.21.mlp.fc2.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.3.21.ls2.gamma: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.3.22.norm1.weight: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.3.22.norm1.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.3.22.attn.qkv.weight: lr_multiplier: 0.81, wd_multiplier: 1.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.3.22.attn.qkv.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.3.22.attn.proj.weight: lr_multiplier: 0.81, wd_multiplier: 1.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.3.22.attn.proj.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.3.22.ls1.gamma: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.3.22.norm2.weight: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.3.22.norm2.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.3.22.mlp.fc1.weight: lr_multiplier: 0.81, wd_multiplier: 1.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.3.22.mlp.fc1.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.3.22.mlp.fc2.weight: lr_multiplier: 0.81, wd_multiplier: 1.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.3.22.mlp.fc2.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.3.22.ls2.gamma: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.3.23.norm1.weight: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.3.23.norm1.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.3.23.attn.qkv.weight: lr_multiplier: 0.9, wd_multiplier: 1.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.3.23.attn.qkv.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.3.23.attn.proj.weight: lr_multiplier: 0.9, wd_multiplier: 1.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.3.23.attn.proj.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.3.23.ls1.gamma: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.3.23.norm2.weight: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.3.23.norm2.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.3.23.mlp.fc1.weight: lr_multiplier: 0.9, wd_multiplier: 1.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.3.23.mlp.fc1.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.3.23.mlp.fc2.weight: lr_multiplier: 0.9, wd_multiplier: 1.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.3.23.mlp.fc2.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] blocks.3.23.ls2.gamma: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] norm.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20240906 13:14:32 10544 dinov2 param_groups.py:87] norm.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20240906 13:14:33 10544 dinov2 ssl_meta_arch.py:378] fusing param groups
I20240906 13:14:33 10544 dinov2 param_groups.py:64] else code branch
I20240906 13:14:33 10544 dinov2 param_groups.py:87] mlp.0.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20240906 13:14:33 10544 dinov2 param_groups.py:87] mlp.0.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20240906 13:14:33 10544 dinov2 param_groups.py:87] mlp.2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20240906 13:14:33 10544 dinov2 param_groups.py:87] mlp.2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20240906 13:14:33 10544 dinov2 param_groups.py:87] mlp.4.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20240906 13:14:33 10544 dinov2 param_groups.py:87] mlp.4.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20240906 13:14:33 10544 dinov2 param_groups.py:87] last_layer.weight_g: lr_multiplier: 1.0, wd_multiplier: 1.0
I20240906 13:14:33 10544 dinov2 param_groups.py:87] last_layer.weight_v: lr_multiplier: 1.0, wd_multiplier: 1.0
I20240906 13:14:33 10544 dinov2 ssl_meta_arch.py:378] fusing param groups
I20240906 13:14:33 10544 dinov2 train.py:98] Schedulers ready.
I20240906 13:14:33 10544 dinov2 augmentations.py:34] ###################################
I20240906 13:14:33 10544 dinov2 augmentations.py:35] Using data augmentation parameters:
I20240906 13:14:33 10544 dinov2 augmentations.py:36] global_crops_scale: [0.32, 1.0]
I20240906 13:14:33 10544 dinov2 augmentations.py:37] local_crops_scale: [0.05, 0.32]
I20240906 13:14:33 10544 dinov2 augmentations.py:38] local_crops_number: 8
I20240906 13:14:33 10544 dinov2 augmentations.py:39] global_crops_size: 224
I20240906 13:14:33 10544 dinov2 augmentations.py:40] local_crops_size: 96
I20240906 13:14:33 10544 dinov2 augmentations.py:41] ###################################
I20240906 13:14:33 10544 dinov2 loaders.py:89] using dataset: "ImageNet:split=TRAIN:root=C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\datasets\ILSVRC\Data\CLS-LOC:extra=C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\datasets\ILSVRC\Data\CLS-LOC"
I20240906 13:14:33 10544 dinov2 loaders.py:94] # of dataset samples: 1,281,167
I20240906 13:14:33 10544 dinov2 loaders.py:117] sampler: infinite
I20240906 13:14:33 10544 dinov2 loaders.py:211] using PyTorch data loader
I20240906 13:14:33 10544 dinov2 loaders.py:226] infinite data loader
I20240906 13:14:33 10544 dinov2 train.py:215] Starting training from iteration 0
I20240906 13:15:29 16360 dinov2 config.py:59] git:
  sha: d43546fae6805ceb22618af8cf78469b61d913c7, status: has uncommitted changes, branch: main

I20240906 13:15:29 16360 dinov2 config.py:60] config_file: configs/train/vitl16_short.yaml
eval: 
eval_only: False
no_resume: False
opts: ['train.dataset_path=ImageNet:split=TRAIN:root=C:\\Users\\ISI_UTS\\Siladittya\\iclr2023\\inexpts\\datasets\\ILSVRC\\Data\\CLS-LOC:extra=C:\\Users\\ISI_UTS\\Siladittya\\iclr2023\\inexpts\\datasets\\ILSVRC\\Data\\CLS-LOC', 'train.output_dir=C:\\Users\\ISI_UTS\\Siladittya\\iclr2023\\inexpts\\dinov2_nosetup\\dinov2\\dinov2_nosetup\\dinov2']
output_dir: C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\dinov2_nosetup\dinov2\dinov2_nosetup\dinov2
I20240906 13:15:29 16360 dinov2 config.py:26] sqrt scaling learning rate; base: 0.004, new: 0.001
I20240906 13:15:29 16360 dinov2 config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
dino:
  loss_weight: 1.0
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
  koleo_loss_weight: 0.1
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  separate_head: false
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
train:
  batch_size_per_gpu: 64
  dataset_path: ImageNet:split=TRAIN:root=C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\datasets\ILSVRC\Data\CLS-LOC:extra=C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\datasets\ILSVRC\Data\CLS-LOC
  output_dir: C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\dinov2_nosetup\dinov2\dinov2_nosetup\dinov2
  saveckp_freq: 20
  seed: 0
  num_workers: 0
  OFFICIAL_EPOCH_LENGTH: 1250
  cache_dataset: true
  centering: centering
student:
  arch: vit_large
  patch_size: 16
  drop_path_rate: 0.3
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: mlp
  block_chunks: 4
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
  num_register_tokens: 0
  interpolate_antialias: false
  interpolate_offset: 0.1
teacher:
  momentum_teacher: 0.992
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 30
optim:
  epochs: 100
  weight_decay: 0.04
  weight_decay_end: 0.4
  base_lr: 0.004
  lr: 0.001
  warmup_epochs: 10
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 0.9
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 8
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 224
  local_crops_size: 96
evaluation:
  eval_period_iterations: 12500

I20240906 13:15:29 16360 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20240906 13:15:31 16360 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20240906 13:15:34 16360 dinov2 ssl_meta_arch.py:43] OPTIONS -- architecture : embed_dim: 1024
I20240906 13:15:34 16360 dinov2 ssl_meta_arch.py:58] OPTIONS -- DINO
I20240906 13:15:34 16360 dinov2 ssl_meta_arch.py:60] OPTIONS -- DINO -- loss_weight: 1.0
I20240906 13:15:34 16360 dinov2 ssl_meta_arch.py:61] OPTIONS -- DINO -- head_n_prototypes: 65536
I20240906 13:15:34 16360 dinov2 ssl_meta_arch.py:62] OPTIONS -- DINO -- head_bottleneck_dim: 256
I20240906 13:15:34 16360 dinov2 ssl_meta_arch.py:63] OPTIONS -- DINO -- head_hidden_dim: 2048
I20240906 13:15:34 16360 dinov2 ssl_meta_arch.py:75] OPTIONS -- DINO -- applying KOLEO regularization
I20240906 13:15:34 16360 dinov2 ssl_meta_arch.py:85] OPTIONS -- IBOT
I20240906 13:15:34 16360 dinov2 ssl_meta_arch.py:86] OPTIONS -- IBOT -- loss_weight: 1.0
I20240906 13:15:34 16360 dinov2 ssl_meta_arch.py:87] OPTIONS -- IBOT masking -- ibot_mask_ratio_tuple: [0.1, 0.5]
I20240906 13:15:34 16360 dinov2 ssl_meta_arch.py:88] OPTIONS -- IBOT masking -- ibot_mask_sample_probability: 0.5
I20240906 13:15:34 16360 dinov2 ssl_meta_arch.py:111] OPTIONS -- IBOT -- head shared with DINO
I20240906 13:15:34 16360 dinov2 ssl_meta_arch.py:121] Student and Teacher are built: they are both vit_large network.
I20240906 13:15:35 16360 dinov2 train.py:301] Model:
SSLMetaArch(
  (dino_loss): DINOLoss()
  (koleo_loss): KoLeoLoss(
    (pdist): PairwiseDistance()
  )
  (ibot_patch_loss): iBOTPatchLoss()
  (student): ModuleDict(
    (backbone): DinoVisionTransformer(
      (patch_embed): PatchEmbed(
        (proj): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))
        (norm): Identity()
      )
      (blocks): ModuleList(
        (0): BlockChunk(
          (0-5): 6 x NestedTensorBlock(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): DropPath()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): DropPath()
          )
        )
        (1): BlockChunk(
          (0-5): 6 x Identity()
          (6-11): 6 x NestedTensorBlock(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): DropPath()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): DropPath()
          )
        )
        (2): BlockChunk(
          (0-11): 12 x Identity()
          (12-17): 6 x NestedTensorBlock(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): DropPath()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): DropPath()
          )
        )
        (3): BlockChunk(
          (0-17): 18 x Identity()
          (18-23): 6 x NestedTensorBlock(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): DropPath()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): DropPath()
          )
        )
      )
      (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (head): Identity()
    )
    (dino_head): DINOHead(
      (mlp): Sequential(
        (0): Linear(in_features=1024, out_features=2048, bias=True)
        (1): GELU(approximate='none')
        (2): Linear(in_features=2048, out_features=2048, bias=True)
        (3): GELU(approximate='none')
        (4): Linear(in_features=2048, out_features=256, bias=True)
      )
      (last_layer): Linear(in_features=256, out_features=65536, bias=False)
    )
  )
  (teacher): ModuleDict(
    (backbone): DinoVisionTransformer(
      (patch_embed): PatchEmbed(
        (proj): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))
        (norm): Identity()
      )
      (blocks): ModuleList(
        (0): BlockChunk(
          (0-5): 6 x NestedTensorBlock(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): Identity()
          )
        )
        (1): BlockChunk(
          (0-5): 6 x Identity()
          (6-11): 6 x NestedTensorBlock(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): Identity()
          )
        )
        (2): BlockChunk(
          (0-11): 12 x Identity()
          (12-17): 6 x NestedTensorBlock(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): Identity()
          )
        )
        (3): BlockChunk(
          (0-17): 18 x Identity()
          (18-23): 6 x NestedTensorBlock(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): Identity()
          )
        )
      )
      (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (head): Identity()
    )
    (dino_head): DINOHead(
      (mlp): Sequential(
        (0): Linear(in_features=1024, out_features=2048, bias=True)
        (1): GELU(approximate='none')
        (2): Linear(in_features=2048, out_features=2048, bias=True)
        (3): GELU(approximate='none')
        (4): Linear(in_features=2048, out_features=256, bias=True)
      )
      (last_layer): Linear(in_features=256, out_features=65536, bias=False)
    )
  )
)
I20240906 13:15:35 16360 dinov2 param_groups.py:54] chunked fsdp
I20240906 13:15:35 16360 dinov2 param_groups.py:87] cls_token: lr_multiplier: 0.0717897987691853, wd_multiplier: 1.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] pos_embed: lr_multiplier: 0.0717897987691853, wd_multiplier: 1.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] mask_token: lr_multiplier: 0.0717897987691853, wd_multiplier: 1.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] patch_embed.proj.weight: lr_multiplier: 0.01435795975383706, wd_multiplier: 1.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] patch_embed.proj.bias: lr_multiplier: 0.01435795975383706, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.0.0.norm1.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.0.0.norm1.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.0.0.attn.qkv.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 1.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.0.0.attn.qkv.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.0.0.attn.proj.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 1.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.0.0.attn.proj.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.0.0.ls1.gamma: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.0.0.norm2.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.0.0.norm2.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.0.0.mlp.fc1.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 1.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.0.0.mlp.fc1.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.0.0.mlp.fc2.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 1.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.0.0.mlp.fc2.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.0.0.ls2.gamma: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.0.1.norm1.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.0.1.norm1.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.0.1.attn.qkv.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 1.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.0.1.attn.qkv.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.0.1.attn.proj.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 1.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.0.1.attn.proj.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.0.1.ls1.gamma: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.0.1.norm2.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.0.1.norm2.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.0.1.mlp.fc1.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 1.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.0.1.mlp.fc1.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.0.1.mlp.fc2.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 1.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.0.1.mlp.fc2.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.0.1.ls2.gamma: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.0.2.norm1.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.0.2.norm1.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.0.2.attn.qkv.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 1.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.0.2.attn.qkv.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.0.2.attn.proj.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 1.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.0.2.attn.proj.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.0.2.ls1.gamma: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.0.2.norm2.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.0.2.norm2.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.0.2.mlp.fc1.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 1.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.0.2.mlp.fc1.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.0.2.mlp.fc2.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 1.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.0.2.mlp.fc2.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.0.2.ls2.gamma: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.0.3.norm1.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.0.3.norm1.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.0.3.attn.qkv.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 1.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.0.3.attn.qkv.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.0.3.attn.proj.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 1.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.0.3.attn.proj.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.0.3.ls1.gamma: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.0.3.norm2.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.0.3.norm2.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.0.3.mlp.fc1.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 1.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.0.3.mlp.fc1.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.0.3.mlp.fc2.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 1.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.0.3.mlp.fc2.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.0.3.ls2.gamma: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.0.4.norm1.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.0.4.norm1.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.0.4.attn.qkv.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 1.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.0.4.attn.qkv.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.0.4.attn.proj.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 1.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.0.4.attn.proj.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.0.4.ls1.gamma: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.0.4.norm2.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.0.4.norm2.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.0.4.mlp.fc1.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 1.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.0.4.mlp.fc1.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.0.4.mlp.fc2.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 1.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.0.4.mlp.fc2.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.0.4.ls2.gamma: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.0.5.norm1.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.0.5.norm1.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.0.5.attn.qkv.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 1.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.0.5.attn.qkv.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.0.5.attn.proj.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 1.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.0.5.attn.proj.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.0.5.ls1.gamma: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.0.5.norm2.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.0.5.norm2.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.0.5.mlp.fc1.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 1.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.0.5.mlp.fc1.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.0.5.mlp.fc2.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 1.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.0.5.mlp.fc2.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.0.5.ls2.gamma: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.1.6.norm1.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.1.6.norm1.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.1.6.attn.qkv.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 1.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.1.6.attn.qkv.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.1.6.attn.proj.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 1.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.1.6.attn.proj.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.1.6.ls1.gamma: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.1.6.norm2.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.1.6.norm2.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.1.6.mlp.fc1.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 1.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.1.6.mlp.fc1.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.1.6.mlp.fc2.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 1.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.1.6.mlp.fc2.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.1.6.ls2.gamma: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.1.7.norm1.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.1.7.norm1.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.1.7.attn.qkv.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 1.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.1.7.attn.qkv.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.1.7.attn.proj.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 1.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.1.7.attn.proj.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.1.7.ls1.gamma: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.1.7.norm2.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.1.7.norm2.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.1.7.mlp.fc1.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 1.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.1.7.mlp.fc1.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.1.7.mlp.fc2.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 1.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.1.7.mlp.fc2.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.1.7.ls2.gamma: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.1.8.norm1.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.1.8.norm1.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.1.8.attn.qkv.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 1.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.1.8.attn.qkv.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.1.8.attn.proj.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 1.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.1.8.attn.proj.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.1.8.ls1.gamma: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.1.8.norm2.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.1.8.norm2.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.1.8.mlp.fc1.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 1.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.1.8.mlp.fc1.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.1.8.mlp.fc2.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 1.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.1.8.mlp.fc2.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.1.8.ls2.gamma: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.1.9.norm1.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.1.9.norm1.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.1.9.attn.qkv.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 1.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.1.9.attn.qkv.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.1.9.attn.proj.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 1.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.1.9.attn.proj.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.1.9.ls1.gamma: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.1.9.norm2.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.1.9.norm2.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.1.9.mlp.fc1.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 1.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.1.9.mlp.fc1.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.1.9.mlp.fc2.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 1.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.1.9.mlp.fc2.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.1.9.ls2.gamma: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.1.10.norm1.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.1.10.norm1.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.1.10.attn.qkv.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 1.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.1.10.attn.qkv.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.1.10.attn.proj.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 1.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.1.10.attn.proj.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.1.10.ls1.gamma: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.1.10.norm2.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.1.10.norm2.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.1.10.mlp.fc1.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 1.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.1.10.mlp.fc1.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.1.10.mlp.fc2.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 1.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.1.10.mlp.fc2.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.1.10.ls2.gamma: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.1.11.norm1.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.1.11.norm1.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.1.11.attn.qkv.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 1.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.1.11.attn.qkv.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.1.11.attn.proj.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 1.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.1.11.attn.proj.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.1.11.ls1.gamma: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.1.11.norm2.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.1.11.norm2.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.1.11.mlp.fc1.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 1.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.1.11.mlp.fc1.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.1.11.mlp.fc2.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 1.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.1.11.mlp.fc2.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.1.11.ls2.gamma: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.2.12.norm1.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.2.12.norm1.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.2.12.attn.qkv.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 1.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.2.12.attn.qkv.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.2.12.attn.proj.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 1.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.2.12.attn.proj.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.2.12.ls1.gamma: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.2.12.norm2.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.2.12.norm2.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.2.12.mlp.fc1.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 1.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.2.12.mlp.fc1.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.2.12.mlp.fc2.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 1.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.2.12.mlp.fc2.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.2.12.ls2.gamma: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.2.13.norm1.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.2.13.norm1.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.2.13.attn.qkv.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 1.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.2.13.attn.qkv.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.2.13.attn.proj.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 1.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.2.13.attn.proj.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.2.13.ls1.gamma: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.2.13.norm2.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.2.13.norm2.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.2.13.mlp.fc1.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 1.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.2.13.mlp.fc1.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.2.13.mlp.fc2.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 1.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.2.13.mlp.fc2.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.2.13.ls2.gamma: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.2.14.norm1.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.2.14.norm1.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.2.14.attn.qkv.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 1.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.2.14.attn.qkv.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.2.14.attn.proj.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 1.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.2.14.attn.proj.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.2.14.ls1.gamma: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.2.14.norm2.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.2.14.norm2.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.2.14.mlp.fc1.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 1.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.2.14.mlp.fc1.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.2.14.mlp.fc2.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 1.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.2.14.mlp.fc2.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.2.14.ls2.gamma: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.2.15.norm1.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.2.15.norm1.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.2.15.attn.qkv.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 1.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.2.15.attn.qkv.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.2.15.attn.proj.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 1.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.2.15.attn.proj.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.2.15.ls1.gamma: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.2.15.norm2.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.2.15.norm2.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.2.15.mlp.fc1.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 1.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.2.15.mlp.fc1.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.2.15.mlp.fc2.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 1.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.2.15.mlp.fc2.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.2.15.ls2.gamma: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.2.16.norm1.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.2.16.norm1.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.2.16.attn.qkv.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 1.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.2.16.attn.qkv.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.2.16.attn.proj.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 1.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.2.16.attn.proj.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.2.16.ls1.gamma: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.2.16.norm2.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.2.16.norm2.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.2.16.mlp.fc1.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 1.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.2.16.mlp.fc1.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.2.16.mlp.fc2.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 1.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.2.16.mlp.fc2.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.2.16.ls2.gamma: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.2.17.norm1.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.2.17.norm1.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.2.17.attn.qkv.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 1.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.2.17.attn.qkv.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.2.17.attn.proj.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 1.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.2.17.attn.proj.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.2.17.ls1.gamma: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.2.17.norm2.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.2.17.norm2.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.2.17.mlp.fc1.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 1.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.2.17.mlp.fc1.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.2.17.mlp.fc2.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 1.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.2.17.mlp.fc2.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.2.17.ls2.gamma: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.3.18.norm1.weight: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.3.18.norm1.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.3.18.attn.qkv.weight: lr_multiplier: 0.531441, wd_multiplier: 1.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.3.18.attn.qkv.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.3.18.attn.proj.weight: lr_multiplier: 0.531441, wd_multiplier: 1.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.3.18.attn.proj.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.3.18.ls1.gamma: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.3.18.norm2.weight: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.3.18.norm2.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.3.18.mlp.fc1.weight: lr_multiplier: 0.531441, wd_multiplier: 1.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.3.18.mlp.fc1.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.3.18.mlp.fc2.weight: lr_multiplier: 0.531441, wd_multiplier: 1.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.3.18.mlp.fc2.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.3.18.ls2.gamma: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.3.19.norm1.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.3.19.norm1.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.3.19.attn.qkv.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 1.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.3.19.attn.qkv.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.3.19.attn.proj.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 1.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.3.19.attn.proj.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.3.19.ls1.gamma: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.3.19.norm2.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.3.19.norm2.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.3.19.mlp.fc1.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 1.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.3.19.mlp.fc1.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.3.19.mlp.fc2.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 1.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.3.19.mlp.fc2.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.3.19.ls2.gamma: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.3.20.norm1.weight: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.3.20.norm1.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.3.20.attn.qkv.weight: lr_multiplier: 0.6561, wd_multiplier: 1.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.3.20.attn.qkv.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.3.20.attn.proj.weight: lr_multiplier: 0.6561, wd_multiplier: 1.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.3.20.attn.proj.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.3.20.ls1.gamma: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.3.20.norm2.weight: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.3.20.norm2.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.3.20.mlp.fc1.weight: lr_multiplier: 0.6561, wd_multiplier: 1.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.3.20.mlp.fc1.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.3.20.mlp.fc2.weight: lr_multiplier: 0.6561, wd_multiplier: 1.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.3.20.mlp.fc2.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.3.20.ls2.gamma: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.3.21.norm1.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.3.21.norm1.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.3.21.attn.qkv.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 1.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.3.21.attn.qkv.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.3.21.attn.proj.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 1.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.3.21.attn.proj.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.3.21.ls1.gamma: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.3.21.norm2.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.3.21.norm2.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.3.21.mlp.fc1.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 1.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.3.21.mlp.fc1.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.3.21.mlp.fc2.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 1.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.3.21.mlp.fc2.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.3.21.ls2.gamma: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.3.22.norm1.weight: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.3.22.norm1.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.3.22.attn.qkv.weight: lr_multiplier: 0.81, wd_multiplier: 1.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.3.22.attn.qkv.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.3.22.attn.proj.weight: lr_multiplier: 0.81, wd_multiplier: 1.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.3.22.attn.proj.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.3.22.ls1.gamma: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.3.22.norm2.weight: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.3.22.norm2.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.3.22.mlp.fc1.weight: lr_multiplier: 0.81, wd_multiplier: 1.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.3.22.mlp.fc1.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.3.22.mlp.fc2.weight: lr_multiplier: 0.81, wd_multiplier: 1.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.3.22.mlp.fc2.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.3.22.ls2.gamma: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.3.23.norm1.weight: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.3.23.norm1.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.3.23.attn.qkv.weight: lr_multiplier: 0.9, wd_multiplier: 1.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.3.23.attn.qkv.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.3.23.attn.proj.weight: lr_multiplier: 0.9, wd_multiplier: 1.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.3.23.attn.proj.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.3.23.ls1.gamma: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.3.23.norm2.weight: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.3.23.norm2.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.3.23.mlp.fc1.weight: lr_multiplier: 0.9, wd_multiplier: 1.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.3.23.mlp.fc1.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.3.23.mlp.fc2.weight: lr_multiplier: 0.9, wd_multiplier: 1.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.3.23.mlp.fc2.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] blocks.3.23.ls2.gamma: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] norm.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] norm.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 ssl_meta_arch.py:378] fusing param groups
I20240906 13:15:35 16360 dinov2 param_groups.py:64] else code branch
I20240906 13:15:35 16360 dinov2 param_groups.py:87] mlp.0.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] mlp.0.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] mlp.2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] mlp.2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] mlp.4.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] mlp.4.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] last_layer.weight_g: lr_multiplier: 1.0, wd_multiplier: 1.0
I20240906 13:15:35 16360 dinov2 param_groups.py:87] last_layer.weight_v: lr_multiplier: 1.0, wd_multiplier: 1.0
I20240906 13:15:35 16360 dinov2 ssl_meta_arch.py:378] fusing param groups
I20240906 13:15:35 16360 dinov2 train.py:98] Schedulers ready.
I20240906 13:15:35 16360 dinov2 augmentations.py:34] ###################################
I20240906 13:15:35 16360 dinov2 augmentations.py:35] Using data augmentation parameters:
I20240906 13:15:35 16360 dinov2 augmentations.py:36] global_crops_scale: [0.32, 1.0]
I20240906 13:15:35 16360 dinov2 augmentations.py:37] local_crops_scale: [0.05, 0.32]
I20240906 13:15:35 16360 dinov2 augmentations.py:38] local_crops_number: 8
I20240906 13:15:35 16360 dinov2 augmentations.py:39] global_crops_size: 224
I20240906 13:15:35 16360 dinov2 augmentations.py:40] local_crops_size: 96
I20240906 13:15:35 16360 dinov2 augmentations.py:41] ###################################
I20240906 13:15:35 16360 dinov2 loaders.py:89] using dataset: "ImageNet:split=TRAIN:root=C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\datasets\ILSVRC\Data\CLS-LOC:extra=C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\datasets\ILSVRC\Data\CLS-LOC"
I20240906 13:15:35 16360 dinov2 loaders.py:94] # of dataset samples: 1,281,167
I20240906 13:15:35 16360 dinov2 loaders.py:117] sampler: infinite
I20240906 13:15:35 16360 dinov2 loaders.py:211] using PyTorch data loader
I20240906 13:15:35 16360 dinov2 loaders.py:226] infinite data loader
I20240906 13:15:35 16360 dinov2 train.py:215] Starting training from iteration 0
I20240906 13:19:22 18896 dinov2 config.py:59] git:
  sha: d43546fae6805ceb22618af8cf78469b61d913c7, status: has uncommitted changes, branch: main

I20240906 13:19:22 18896 dinov2 config.py:60] config_file: configs/train/vitl16_short.yaml
eval: 
eval_only: False
no_resume: False
opts: ['train.dataset_path=ImageNet:split=TRAIN:root=C:\\Users\\ISI_UTS\\Siladittya\\iclr2023\\inexpts\\datasets\\ILSVRC\\Data\\CLS-LOC:extra=C:\\Users\\ISI_UTS\\Siladittya\\iclr2023\\inexpts\\datasets\\ILSVRC\\Data\\CLS-LOC', 'train.output_dir=C:\\Users\\ISI_UTS\\Siladittya\\iclr2023\\inexpts\\dinov2_nosetup\\dinov2\\dinov2_nosetup\\dinov2']
output_dir: C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\dinov2_nosetup\dinov2\dinov2_nosetup\dinov2
I20240906 13:19:22 18896 dinov2 config.py:26] sqrt scaling learning rate; base: 0.004, new: 0.001
I20240906 13:19:22 18896 dinov2 config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
dino:
  loss_weight: 1.0
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
  koleo_loss_weight: 0.1
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  separate_head: false
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
train:
  batch_size_per_gpu: 64
  dataset_path: ImageNet:split=TRAIN:root=C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\datasets\ILSVRC\Data\CLS-LOC:extra=C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\datasets\ILSVRC\Data\CLS-LOC
  output_dir: C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\dinov2_nosetup\dinov2\dinov2_nosetup\dinov2
  saveckp_freq: 20
  seed: 0
  num_workers: 0
  OFFICIAL_EPOCH_LENGTH: 1250
  cache_dataset: true
  centering: centering
student:
  arch: vit_large
  patch_size: 16
  drop_path_rate: 0.3
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: mlp
  block_chunks: 4
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
  num_register_tokens: 0
  interpolate_antialias: false
  interpolate_offset: 0.1
teacher:
  momentum_teacher: 0.992
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 30
optim:
  epochs: 100
  weight_decay: 0.04
  weight_decay_end: 0.4
  base_lr: 0.004
  lr: 0.001
  warmup_epochs: 10
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 0.9
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 8
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 224
  local_crops_size: 96
evaluation:
  eval_period_iterations: 12500

I20240906 13:19:22 18896 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20240906 13:19:24 18896 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20240906 13:19:27 18896 dinov2 ssl_meta_arch.py:43] OPTIONS -- architecture : embed_dim: 1024
I20240906 13:19:27 18896 dinov2 ssl_meta_arch.py:58] OPTIONS -- DINO
I20240906 13:19:27 18896 dinov2 ssl_meta_arch.py:60] OPTIONS -- DINO -- loss_weight: 1.0
I20240906 13:19:27 18896 dinov2 ssl_meta_arch.py:61] OPTIONS -- DINO -- head_n_prototypes: 65536
I20240906 13:19:27 18896 dinov2 ssl_meta_arch.py:62] OPTIONS -- DINO -- head_bottleneck_dim: 256
I20240906 13:19:27 18896 dinov2 ssl_meta_arch.py:63] OPTIONS -- DINO -- head_hidden_dim: 2048
I20240906 13:19:27 18896 dinov2 ssl_meta_arch.py:75] OPTIONS -- DINO -- applying KOLEO regularization
I20240906 13:19:27 18896 dinov2 ssl_meta_arch.py:85] OPTIONS -- IBOT
I20240906 13:19:27 18896 dinov2 ssl_meta_arch.py:86] OPTIONS -- IBOT -- loss_weight: 1.0
I20240906 13:19:27 18896 dinov2 ssl_meta_arch.py:87] OPTIONS -- IBOT masking -- ibot_mask_ratio_tuple: [0.1, 0.5]
I20240906 13:19:27 18896 dinov2 ssl_meta_arch.py:88] OPTIONS -- IBOT masking -- ibot_mask_sample_probability: 0.5
I20240906 13:19:27 18896 dinov2 ssl_meta_arch.py:111] OPTIONS -- IBOT -- head shared with DINO
I20240906 13:19:27 18896 dinov2 ssl_meta_arch.py:121] Student and Teacher are built: they are both vit_large network.
I20240906 13:19:28 18896 dinov2 train.py:301] Model:
SSLMetaArch(
  (dino_loss): DINOLoss()
  (koleo_loss): KoLeoLoss(
    (pdist): PairwiseDistance()
  )
  (ibot_patch_loss): iBOTPatchLoss()
  (student): ModuleDict(
    (backbone): DinoVisionTransformer(
      (patch_embed): PatchEmbed(
        (proj): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))
        (norm): Identity()
      )
      (blocks): ModuleList(
        (0): BlockChunk(
          (0-5): 6 x NestedTensorBlock(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): DropPath()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): DropPath()
          )
        )
        (1): BlockChunk(
          (0-5): 6 x Identity()
          (6-11): 6 x NestedTensorBlock(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): DropPath()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): DropPath()
          )
        )
        (2): BlockChunk(
          (0-11): 12 x Identity()
          (12-17): 6 x NestedTensorBlock(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): DropPath()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): DropPath()
          )
        )
        (3): BlockChunk(
          (0-17): 18 x Identity()
          (18-23): 6 x NestedTensorBlock(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): DropPath()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): DropPath()
          )
        )
      )
      (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (head): Identity()
    )
    (dino_head): DINOHead(
      (mlp): Sequential(
        (0): Linear(in_features=1024, out_features=2048, bias=True)
        (1): GELU(approximate='none')
        (2): Linear(in_features=2048, out_features=2048, bias=True)
        (3): GELU(approximate='none')
        (4): Linear(in_features=2048, out_features=256, bias=True)
      )
      (last_layer): Linear(in_features=256, out_features=65536, bias=False)
    )
  )
  (teacher): ModuleDict(
    (backbone): DinoVisionTransformer(
      (patch_embed): PatchEmbed(
        (proj): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))
        (norm): Identity()
      )
      (blocks): ModuleList(
        (0): BlockChunk(
          (0-5): 6 x NestedTensorBlock(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): Identity()
          )
        )
        (1): BlockChunk(
          (0-5): 6 x Identity()
          (6-11): 6 x NestedTensorBlock(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): Identity()
          )
        )
        (2): BlockChunk(
          (0-11): 12 x Identity()
          (12-17): 6 x NestedTensorBlock(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): Identity()
          )
        )
        (3): BlockChunk(
          (0-17): 18 x Identity()
          (18-23): 6 x NestedTensorBlock(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): Identity()
          )
        )
      )
      (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (head): Identity()
    )
    (dino_head): DINOHead(
      (mlp): Sequential(
        (0): Linear(in_features=1024, out_features=2048, bias=True)
        (1): GELU(approximate='none')
        (2): Linear(in_features=2048, out_features=2048, bias=True)
        (3): GELU(approximate='none')
        (4): Linear(in_features=2048, out_features=256, bias=True)
      )
      (last_layer): Linear(in_features=256, out_features=65536, bias=False)
    )
  )
)
I20240906 13:19:28 18896 dinov2 param_groups.py:54] chunked fsdp
I20240906 13:19:28 18896 dinov2 param_groups.py:87] cls_token: lr_multiplier: 0.0717897987691853, wd_multiplier: 1.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] pos_embed: lr_multiplier: 0.0717897987691853, wd_multiplier: 1.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] mask_token: lr_multiplier: 0.0717897987691853, wd_multiplier: 1.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] patch_embed.proj.weight: lr_multiplier: 0.01435795975383706, wd_multiplier: 1.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] patch_embed.proj.bias: lr_multiplier: 0.01435795975383706, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.0.0.norm1.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.0.0.norm1.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.0.0.attn.qkv.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 1.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.0.0.attn.qkv.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.0.0.attn.proj.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 1.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.0.0.attn.proj.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.0.0.ls1.gamma: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.0.0.norm2.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.0.0.norm2.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.0.0.mlp.fc1.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 1.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.0.0.mlp.fc1.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.0.0.mlp.fc2.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 1.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.0.0.mlp.fc2.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.0.0.ls2.gamma: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.0.1.norm1.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.0.1.norm1.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.0.1.attn.qkv.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 1.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.0.1.attn.qkv.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.0.1.attn.proj.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 1.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.0.1.attn.proj.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.0.1.ls1.gamma: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.0.1.norm2.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.0.1.norm2.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.0.1.mlp.fc1.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 1.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.0.1.mlp.fc1.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.0.1.mlp.fc2.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 1.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.0.1.mlp.fc2.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.0.1.ls2.gamma: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.0.2.norm1.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.0.2.norm1.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.0.2.attn.qkv.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 1.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.0.2.attn.qkv.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.0.2.attn.proj.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 1.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.0.2.attn.proj.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.0.2.ls1.gamma: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.0.2.norm2.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.0.2.norm2.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.0.2.mlp.fc1.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 1.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.0.2.mlp.fc1.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.0.2.mlp.fc2.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 1.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.0.2.mlp.fc2.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.0.2.ls2.gamma: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.0.3.norm1.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.0.3.norm1.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.0.3.attn.qkv.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 1.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.0.3.attn.qkv.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.0.3.attn.proj.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 1.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.0.3.attn.proj.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.0.3.ls1.gamma: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.0.3.norm2.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.0.3.norm2.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.0.3.mlp.fc1.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 1.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.0.3.mlp.fc1.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.0.3.mlp.fc2.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 1.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.0.3.mlp.fc2.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.0.3.ls2.gamma: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.0.4.norm1.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.0.4.norm1.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.0.4.attn.qkv.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 1.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.0.4.attn.qkv.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.0.4.attn.proj.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 1.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.0.4.attn.proj.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.0.4.ls1.gamma: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.0.4.norm2.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.0.4.norm2.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.0.4.mlp.fc1.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 1.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.0.4.mlp.fc1.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.0.4.mlp.fc2.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 1.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.0.4.mlp.fc2.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.0.4.ls2.gamma: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.0.5.norm1.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.0.5.norm1.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.0.5.attn.qkv.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 1.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.0.5.attn.qkv.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.0.5.attn.proj.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 1.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.0.5.attn.proj.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.0.5.ls1.gamma: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.0.5.norm2.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.0.5.norm2.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.0.5.mlp.fc1.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 1.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.0.5.mlp.fc1.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.0.5.mlp.fc2.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 1.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.0.5.mlp.fc2.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.0.5.ls2.gamma: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.1.6.norm1.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.1.6.norm1.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.1.6.attn.qkv.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 1.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.1.6.attn.qkv.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.1.6.attn.proj.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 1.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.1.6.attn.proj.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.1.6.ls1.gamma: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.1.6.norm2.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.1.6.norm2.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.1.6.mlp.fc1.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 1.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.1.6.mlp.fc1.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.1.6.mlp.fc2.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 1.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.1.6.mlp.fc2.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.1.6.ls2.gamma: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.1.7.norm1.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.1.7.norm1.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.1.7.attn.qkv.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 1.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.1.7.attn.qkv.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.1.7.attn.proj.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 1.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.1.7.attn.proj.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.1.7.ls1.gamma: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.1.7.norm2.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.1.7.norm2.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.1.7.mlp.fc1.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 1.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.1.7.mlp.fc1.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.1.7.mlp.fc2.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 1.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.1.7.mlp.fc2.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.1.7.ls2.gamma: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.1.8.norm1.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.1.8.norm1.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.1.8.attn.qkv.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 1.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.1.8.attn.qkv.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.1.8.attn.proj.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 1.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.1.8.attn.proj.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.1.8.ls1.gamma: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.1.8.norm2.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.1.8.norm2.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.1.8.mlp.fc1.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 1.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.1.8.mlp.fc1.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.1.8.mlp.fc2.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 1.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.1.8.mlp.fc2.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.1.8.ls2.gamma: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.1.9.norm1.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.1.9.norm1.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.1.9.attn.qkv.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 1.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.1.9.attn.qkv.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.1.9.attn.proj.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 1.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.1.9.attn.proj.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.1.9.ls1.gamma: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.1.9.norm2.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.1.9.norm2.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.1.9.mlp.fc1.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 1.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.1.9.mlp.fc1.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.1.9.mlp.fc2.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 1.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.1.9.mlp.fc2.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.1.9.ls2.gamma: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.1.10.norm1.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.1.10.norm1.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.1.10.attn.qkv.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 1.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.1.10.attn.qkv.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.1.10.attn.proj.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 1.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.1.10.attn.proj.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.1.10.ls1.gamma: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.1.10.norm2.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.1.10.norm2.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.1.10.mlp.fc1.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 1.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.1.10.mlp.fc1.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.1.10.mlp.fc2.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 1.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.1.10.mlp.fc2.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.1.10.ls2.gamma: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.1.11.norm1.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.1.11.norm1.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.1.11.attn.qkv.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 1.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.1.11.attn.qkv.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.1.11.attn.proj.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 1.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.1.11.attn.proj.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.1.11.ls1.gamma: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.1.11.norm2.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.1.11.norm2.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.1.11.mlp.fc1.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 1.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.1.11.mlp.fc1.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.1.11.mlp.fc2.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 1.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.1.11.mlp.fc2.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.1.11.ls2.gamma: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.2.12.norm1.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.2.12.norm1.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.2.12.attn.qkv.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 1.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.2.12.attn.qkv.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.2.12.attn.proj.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 1.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.2.12.attn.proj.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.2.12.ls1.gamma: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.2.12.norm2.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.2.12.norm2.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.2.12.mlp.fc1.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 1.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.2.12.mlp.fc1.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.2.12.mlp.fc2.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 1.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.2.12.mlp.fc2.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.2.12.ls2.gamma: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.2.13.norm1.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.2.13.norm1.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.2.13.attn.qkv.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 1.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.2.13.attn.qkv.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.2.13.attn.proj.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 1.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.2.13.attn.proj.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.2.13.ls1.gamma: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.2.13.norm2.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.2.13.norm2.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.2.13.mlp.fc1.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 1.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.2.13.mlp.fc1.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.2.13.mlp.fc2.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 1.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.2.13.mlp.fc2.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.2.13.ls2.gamma: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.2.14.norm1.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.2.14.norm1.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.2.14.attn.qkv.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 1.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.2.14.attn.qkv.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.2.14.attn.proj.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 1.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.2.14.attn.proj.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.2.14.ls1.gamma: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.2.14.norm2.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.2.14.norm2.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.2.14.mlp.fc1.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 1.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.2.14.mlp.fc1.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.2.14.mlp.fc2.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 1.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.2.14.mlp.fc2.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.2.14.ls2.gamma: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.2.15.norm1.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.2.15.norm1.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.2.15.attn.qkv.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 1.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.2.15.attn.qkv.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.2.15.attn.proj.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 1.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.2.15.attn.proj.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.2.15.ls1.gamma: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.2.15.norm2.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.2.15.norm2.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.2.15.mlp.fc1.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 1.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.2.15.mlp.fc1.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.2.15.mlp.fc2.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 1.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.2.15.mlp.fc2.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.2.15.ls2.gamma: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.2.16.norm1.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.2.16.norm1.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.2.16.attn.qkv.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 1.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.2.16.attn.qkv.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.2.16.attn.proj.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 1.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.2.16.attn.proj.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.2.16.ls1.gamma: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.2.16.norm2.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.2.16.norm2.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.2.16.mlp.fc1.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 1.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.2.16.mlp.fc1.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.2.16.mlp.fc2.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 1.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.2.16.mlp.fc2.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.2.16.ls2.gamma: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.2.17.norm1.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.2.17.norm1.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.2.17.attn.qkv.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 1.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.2.17.attn.qkv.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.2.17.attn.proj.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 1.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.2.17.attn.proj.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.2.17.ls1.gamma: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.2.17.norm2.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.2.17.norm2.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.2.17.mlp.fc1.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 1.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.2.17.mlp.fc1.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.2.17.mlp.fc2.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 1.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.2.17.mlp.fc2.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.2.17.ls2.gamma: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.3.18.norm1.weight: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.3.18.norm1.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.3.18.attn.qkv.weight: lr_multiplier: 0.531441, wd_multiplier: 1.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.3.18.attn.qkv.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.3.18.attn.proj.weight: lr_multiplier: 0.531441, wd_multiplier: 1.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.3.18.attn.proj.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.3.18.ls1.gamma: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.3.18.norm2.weight: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.3.18.norm2.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.3.18.mlp.fc1.weight: lr_multiplier: 0.531441, wd_multiplier: 1.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.3.18.mlp.fc1.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.3.18.mlp.fc2.weight: lr_multiplier: 0.531441, wd_multiplier: 1.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.3.18.mlp.fc2.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.3.18.ls2.gamma: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.3.19.norm1.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.3.19.norm1.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.3.19.attn.qkv.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 1.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.3.19.attn.qkv.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.3.19.attn.proj.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 1.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.3.19.attn.proj.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.3.19.ls1.gamma: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.3.19.norm2.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.3.19.norm2.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.3.19.mlp.fc1.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 1.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.3.19.mlp.fc1.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.3.19.mlp.fc2.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 1.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.3.19.mlp.fc2.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.3.19.ls2.gamma: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.3.20.norm1.weight: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.3.20.norm1.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.3.20.attn.qkv.weight: lr_multiplier: 0.6561, wd_multiplier: 1.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.3.20.attn.qkv.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.3.20.attn.proj.weight: lr_multiplier: 0.6561, wd_multiplier: 1.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.3.20.attn.proj.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.3.20.ls1.gamma: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.3.20.norm2.weight: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.3.20.norm2.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.3.20.mlp.fc1.weight: lr_multiplier: 0.6561, wd_multiplier: 1.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.3.20.mlp.fc1.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.3.20.mlp.fc2.weight: lr_multiplier: 0.6561, wd_multiplier: 1.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.3.20.mlp.fc2.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.3.20.ls2.gamma: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.3.21.norm1.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.3.21.norm1.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.3.21.attn.qkv.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 1.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.3.21.attn.qkv.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.3.21.attn.proj.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 1.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.3.21.attn.proj.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.3.21.ls1.gamma: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.3.21.norm2.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.3.21.norm2.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.3.21.mlp.fc1.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 1.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.3.21.mlp.fc1.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.3.21.mlp.fc2.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 1.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.3.21.mlp.fc2.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.3.21.ls2.gamma: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.3.22.norm1.weight: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.3.22.norm1.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.3.22.attn.qkv.weight: lr_multiplier: 0.81, wd_multiplier: 1.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.3.22.attn.qkv.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.3.22.attn.proj.weight: lr_multiplier: 0.81, wd_multiplier: 1.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.3.22.attn.proj.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.3.22.ls1.gamma: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.3.22.norm2.weight: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.3.22.norm2.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.3.22.mlp.fc1.weight: lr_multiplier: 0.81, wd_multiplier: 1.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.3.22.mlp.fc1.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.3.22.mlp.fc2.weight: lr_multiplier: 0.81, wd_multiplier: 1.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.3.22.mlp.fc2.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.3.22.ls2.gamma: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.3.23.norm1.weight: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.3.23.norm1.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.3.23.attn.qkv.weight: lr_multiplier: 0.9, wd_multiplier: 1.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.3.23.attn.qkv.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.3.23.attn.proj.weight: lr_multiplier: 0.9, wd_multiplier: 1.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.3.23.attn.proj.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.3.23.ls1.gamma: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.3.23.norm2.weight: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.3.23.norm2.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.3.23.mlp.fc1.weight: lr_multiplier: 0.9, wd_multiplier: 1.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.3.23.mlp.fc1.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.3.23.mlp.fc2.weight: lr_multiplier: 0.9, wd_multiplier: 1.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.3.23.mlp.fc2.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] blocks.3.23.ls2.gamma: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] norm.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] norm.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 ssl_meta_arch.py:378] fusing param groups
I20240906 13:19:28 18896 dinov2 param_groups.py:64] else code branch
I20240906 13:19:28 18896 dinov2 param_groups.py:87] mlp.0.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] mlp.0.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] mlp.2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] mlp.2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] mlp.4.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] mlp.4.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] last_layer.weight_g: lr_multiplier: 1.0, wd_multiplier: 1.0
I20240906 13:19:28 18896 dinov2 param_groups.py:87] last_layer.weight_v: lr_multiplier: 1.0, wd_multiplier: 1.0
I20240906 13:19:28 18896 dinov2 ssl_meta_arch.py:378] fusing param groups
I20240906 13:19:28 18896 dinov2 train.py:98] Schedulers ready.
I20240906 13:19:28 18896 dinov2 augmentations.py:34] ###################################
I20240906 13:19:28 18896 dinov2 augmentations.py:35] Using data augmentation parameters:
I20240906 13:19:28 18896 dinov2 augmentations.py:36] global_crops_scale: [0.32, 1.0]
I20240906 13:19:28 18896 dinov2 augmentations.py:37] local_crops_scale: [0.05, 0.32]
I20240906 13:19:28 18896 dinov2 augmentations.py:38] local_crops_number: 8
I20240906 13:19:28 18896 dinov2 augmentations.py:39] global_crops_size: 224
I20240906 13:19:28 18896 dinov2 augmentations.py:40] local_crops_size: 96
I20240906 13:19:28 18896 dinov2 augmentations.py:41] ###################################
I20240906 13:19:28 18896 dinov2 loaders.py:89] using dataset: "ImageNet:split=TRAIN:root=C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\datasets\ILSVRC\Data\CLS-LOC:extra=C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\datasets\ILSVRC\Data\CLS-LOC"
I20240906 13:19:28 18896 dinov2 loaders.py:94] # of dataset samples: 1,281,167
I20240906 13:19:28 18896 dinov2 loaders.py:117] sampler: infinite
I20240906 13:19:28 18896 dinov2 loaders.py:211] using PyTorch data loader
I20240906 13:19:28 18896 dinov2 loaders.py:226] infinite data loader
I20240906 13:19:28 18896 dinov2 train.py:215] Starting training from iteration 0
I20240906 13:20:21 2972 dinov2 config.py:59] git:
  sha: d43546fae6805ceb22618af8cf78469b61d913c7, status: has uncommitted changes, branch: main

I20240906 13:20:21 2972 dinov2 config.py:60] config_file: configs/train/vitl16_short.yaml
eval: 
eval_only: False
no_resume: False
opts: ['train.dataset_path=ImageNet:split=TRAIN:root=C:\\Users\\ISI_UTS\\Siladittya\\iclr2023\\inexpts\\datasets\\ILSVRC\\Data\\CLS-LOC:extra=C:\\Users\\ISI_UTS\\Siladittya\\iclr2023\\inexpts\\datasets\\ILSVRC\\Data\\CLS-LOC', 'train.output_dir=C:\\Users\\ISI_UTS\\Siladittya\\iclr2023\\inexpts\\dinov2_nosetup\\dinov2\\dinov2_nosetup\\dinov2']
output_dir: C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\dinov2_nosetup\dinov2\dinov2_nosetup\dinov2
I20240906 13:20:21 2972 dinov2 config.py:26] sqrt scaling learning rate; base: 0.004, new: 0.001
I20240906 13:20:21 2972 dinov2 config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
dino:
  loss_weight: 1.0
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
  koleo_loss_weight: 0.1
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  separate_head: false
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
train:
  batch_size_per_gpu: 64
  dataset_path: ImageNet:split=TRAIN:root=C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\datasets\ILSVRC\Data\CLS-LOC:extra=C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\datasets\ILSVRC\Data\CLS-LOC
  output_dir: C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\dinov2_nosetup\dinov2\dinov2_nosetup\dinov2
  saveckp_freq: 20
  seed: 0
  num_workers: 0
  OFFICIAL_EPOCH_LENGTH: 1250
  cache_dataset: true
  centering: centering
student:
  arch: vit_large
  patch_size: 16
  drop_path_rate: 0.3
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: mlp
  block_chunks: 4
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
  num_register_tokens: 0
  interpolate_antialias: false
  interpolate_offset: 0.1
teacher:
  momentum_teacher: 0.992
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 30
optim:
  epochs: 100
  weight_decay: 0.04
  weight_decay_end: 0.4
  base_lr: 0.004
  lr: 0.001
  warmup_epochs: 10
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 0.9
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 8
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 224
  local_crops_size: 96
evaluation:
  eval_period_iterations: 12500

I20240906 13:20:21 2972 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20240906 13:20:23 2972 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20240906 13:20:26 2972 dinov2 ssl_meta_arch.py:43] OPTIONS -- architecture : embed_dim: 1024
I20240906 13:20:26 2972 dinov2 ssl_meta_arch.py:58] OPTIONS -- DINO
I20240906 13:20:26 2972 dinov2 ssl_meta_arch.py:60] OPTIONS -- DINO -- loss_weight: 1.0
I20240906 13:20:26 2972 dinov2 ssl_meta_arch.py:61] OPTIONS -- DINO -- head_n_prototypes: 65536
I20240906 13:20:26 2972 dinov2 ssl_meta_arch.py:62] OPTIONS -- DINO -- head_bottleneck_dim: 256
I20240906 13:20:26 2972 dinov2 ssl_meta_arch.py:63] OPTIONS -- DINO -- head_hidden_dim: 2048
I20240906 13:20:26 2972 dinov2 ssl_meta_arch.py:75] OPTIONS -- DINO -- applying KOLEO regularization
I20240906 13:20:26 2972 dinov2 ssl_meta_arch.py:85] OPTIONS -- IBOT
I20240906 13:20:26 2972 dinov2 ssl_meta_arch.py:86] OPTIONS -- IBOT -- loss_weight: 1.0
I20240906 13:20:26 2972 dinov2 ssl_meta_arch.py:87] OPTIONS -- IBOT masking -- ibot_mask_ratio_tuple: [0.1, 0.5]
I20240906 13:20:26 2972 dinov2 ssl_meta_arch.py:88] OPTIONS -- IBOT masking -- ibot_mask_sample_probability: 0.5
I20240906 13:20:26 2972 dinov2 ssl_meta_arch.py:111] OPTIONS -- IBOT -- head shared with DINO
I20240906 13:20:26 2972 dinov2 ssl_meta_arch.py:121] Student and Teacher are built: they are both vit_large network.
I20240906 13:20:27 2972 dinov2 train.py:301] Model:
SSLMetaArch(
  (dino_loss): DINOLoss()
  (koleo_loss): KoLeoLoss(
    (pdist): PairwiseDistance()
  )
  (ibot_patch_loss): iBOTPatchLoss()
  (student): ModuleDict(
    (backbone): DinoVisionTransformer(
      (patch_embed): PatchEmbed(
        (proj): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))
        (norm): Identity()
      )
      (blocks): ModuleList(
        (0): BlockChunk(
          (0-5): 6 x NestedTensorBlock(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): DropPath()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): DropPath()
          )
        )
        (1): BlockChunk(
          (0-5): 6 x Identity()
          (6-11): 6 x NestedTensorBlock(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): DropPath()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): DropPath()
          )
        )
        (2): BlockChunk(
          (0-11): 12 x Identity()
          (12-17): 6 x NestedTensorBlock(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): DropPath()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): DropPath()
          )
        )
        (3): BlockChunk(
          (0-17): 18 x Identity()
          (18-23): 6 x NestedTensorBlock(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): DropPath()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): DropPath()
          )
        )
      )
      (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (head): Identity()
    )
    (dino_head): DINOHead(
      (mlp): Sequential(
        (0): Linear(in_features=1024, out_features=2048, bias=True)
        (1): GELU(approximate='none')
        (2): Linear(in_features=2048, out_features=2048, bias=True)
        (3): GELU(approximate='none')
        (4): Linear(in_features=2048, out_features=256, bias=True)
      )
      (last_layer): Linear(in_features=256, out_features=65536, bias=False)
    )
  )
  (teacher): ModuleDict(
    (backbone): DinoVisionTransformer(
      (patch_embed): PatchEmbed(
        (proj): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))
        (norm): Identity()
      )
      (blocks): ModuleList(
        (0): BlockChunk(
          (0-5): 6 x NestedTensorBlock(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): Identity()
          )
        )
        (1): BlockChunk(
          (0-5): 6 x Identity()
          (6-11): 6 x NestedTensorBlock(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): Identity()
          )
        )
        (2): BlockChunk(
          (0-11): 12 x Identity()
          (12-17): 6 x NestedTensorBlock(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): Identity()
          )
        )
        (3): BlockChunk(
          (0-17): 18 x Identity()
          (18-23): 6 x NestedTensorBlock(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): Identity()
          )
        )
      )
      (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (head): Identity()
    )
    (dino_head): DINOHead(
      (mlp): Sequential(
        (0): Linear(in_features=1024, out_features=2048, bias=True)
        (1): GELU(approximate='none')
        (2): Linear(in_features=2048, out_features=2048, bias=True)
        (3): GELU(approximate='none')
        (4): Linear(in_features=2048, out_features=256, bias=True)
      )
      (last_layer): Linear(in_features=256, out_features=65536, bias=False)
    )
  )
)
I20240906 13:20:27 2972 dinov2 param_groups.py:54] chunked fsdp
I20240906 13:20:27 2972 dinov2 param_groups.py:87] cls_token: lr_multiplier: 0.0717897987691853, wd_multiplier: 1.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] pos_embed: lr_multiplier: 0.0717897987691853, wd_multiplier: 1.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] mask_token: lr_multiplier: 0.0717897987691853, wd_multiplier: 1.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] patch_embed.proj.weight: lr_multiplier: 0.01435795975383706, wd_multiplier: 1.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] patch_embed.proj.bias: lr_multiplier: 0.01435795975383706, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.0.0.norm1.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.0.0.norm1.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.0.0.attn.qkv.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 1.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.0.0.attn.qkv.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.0.0.attn.proj.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 1.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.0.0.attn.proj.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.0.0.ls1.gamma: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.0.0.norm2.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.0.0.norm2.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.0.0.mlp.fc1.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 1.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.0.0.mlp.fc1.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.0.0.mlp.fc2.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 1.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.0.0.mlp.fc2.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.0.0.ls2.gamma: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.0.1.norm1.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.0.1.norm1.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.0.1.attn.qkv.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 1.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.0.1.attn.qkv.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.0.1.attn.proj.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 1.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.0.1.attn.proj.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.0.1.ls1.gamma: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.0.1.norm2.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.0.1.norm2.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.0.1.mlp.fc1.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 1.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.0.1.mlp.fc1.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.0.1.mlp.fc2.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 1.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.0.1.mlp.fc2.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.0.1.ls2.gamma: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.0.2.norm1.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.0.2.norm1.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.0.2.attn.qkv.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 1.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.0.2.attn.qkv.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.0.2.attn.proj.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 1.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.0.2.attn.proj.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.0.2.ls1.gamma: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.0.2.norm2.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.0.2.norm2.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.0.2.mlp.fc1.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 1.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.0.2.mlp.fc1.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.0.2.mlp.fc2.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 1.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.0.2.mlp.fc2.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.0.2.ls2.gamma: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.0.3.norm1.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.0.3.norm1.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.0.3.attn.qkv.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 1.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.0.3.attn.qkv.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.0.3.attn.proj.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 1.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.0.3.attn.proj.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.0.3.ls1.gamma: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.0.3.norm2.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.0.3.norm2.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.0.3.mlp.fc1.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 1.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.0.3.mlp.fc1.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.0.3.mlp.fc2.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 1.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.0.3.mlp.fc2.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.0.3.ls2.gamma: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.0.4.norm1.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.0.4.norm1.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.0.4.attn.qkv.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 1.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.0.4.attn.qkv.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.0.4.attn.proj.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 1.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.0.4.attn.proj.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.0.4.ls1.gamma: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.0.4.norm2.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.0.4.norm2.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.0.4.mlp.fc1.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 1.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.0.4.mlp.fc1.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.0.4.mlp.fc2.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 1.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.0.4.mlp.fc2.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.0.4.ls2.gamma: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.0.5.norm1.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.0.5.norm1.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.0.5.attn.qkv.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 1.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.0.5.attn.qkv.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.0.5.attn.proj.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 1.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.0.5.attn.proj.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.0.5.ls1.gamma: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.0.5.norm2.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.0.5.norm2.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.0.5.mlp.fc1.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 1.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.0.5.mlp.fc1.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.0.5.mlp.fc2.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 1.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.0.5.mlp.fc2.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.0.5.ls2.gamma: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.1.6.norm1.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.1.6.norm1.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.1.6.attn.qkv.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 1.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.1.6.attn.qkv.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.1.6.attn.proj.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 1.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.1.6.attn.proj.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.1.6.ls1.gamma: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.1.6.norm2.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.1.6.norm2.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.1.6.mlp.fc1.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 1.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.1.6.mlp.fc1.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.1.6.mlp.fc2.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 1.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.1.6.mlp.fc2.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.1.6.ls2.gamma: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.1.7.norm1.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.1.7.norm1.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.1.7.attn.qkv.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 1.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.1.7.attn.qkv.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.1.7.attn.proj.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 1.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.1.7.attn.proj.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.1.7.ls1.gamma: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.1.7.norm2.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.1.7.norm2.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.1.7.mlp.fc1.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 1.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.1.7.mlp.fc1.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.1.7.mlp.fc2.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 1.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.1.7.mlp.fc2.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.1.7.ls2.gamma: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.1.8.norm1.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.1.8.norm1.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.1.8.attn.qkv.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 1.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.1.8.attn.qkv.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.1.8.attn.proj.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 1.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.1.8.attn.proj.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.1.8.ls1.gamma: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.1.8.norm2.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.1.8.norm2.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.1.8.mlp.fc1.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 1.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.1.8.mlp.fc1.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.1.8.mlp.fc2.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 1.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.1.8.mlp.fc2.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.1.8.ls2.gamma: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.1.9.norm1.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.1.9.norm1.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.1.9.attn.qkv.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 1.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.1.9.attn.qkv.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.1.9.attn.proj.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 1.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.1.9.attn.proj.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.1.9.ls1.gamma: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.1.9.norm2.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.1.9.norm2.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.1.9.mlp.fc1.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 1.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.1.9.mlp.fc1.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.1.9.mlp.fc2.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 1.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.1.9.mlp.fc2.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.1.9.ls2.gamma: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.1.10.norm1.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.1.10.norm1.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.1.10.attn.qkv.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 1.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.1.10.attn.qkv.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.1.10.attn.proj.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 1.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.1.10.attn.proj.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.1.10.ls1.gamma: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.1.10.norm2.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.1.10.norm2.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.1.10.mlp.fc1.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 1.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.1.10.mlp.fc1.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.1.10.mlp.fc2.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 1.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.1.10.mlp.fc2.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.1.10.ls2.gamma: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.1.11.norm1.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.1.11.norm1.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.1.11.attn.qkv.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 1.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.1.11.attn.qkv.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.1.11.attn.proj.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 1.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.1.11.attn.proj.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.1.11.ls1.gamma: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.1.11.norm2.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.1.11.norm2.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.1.11.mlp.fc1.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 1.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.1.11.mlp.fc1.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.1.11.mlp.fc2.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 1.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.1.11.mlp.fc2.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.1.11.ls2.gamma: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.2.12.norm1.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.2.12.norm1.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.2.12.attn.qkv.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 1.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.2.12.attn.qkv.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.2.12.attn.proj.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 1.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.2.12.attn.proj.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.2.12.ls1.gamma: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.2.12.norm2.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.2.12.norm2.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.2.12.mlp.fc1.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 1.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.2.12.mlp.fc1.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.2.12.mlp.fc2.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 1.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.2.12.mlp.fc2.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.2.12.ls2.gamma: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.2.13.norm1.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.2.13.norm1.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.2.13.attn.qkv.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 1.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.2.13.attn.qkv.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.2.13.attn.proj.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 1.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.2.13.attn.proj.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.2.13.ls1.gamma: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.2.13.norm2.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.2.13.norm2.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.2.13.mlp.fc1.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 1.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.2.13.mlp.fc1.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.2.13.mlp.fc2.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 1.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.2.13.mlp.fc2.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.2.13.ls2.gamma: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.2.14.norm1.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.2.14.norm1.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.2.14.attn.qkv.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 1.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.2.14.attn.qkv.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.2.14.attn.proj.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 1.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.2.14.attn.proj.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.2.14.ls1.gamma: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.2.14.norm2.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.2.14.norm2.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.2.14.mlp.fc1.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 1.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.2.14.mlp.fc1.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.2.14.mlp.fc2.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 1.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.2.14.mlp.fc2.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.2.14.ls2.gamma: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.2.15.norm1.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.2.15.norm1.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.2.15.attn.qkv.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 1.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.2.15.attn.qkv.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.2.15.attn.proj.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 1.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.2.15.attn.proj.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.2.15.ls1.gamma: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.2.15.norm2.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.2.15.norm2.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.2.15.mlp.fc1.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 1.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.2.15.mlp.fc1.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.2.15.mlp.fc2.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 1.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.2.15.mlp.fc2.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.2.15.ls2.gamma: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.2.16.norm1.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.2.16.norm1.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.2.16.attn.qkv.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 1.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.2.16.attn.qkv.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.2.16.attn.proj.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 1.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.2.16.attn.proj.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.2.16.ls1.gamma: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.2.16.norm2.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.2.16.norm2.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.2.16.mlp.fc1.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 1.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.2.16.mlp.fc1.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.2.16.mlp.fc2.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 1.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.2.16.mlp.fc2.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.2.16.ls2.gamma: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.2.17.norm1.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.2.17.norm1.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.2.17.attn.qkv.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 1.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.2.17.attn.qkv.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.2.17.attn.proj.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 1.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.2.17.attn.proj.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.2.17.ls1.gamma: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.2.17.norm2.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.2.17.norm2.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.2.17.mlp.fc1.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 1.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.2.17.mlp.fc1.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.2.17.mlp.fc2.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 1.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.2.17.mlp.fc2.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.2.17.ls2.gamma: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.3.18.norm1.weight: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.3.18.norm1.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.3.18.attn.qkv.weight: lr_multiplier: 0.531441, wd_multiplier: 1.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.3.18.attn.qkv.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.3.18.attn.proj.weight: lr_multiplier: 0.531441, wd_multiplier: 1.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.3.18.attn.proj.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.3.18.ls1.gamma: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.3.18.norm2.weight: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.3.18.norm2.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.3.18.mlp.fc1.weight: lr_multiplier: 0.531441, wd_multiplier: 1.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.3.18.mlp.fc1.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.3.18.mlp.fc2.weight: lr_multiplier: 0.531441, wd_multiplier: 1.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.3.18.mlp.fc2.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.3.18.ls2.gamma: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.3.19.norm1.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.3.19.norm1.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.3.19.attn.qkv.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 1.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.3.19.attn.qkv.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.3.19.attn.proj.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 1.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.3.19.attn.proj.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.3.19.ls1.gamma: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.3.19.norm2.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.3.19.norm2.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.3.19.mlp.fc1.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 1.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.3.19.mlp.fc1.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.3.19.mlp.fc2.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 1.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.3.19.mlp.fc2.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.3.19.ls2.gamma: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.3.20.norm1.weight: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.3.20.norm1.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.3.20.attn.qkv.weight: lr_multiplier: 0.6561, wd_multiplier: 1.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.3.20.attn.qkv.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.3.20.attn.proj.weight: lr_multiplier: 0.6561, wd_multiplier: 1.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.3.20.attn.proj.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.3.20.ls1.gamma: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.3.20.norm2.weight: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.3.20.norm2.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.3.20.mlp.fc1.weight: lr_multiplier: 0.6561, wd_multiplier: 1.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.3.20.mlp.fc1.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.3.20.mlp.fc2.weight: lr_multiplier: 0.6561, wd_multiplier: 1.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.3.20.mlp.fc2.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.3.20.ls2.gamma: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.3.21.norm1.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.3.21.norm1.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.3.21.attn.qkv.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 1.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.3.21.attn.qkv.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.3.21.attn.proj.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 1.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.3.21.attn.proj.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.3.21.ls1.gamma: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.3.21.norm2.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.3.21.norm2.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.3.21.mlp.fc1.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 1.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.3.21.mlp.fc1.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.3.21.mlp.fc2.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 1.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.3.21.mlp.fc2.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.3.21.ls2.gamma: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.3.22.norm1.weight: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.3.22.norm1.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.3.22.attn.qkv.weight: lr_multiplier: 0.81, wd_multiplier: 1.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.3.22.attn.qkv.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.3.22.attn.proj.weight: lr_multiplier: 0.81, wd_multiplier: 1.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.3.22.attn.proj.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.3.22.ls1.gamma: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.3.22.norm2.weight: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.3.22.norm2.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.3.22.mlp.fc1.weight: lr_multiplier: 0.81, wd_multiplier: 1.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.3.22.mlp.fc1.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.3.22.mlp.fc2.weight: lr_multiplier: 0.81, wd_multiplier: 1.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.3.22.mlp.fc2.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.3.22.ls2.gamma: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.3.23.norm1.weight: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.3.23.norm1.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.3.23.attn.qkv.weight: lr_multiplier: 0.9, wd_multiplier: 1.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.3.23.attn.qkv.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.3.23.attn.proj.weight: lr_multiplier: 0.9, wd_multiplier: 1.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.3.23.attn.proj.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.3.23.ls1.gamma: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.3.23.norm2.weight: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.3.23.norm2.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.3.23.mlp.fc1.weight: lr_multiplier: 0.9, wd_multiplier: 1.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.3.23.mlp.fc1.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.3.23.mlp.fc2.weight: lr_multiplier: 0.9, wd_multiplier: 1.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.3.23.mlp.fc2.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] blocks.3.23.ls2.gamma: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] norm.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] norm.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 ssl_meta_arch.py:378] fusing param groups
I20240906 13:20:27 2972 dinov2 param_groups.py:64] else code branch
I20240906 13:20:27 2972 dinov2 param_groups.py:87] mlp.0.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] mlp.0.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] mlp.2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] mlp.2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] mlp.4.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] mlp.4.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] last_layer.weight_g: lr_multiplier: 1.0, wd_multiplier: 1.0
I20240906 13:20:27 2972 dinov2 param_groups.py:87] last_layer.weight_v: lr_multiplier: 1.0, wd_multiplier: 1.0
I20240906 13:20:27 2972 dinov2 ssl_meta_arch.py:378] fusing param groups
I20240906 13:20:27 2972 dinov2 train.py:98] Schedulers ready.
I20240906 13:20:27 2972 dinov2 augmentations.py:34] ###################################
I20240906 13:20:27 2972 dinov2 augmentations.py:35] Using data augmentation parameters:
I20240906 13:20:27 2972 dinov2 augmentations.py:36] global_crops_scale: [0.32, 1.0]
I20240906 13:20:27 2972 dinov2 augmentations.py:37] local_crops_scale: [0.05, 0.32]
I20240906 13:20:27 2972 dinov2 augmentations.py:38] local_crops_number: 8
I20240906 13:20:27 2972 dinov2 augmentations.py:39] global_crops_size: 224
I20240906 13:20:27 2972 dinov2 augmentations.py:40] local_crops_size: 96
I20240906 13:20:27 2972 dinov2 augmentations.py:41] ###################################
I20240906 13:20:27 2972 dinov2 loaders.py:89] using dataset: "ImageNet:split=TRAIN:root=C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\datasets\ILSVRC\Data\CLS-LOC:extra=C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\datasets\ILSVRC\Data\CLS-LOC"
I20240906 13:20:27 2972 dinov2 loaders.py:94] # of dataset samples: 1,281,167
I20240906 13:20:27 2972 dinov2 loaders.py:117] sampler: infinite
I20240906 13:20:27 2972 dinov2 loaders.py:211] using PyTorch data loader
I20240906 13:20:27 2972 dinov2 loaders.py:226] infinite data loader
I20240906 13:20:27 2972 dinov2 train.py:215] Starting training from iteration 0
I20240906 13:22:19 11988 dinov2 config.py:59] git:
  sha: d43546fae6805ceb22618af8cf78469b61d913c7, status: has uncommitted changes, branch: main

I20240906 13:22:19 11988 dinov2 config.py:60] config_file: configs/train/vitl16_short.yaml
eval: 
eval_only: False
no_resume: False
opts: ['train.dataset_path=ImageNet:split=TRAIN:root=C:\\Users\\ISI_UTS\\Siladittya\\iclr2023\\inexpts\\datasets\\ILSVRC\\Data\\CLS-LOC:extra=C:\\Users\\ISI_UTS\\Siladittya\\iclr2023\\inexpts\\datasets\\ILSVRC\\Data\\CLS-LOC', 'train.output_dir=C:\\Users\\ISI_UTS\\Siladittya\\iclr2023\\inexpts\\dinov2_nosetup\\dinov2\\dinov2_nosetup\\dinov2']
output_dir: C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\dinov2_nosetup\dinov2\dinov2_nosetup\dinov2
I20240906 13:22:19 11988 dinov2 config.py:26] sqrt scaling learning rate; base: 0.004, new: 0.001
I20240906 13:22:19 11988 dinov2 config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
dino:
  loss_weight: 1.0
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
  koleo_loss_weight: 0.1
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  separate_head: false
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
train:
  batch_size_per_gpu: 64
  dataset_path: ImageNet:split=TRAIN:root=C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\datasets\ILSVRC\Data\CLS-LOC:extra=C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\datasets\ILSVRC\Data\CLS-LOC
  output_dir: C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\dinov2_nosetup\dinov2\dinov2_nosetup\dinov2
  saveckp_freq: 20
  seed: 0
  num_workers: 0
  OFFICIAL_EPOCH_LENGTH: 1250
  cache_dataset: true
  centering: centering
student:
  arch: vit_large
  patch_size: 16
  drop_path_rate: 0.3
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: mlp
  block_chunks: 4
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
  num_register_tokens: 0
  interpolate_antialias: false
  interpolate_offset: 0.1
teacher:
  momentum_teacher: 0.992
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 30
optim:
  epochs: 100
  weight_decay: 0.04
  weight_decay_end: 0.4
  base_lr: 0.004
  lr: 0.001
  warmup_epochs: 10
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 0.9
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 8
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 224
  local_crops_size: 96
evaluation:
  eval_period_iterations: 12500

I20240906 13:22:19 11988 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20240906 13:22:21 11988 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20240906 13:22:24 11988 dinov2 ssl_meta_arch.py:43] OPTIONS -- architecture : embed_dim: 1024
I20240906 13:22:24 11988 dinov2 ssl_meta_arch.py:58] OPTIONS -- DINO
I20240906 13:22:24 11988 dinov2 ssl_meta_arch.py:60] OPTIONS -- DINO -- loss_weight: 1.0
I20240906 13:22:24 11988 dinov2 ssl_meta_arch.py:61] OPTIONS -- DINO -- head_n_prototypes: 65536
I20240906 13:22:24 11988 dinov2 ssl_meta_arch.py:62] OPTIONS -- DINO -- head_bottleneck_dim: 256
I20240906 13:22:24 11988 dinov2 ssl_meta_arch.py:63] OPTIONS -- DINO -- head_hidden_dim: 2048
I20240906 13:22:24 11988 dinov2 ssl_meta_arch.py:75] OPTIONS -- DINO -- applying KOLEO regularization
I20240906 13:22:24 11988 dinov2 ssl_meta_arch.py:85] OPTIONS -- IBOT
I20240906 13:22:24 11988 dinov2 ssl_meta_arch.py:86] OPTIONS -- IBOT -- loss_weight: 1.0
I20240906 13:22:24 11988 dinov2 ssl_meta_arch.py:87] OPTIONS -- IBOT masking -- ibot_mask_ratio_tuple: [0.1, 0.5]
I20240906 13:22:24 11988 dinov2 ssl_meta_arch.py:88] OPTIONS -- IBOT masking -- ibot_mask_sample_probability: 0.5
I20240906 13:22:24 11988 dinov2 ssl_meta_arch.py:111] OPTIONS -- IBOT -- head shared with DINO
I20240906 13:22:24 11988 dinov2 ssl_meta_arch.py:121] Student and Teacher are built: they are both vit_large network.
I20240906 13:22:25 11988 dinov2 train.py:301] Model:
SSLMetaArch(
  (dino_loss): DINOLoss()
  (koleo_loss): KoLeoLoss(
    (pdist): PairwiseDistance()
  )
  (ibot_patch_loss): iBOTPatchLoss()
  (student): ModuleDict(
    (backbone): DinoVisionTransformer(
      (patch_embed): PatchEmbed(
        (proj): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))
        (norm): Identity()
      )
      (blocks): ModuleList(
        (0): BlockChunk(
          (0-5): 6 x NestedTensorBlock(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): DropPath()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): DropPath()
          )
        )
        (1): BlockChunk(
          (0-5): 6 x Identity()
          (6-11): 6 x NestedTensorBlock(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): DropPath()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): DropPath()
          )
        )
        (2): BlockChunk(
          (0-11): 12 x Identity()
          (12-17): 6 x NestedTensorBlock(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): DropPath()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): DropPath()
          )
        )
        (3): BlockChunk(
          (0-17): 18 x Identity()
          (18-23): 6 x NestedTensorBlock(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): DropPath()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): DropPath()
          )
        )
      )
      (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (head): Identity()
    )
    (dino_head): DINOHead(
      (mlp): Sequential(
        (0): Linear(in_features=1024, out_features=2048, bias=True)
        (1): GELU(approximate='none')
        (2): Linear(in_features=2048, out_features=2048, bias=True)
        (3): GELU(approximate='none')
        (4): Linear(in_features=2048, out_features=256, bias=True)
      )
      (last_layer): Linear(in_features=256, out_features=65536, bias=False)
    )
  )
  (teacher): ModuleDict(
    (backbone): DinoVisionTransformer(
      (patch_embed): PatchEmbed(
        (proj): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))
        (norm): Identity()
      )
      (blocks): ModuleList(
        (0): BlockChunk(
          (0-5): 6 x NestedTensorBlock(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): Identity()
          )
        )
        (1): BlockChunk(
          (0-5): 6 x Identity()
          (6-11): 6 x NestedTensorBlock(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): Identity()
          )
        )
        (2): BlockChunk(
          (0-11): 12 x Identity()
          (12-17): 6 x NestedTensorBlock(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): Identity()
          )
        )
        (3): BlockChunk(
          (0-17): 18 x Identity()
          (18-23): 6 x NestedTensorBlock(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): Identity()
          )
        )
      )
      (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (head): Identity()
    )
    (dino_head): DINOHead(
      (mlp): Sequential(
        (0): Linear(in_features=1024, out_features=2048, bias=True)
        (1): GELU(approximate='none')
        (2): Linear(in_features=2048, out_features=2048, bias=True)
        (3): GELU(approximate='none')
        (4): Linear(in_features=2048, out_features=256, bias=True)
      )
      (last_layer): Linear(in_features=256, out_features=65536, bias=False)
    )
  )
)
I20240906 13:22:25 11988 dinov2 param_groups.py:54] chunked fsdp
I20240906 13:22:25 11988 dinov2 param_groups.py:87] cls_token: lr_multiplier: 0.0717897987691853, wd_multiplier: 1.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] pos_embed: lr_multiplier: 0.0717897987691853, wd_multiplier: 1.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] mask_token: lr_multiplier: 0.0717897987691853, wd_multiplier: 1.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] patch_embed.proj.weight: lr_multiplier: 0.01435795975383706, wd_multiplier: 1.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] patch_embed.proj.bias: lr_multiplier: 0.01435795975383706, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.0.0.norm1.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.0.0.norm1.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.0.0.attn.qkv.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 1.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.0.0.attn.qkv.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.0.0.attn.proj.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 1.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.0.0.attn.proj.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.0.0.ls1.gamma: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.0.0.norm2.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.0.0.norm2.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.0.0.mlp.fc1.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 1.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.0.0.mlp.fc1.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.0.0.mlp.fc2.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 1.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.0.0.mlp.fc2.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.0.0.ls2.gamma: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.0.1.norm1.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.0.1.norm1.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.0.1.attn.qkv.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 1.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.0.1.attn.qkv.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.0.1.attn.proj.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 1.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.0.1.attn.proj.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.0.1.ls1.gamma: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.0.1.norm2.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.0.1.norm2.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.0.1.mlp.fc1.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 1.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.0.1.mlp.fc1.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.0.1.mlp.fc2.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 1.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.0.1.mlp.fc2.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.0.1.ls2.gamma: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.0.2.norm1.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.0.2.norm1.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.0.2.attn.qkv.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 1.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.0.2.attn.qkv.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.0.2.attn.proj.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 1.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.0.2.attn.proj.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.0.2.ls1.gamma: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.0.2.norm2.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.0.2.norm2.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.0.2.mlp.fc1.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 1.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.0.2.mlp.fc1.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.0.2.mlp.fc2.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 1.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.0.2.mlp.fc2.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.0.2.ls2.gamma: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.0.3.norm1.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.0.3.norm1.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.0.3.attn.qkv.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 1.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.0.3.attn.qkv.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.0.3.attn.proj.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 1.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.0.3.attn.proj.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.0.3.ls1.gamma: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.0.3.norm2.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.0.3.norm2.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.0.3.mlp.fc1.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 1.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.0.3.mlp.fc1.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.0.3.mlp.fc2.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 1.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.0.3.mlp.fc2.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.0.3.ls2.gamma: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.0.4.norm1.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.0.4.norm1.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.0.4.attn.qkv.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 1.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.0.4.attn.qkv.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.0.4.attn.proj.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 1.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.0.4.attn.proj.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.0.4.ls1.gamma: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.0.4.norm2.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.0.4.norm2.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.0.4.mlp.fc1.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 1.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.0.4.mlp.fc1.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.0.4.mlp.fc2.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 1.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.0.4.mlp.fc2.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.0.4.ls2.gamma: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.0.5.norm1.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.0.5.norm1.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.0.5.attn.qkv.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 1.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.0.5.attn.qkv.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.0.5.attn.proj.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 1.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.0.5.attn.proj.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.0.5.ls1.gamma: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.0.5.norm2.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.0.5.norm2.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.0.5.mlp.fc1.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 1.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.0.5.mlp.fc1.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.0.5.mlp.fc2.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 1.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.0.5.mlp.fc2.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.0.5.ls2.gamma: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.1.6.norm1.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.1.6.norm1.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.1.6.attn.qkv.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 1.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.1.6.attn.qkv.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.1.6.attn.proj.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 1.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.1.6.attn.proj.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.1.6.ls1.gamma: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.1.6.norm2.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.1.6.norm2.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.1.6.mlp.fc1.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 1.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.1.6.mlp.fc1.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.1.6.mlp.fc2.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 1.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.1.6.mlp.fc2.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.1.6.ls2.gamma: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.1.7.norm1.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.1.7.norm1.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.1.7.attn.qkv.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 1.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.1.7.attn.qkv.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.1.7.attn.proj.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 1.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.1.7.attn.proj.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.1.7.ls1.gamma: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.1.7.norm2.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.1.7.norm2.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.1.7.mlp.fc1.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 1.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.1.7.mlp.fc1.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.1.7.mlp.fc2.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 1.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.1.7.mlp.fc2.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.1.7.ls2.gamma: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.1.8.norm1.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.1.8.norm1.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.1.8.attn.qkv.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 1.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.1.8.attn.qkv.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.1.8.attn.proj.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 1.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.1.8.attn.proj.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.1.8.ls1.gamma: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.1.8.norm2.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.1.8.norm2.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.1.8.mlp.fc1.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 1.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.1.8.mlp.fc1.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.1.8.mlp.fc2.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 1.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.1.8.mlp.fc2.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.1.8.ls2.gamma: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.1.9.norm1.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.1.9.norm1.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.1.9.attn.qkv.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 1.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.1.9.attn.qkv.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.1.9.attn.proj.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 1.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.1.9.attn.proj.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.1.9.ls1.gamma: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.1.9.norm2.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.1.9.norm2.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.1.9.mlp.fc1.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 1.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.1.9.mlp.fc1.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.1.9.mlp.fc2.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 1.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.1.9.mlp.fc2.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.1.9.ls2.gamma: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.1.10.norm1.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.1.10.norm1.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.1.10.attn.qkv.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 1.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.1.10.attn.qkv.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.1.10.attn.proj.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 1.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.1.10.attn.proj.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.1.10.ls1.gamma: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.1.10.norm2.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.1.10.norm2.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.1.10.mlp.fc1.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 1.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.1.10.mlp.fc1.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.1.10.mlp.fc2.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 1.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.1.10.mlp.fc2.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.1.10.ls2.gamma: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.1.11.norm1.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.1.11.norm1.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.1.11.attn.qkv.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 1.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.1.11.attn.qkv.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.1.11.attn.proj.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 1.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.1.11.attn.proj.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.1.11.ls1.gamma: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.1.11.norm2.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.1.11.norm2.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.1.11.mlp.fc1.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 1.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.1.11.mlp.fc1.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.1.11.mlp.fc2.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 1.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.1.11.mlp.fc2.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.1.11.ls2.gamma: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.2.12.norm1.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.2.12.norm1.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.2.12.attn.qkv.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 1.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.2.12.attn.qkv.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.2.12.attn.proj.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 1.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.2.12.attn.proj.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.2.12.ls1.gamma: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.2.12.norm2.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.2.12.norm2.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.2.12.mlp.fc1.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 1.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.2.12.mlp.fc1.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.2.12.mlp.fc2.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 1.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.2.12.mlp.fc2.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.2.12.ls2.gamma: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.2.13.norm1.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.2.13.norm1.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.2.13.attn.qkv.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 1.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.2.13.attn.qkv.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.2.13.attn.proj.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 1.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.2.13.attn.proj.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.2.13.ls1.gamma: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.2.13.norm2.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.2.13.norm2.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.2.13.mlp.fc1.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 1.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.2.13.mlp.fc1.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.2.13.mlp.fc2.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 1.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.2.13.mlp.fc2.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.2.13.ls2.gamma: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.2.14.norm1.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.2.14.norm1.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.2.14.attn.qkv.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 1.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.2.14.attn.qkv.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.2.14.attn.proj.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 1.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.2.14.attn.proj.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.2.14.ls1.gamma: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.2.14.norm2.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.2.14.norm2.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.2.14.mlp.fc1.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 1.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.2.14.mlp.fc1.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.2.14.mlp.fc2.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 1.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.2.14.mlp.fc2.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.2.14.ls2.gamma: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.2.15.norm1.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.2.15.norm1.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.2.15.attn.qkv.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 1.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.2.15.attn.qkv.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.2.15.attn.proj.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 1.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.2.15.attn.proj.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.2.15.ls1.gamma: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.2.15.norm2.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.2.15.norm2.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.2.15.mlp.fc1.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 1.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.2.15.mlp.fc1.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.2.15.mlp.fc2.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 1.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.2.15.mlp.fc2.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.2.15.ls2.gamma: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.2.16.norm1.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.2.16.norm1.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.2.16.attn.qkv.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 1.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.2.16.attn.qkv.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.2.16.attn.proj.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 1.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.2.16.attn.proj.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.2.16.ls1.gamma: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.2.16.norm2.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.2.16.norm2.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.2.16.mlp.fc1.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 1.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.2.16.mlp.fc1.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.2.16.mlp.fc2.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 1.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.2.16.mlp.fc2.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.2.16.ls2.gamma: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.2.17.norm1.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.2.17.norm1.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.2.17.attn.qkv.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 1.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.2.17.attn.qkv.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.2.17.attn.proj.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 1.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.2.17.attn.proj.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.2.17.ls1.gamma: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.2.17.norm2.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.2.17.norm2.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.2.17.mlp.fc1.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 1.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.2.17.mlp.fc1.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.2.17.mlp.fc2.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 1.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.2.17.mlp.fc2.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.2.17.ls2.gamma: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.3.18.norm1.weight: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.3.18.norm1.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.3.18.attn.qkv.weight: lr_multiplier: 0.531441, wd_multiplier: 1.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.3.18.attn.qkv.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.3.18.attn.proj.weight: lr_multiplier: 0.531441, wd_multiplier: 1.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.3.18.attn.proj.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.3.18.ls1.gamma: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.3.18.norm2.weight: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.3.18.norm2.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.3.18.mlp.fc1.weight: lr_multiplier: 0.531441, wd_multiplier: 1.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.3.18.mlp.fc1.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.3.18.mlp.fc2.weight: lr_multiplier: 0.531441, wd_multiplier: 1.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.3.18.mlp.fc2.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.3.18.ls2.gamma: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.3.19.norm1.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.3.19.norm1.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.3.19.attn.qkv.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 1.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.3.19.attn.qkv.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.3.19.attn.proj.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 1.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.3.19.attn.proj.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.3.19.ls1.gamma: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.3.19.norm2.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.3.19.norm2.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.3.19.mlp.fc1.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 1.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.3.19.mlp.fc1.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.3.19.mlp.fc2.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 1.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.3.19.mlp.fc2.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.3.19.ls2.gamma: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.3.20.norm1.weight: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.3.20.norm1.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.3.20.attn.qkv.weight: lr_multiplier: 0.6561, wd_multiplier: 1.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.3.20.attn.qkv.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.3.20.attn.proj.weight: lr_multiplier: 0.6561, wd_multiplier: 1.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.3.20.attn.proj.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.3.20.ls1.gamma: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.3.20.norm2.weight: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.3.20.norm2.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.3.20.mlp.fc1.weight: lr_multiplier: 0.6561, wd_multiplier: 1.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.3.20.mlp.fc1.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.3.20.mlp.fc2.weight: lr_multiplier: 0.6561, wd_multiplier: 1.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.3.20.mlp.fc2.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.3.20.ls2.gamma: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.3.21.norm1.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.3.21.norm1.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.3.21.attn.qkv.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 1.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.3.21.attn.qkv.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.3.21.attn.proj.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 1.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.3.21.attn.proj.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.3.21.ls1.gamma: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.3.21.norm2.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.3.21.norm2.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.3.21.mlp.fc1.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 1.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.3.21.mlp.fc1.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.3.21.mlp.fc2.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 1.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.3.21.mlp.fc2.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.3.21.ls2.gamma: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.3.22.norm1.weight: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.3.22.norm1.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.3.22.attn.qkv.weight: lr_multiplier: 0.81, wd_multiplier: 1.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.3.22.attn.qkv.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.3.22.attn.proj.weight: lr_multiplier: 0.81, wd_multiplier: 1.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.3.22.attn.proj.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.3.22.ls1.gamma: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.3.22.norm2.weight: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.3.22.norm2.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.3.22.mlp.fc1.weight: lr_multiplier: 0.81, wd_multiplier: 1.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.3.22.mlp.fc1.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.3.22.mlp.fc2.weight: lr_multiplier: 0.81, wd_multiplier: 1.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.3.22.mlp.fc2.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.3.22.ls2.gamma: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.3.23.norm1.weight: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.3.23.norm1.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.3.23.attn.qkv.weight: lr_multiplier: 0.9, wd_multiplier: 1.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.3.23.attn.qkv.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.3.23.attn.proj.weight: lr_multiplier: 0.9, wd_multiplier: 1.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.3.23.attn.proj.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.3.23.ls1.gamma: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.3.23.norm2.weight: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.3.23.norm2.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.3.23.mlp.fc1.weight: lr_multiplier: 0.9, wd_multiplier: 1.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.3.23.mlp.fc1.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.3.23.mlp.fc2.weight: lr_multiplier: 0.9, wd_multiplier: 1.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.3.23.mlp.fc2.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] blocks.3.23.ls2.gamma: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] norm.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] norm.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 ssl_meta_arch.py:378] fusing param groups
I20240906 13:22:25 11988 dinov2 param_groups.py:64] else code branch
I20240906 13:22:25 11988 dinov2 param_groups.py:87] mlp.0.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] mlp.0.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] mlp.2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] mlp.2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] mlp.4.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] mlp.4.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] last_layer.weight_g: lr_multiplier: 1.0, wd_multiplier: 1.0
I20240906 13:22:25 11988 dinov2 param_groups.py:87] last_layer.weight_v: lr_multiplier: 1.0, wd_multiplier: 1.0
I20240906 13:22:25 11988 dinov2 ssl_meta_arch.py:378] fusing param groups
I20240906 13:22:25 11988 dinov2 train.py:98] Schedulers ready.
I20240906 13:22:25 11988 dinov2 augmentations.py:34] ###################################
I20240906 13:22:25 11988 dinov2 augmentations.py:35] Using data augmentation parameters:
I20240906 13:22:25 11988 dinov2 augmentations.py:36] global_crops_scale: [0.32, 1.0]
I20240906 13:22:25 11988 dinov2 augmentations.py:37] local_crops_scale: [0.05, 0.32]
I20240906 13:22:25 11988 dinov2 augmentations.py:38] local_crops_number: 8
I20240906 13:22:25 11988 dinov2 augmentations.py:39] global_crops_size: 224
I20240906 13:22:25 11988 dinov2 augmentations.py:40] local_crops_size: 96
I20240906 13:22:25 11988 dinov2 augmentations.py:41] ###################################
I20240906 13:22:25 11988 dinov2 loaders.py:89] using dataset: "ImageNet:split=TRAIN:root=C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\datasets\ILSVRC\Data\CLS-LOC:extra=C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\datasets\ILSVRC\Data\CLS-LOC"
I20240906 13:22:25 11988 dinov2 loaders.py:94] # of dataset samples: 1,281,167
I20240906 13:22:25 11988 dinov2 loaders.py:117] sampler: infinite
I20240906 13:22:25 11988 dinov2 loaders.py:211] using PyTorch data loader
I20240906 13:22:25 11988 dinov2 loaders.py:226] infinite data loader
I20240906 13:22:25 11988 dinov2 train.py:215] Starting training from iteration 0
I20240906 13:31:14 16312 dinov2 config.py:59] git:
  sha: d43546fae6805ceb22618af8cf78469b61d913c7, status: has uncommitted changes, branch: main

I20240906 13:31:14 16312 dinov2 config.py:60] config_file: configs/train/vitl16_short.yaml
eval: 
eval_only: False
no_resume: False
opts: ['train.dataset_path=ImageNet:split=TRAIN:root=C:\\Users\\ISI_UTS\\Siladittya\\iclr2023\\inexpts\\datasets\\ILSVRC\\Data\\CLS-LOC:extra=C:\\Users\\ISI_UTS\\Siladittya\\iclr2023\\inexpts\\datasets\\ILSVRC\\Data\\CLS-LOC', 'train.output_dir=C:\\Users\\ISI_UTS\\Siladittya\\iclr2023\\inexpts\\dinov2_nosetup\\dinov2\\dinov2_nosetup\\dinov2']
output_dir: C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\dinov2_nosetup\dinov2\dinov2_nosetup\dinov2
I20240906 13:31:14 16312 dinov2 config.py:26] sqrt scaling learning rate; base: 0.004, new: 0.001
I20240906 13:31:14 16312 dinov2 config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
dino:
  loss_weight: 1.0
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
  koleo_loss_weight: 0.1
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  separate_head: false
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
train:
  batch_size_per_gpu: 64
  dataset_path: ImageNet:split=TRAIN:root=C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\datasets\ILSVRC\Data\CLS-LOC:extra=C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\datasets\ILSVRC\Data\CLS-LOC
  output_dir: C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\dinov2_nosetup\dinov2\dinov2_nosetup\dinov2
  saveckp_freq: 20
  seed: 0
  num_workers: 0
  OFFICIAL_EPOCH_LENGTH: 1250
  cache_dataset: true
  centering: centering
student:
  arch: vit_large
  patch_size: 16
  drop_path_rate: 0.3
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: mlp
  block_chunks: 4
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
  num_register_tokens: 0
  interpolate_antialias: false
  interpolate_offset: 0.1
teacher:
  momentum_teacher: 0.992
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 30
optim:
  epochs: 100
  weight_decay: 0.04
  weight_decay_end: 0.4
  base_lr: 0.004
  lr: 0.001
  warmup_epochs: 10
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 0.9
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 8
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 224
  local_crops_size: 96
evaluation:
  eval_period_iterations: 12500

I20240906 13:31:14 16312 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20240906 13:31:17 16312 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20240906 13:31:20 16312 dinov2 ssl_meta_arch.py:43] OPTIONS -- architecture : embed_dim: 1024
I20240906 13:31:20 16312 dinov2 ssl_meta_arch.py:58] OPTIONS -- DINO
I20240906 13:31:20 16312 dinov2 ssl_meta_arch.py:60] OPTIONS -- DINO -- loss_weight: 1.0
I20240906 13:31:20 16312 dinov2 ssl_meta_arch.py:61] OPTIONS -- DINO -- head_n_prototypes: 65536
I20240906 13:31:20 16312 dinov2 ssl_meta_arch.py:62] OPTIONS -- DINO -- head_bottleneck_dim: 256
I20240906 13:31:20 16312 dinov2 ssl_meta_arch.py:63] OPTIONS -- DINO -- head_hidden_dim: 2048
I20240906 13:31:20 16312 dinov2 ssl_meta_arch.py:75] OPTIONS -- DINO -- applying KOLEO regularization
I20240906 13:31:20 16312 dinov2 ssl_meta_arch.py:85] OPTIONS -- IBOT
I20240906 13:31:20 16312 dinov2 ssl_meta_arch.py:86] OPTIONS -- IBOT -- loss_weight: 1.0
I20240906 13:31:20 16312 dinov2 ssl_meta_arch.py:87] OPTIONS -- IBOT masking -- ibot_mask_ratio_tuple: [0.1, 0.5]
I20240906 13:31:20 16312 dinov2 ssl_meta_arch.py:88] OPTIONS -- IBOT masking -- ibot_mask_sample_probability: 0.5
I20240906 13:31:20 16312 dinov2 ssl_meta_arch.py:111] OPTIONS -- IBOT -- head shared with DINO
I20240906 13:31:20 16312 dinov2 ssl_meta_arch.py:121] Student and Teacher are built: they are both vit_large network.
I20240906 13:31:21 16312 dinov2 train.py:302] Model:
SSLMetaArch(
  (dino_loss): DINOLoss()
  (koleo_loss): KoLeoLoss(
    (pdist): PairwiseDistance()
  )
  (ibot_patch_loss): iBOTPatchLoss()
  (student): ModuleDict(
    (backbone): DinoVisionTransformer(
      (patch_embed): PatchEmbed(
        (proj): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))
        (norm): Identity()
      )
      (blocks): ModuleList(
        (0): BlockChunk(
          (0-5): 6 x NestedTensorBlock(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): DropPath()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): DropPath()
          )
        )
        (1): BlockChunk(
          (0-5): 6 x Identity()
          (6-11): 6 x NestedTensorBlock(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): DropPath()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): DropPath()
          )
        )
        (2): BlockChunk(
          (0-11): 12 x Identity()
          (12-17): 6 x NestedTensorBlock(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): DropPath()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): DropPath()
          )
        )
        (3): BlockChunk(
          (0-17): 18 x Identity()
          (18-23): 6 x NestedTensorBlock(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): DropPath()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): DropPath()
          )
        )
      )
      (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (head): Identity()
    )
    (dino_head): DINOHead(
      (mlp): Sequential(
        (0): Linear(in_features=1024, out_features=2048, bias=True)
        (1): GELU(approximate='none')
        (2): Linear(in_features=2048, out_features=2048, bias=True)
        (3): GELU(approximate='none')
        (4): Linear(in_features=2048, out_features=256, bias=True)
      )
      (last_layer): Linear(in_features=256, out_features=65536, bias=False)
    )
  )
  (teacher): ModuleDict(
    (backbone): DinoVisionTransformer(
      (patch_embed): PatchEmbed(
        (proj): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))
        (norm): Identity()
      )
      (blocks): ModuleList(
        (0): BlockChunk(
          (0-5): 6 x NestedTensorBlock(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): Identity()
          )
        )
        (1): BlockChunk(
          (0-5): 6 x Identity()
          (6-11): 6 x NestedTensorBlock(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): Identity()
          )
        )
        (2): BlockChunk(
          (0-11): 12 x Identity()
          (12-17): 6 x NestedTensorBlock(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): Identity()
          )
        )
        (3): BlockChunk(
          (0-17): 18 x Identity()
          (18-23): 6 x NestedTensorBlock(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): Identity()
          )
        )
      )
      (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (head): Identity()
    )
    (dino_head): DINOHead(
      (mlp): Sequential(
        (0): Linear(in_features=1024, out_features=2048, bias=True)
        (1): GELU(approximate='none')
        (2): Linear(in_features=2048, out_features=2048, bias=True)
        (3): GELU(approximate='none')
        (4): Linear(in_features=2048, out_features=256, bias=True)
      )
      (last_layer): Linear(in_features=256, out_features=65536, bias=False)
    )
  )
)
I20240906 13:34:08 18380 dinov2 config.py:59] git:
  sha: d43546fae6805ceb22618af8cf78469b61d913c7, status: has uncommitted changes, branch: main

I20240906 13:34:08 18380 dinov2 config.py:60] config_file: configs/train/vitl16_short.yaml
eval: 
eval_only: False
no_resume: False
opts: ['train.dataset_path=ImageNet:split=TRAIN:root=C:\\Users\\ISI_UTS\\Siladittya\\iclr2023\\inexpts\\datasets\\ILSVRC\\Data\\CLS-LOC:extra=C:\\Users\\ISI_UTS\\Siladittya\\iclr2023\\inexpts\\datasets\\ILSVRC\\Data\\CLS-LOC', 'train.output_dir=C:\\Users\\ISI_UTS\\Siladittya\\iclr2023\\inexpts\\dinov2_nosetup\\dinov2\\dinov2_nosetup\\dinov2']
output_dir: C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\dinov2_nosetup\dinov2\dinov2_nosetup\dinov2
I20240906 13:34:08 18380 dinov2 config.py:26] sqrt scaling learning rate; base: 0.004, new: 0.001
I20240906 13:34:08 18380 dinov2 config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
dino:
  loss_weight: 1.0
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
  koleo_loss_weight: 0.1
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  separate_head: false
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
train:
  batch_size_per_gpu: 64
  dataset_path: ImageNet:split=TRAIN:root=C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\datasets\ILSVRC\Data\CLS-LOC:extra=C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\datasets\ILSVRC\Data\CLS-LOC
  output_dir: C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\dinov2_nosetup\dinov2\dinov2_nosetup\dinov2
  saveckp_freq: 20
  seed: 0
  num_workers: 0
  OFFICIAL_EPOCH_LENGTH: 1250
  cache_dataset: true
  centering: centering
student:
  arch: vit_large
  patch_size: 16
  drop_path_rate: 0.3
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: mlp
  block_chunks: 4
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
  num_register_tokens: 0
  interpolate_antialias: false
  interpolate_offset: 0.1
teacher:
  momentum_teacher: 0.992
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 30
optim:
  epochs: 100
  weight_decay: 0.04
  weight_decay_end: 0.4
  base_lr: 0.004
  lr: 0.001
  warmup_epochs: 10
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 0.9
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 8
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 224
  local_crops_size: 96
evaluation:
  eval_period_iterations: 12500

I20240906 13:34:08 18380 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20240906 13:34:11 18380 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20240906 13:34:13 18380 dinov2 ssl_meta_arch.py:43] OPTIONS -- architecture : embed_dim: 1024
I20240906 13:34:13 18380 dinov2 ssl_meta_arch.py:58] OPTIONS -- DINO
I20240906 13:34:13 18380 dinov2 ssl_meta_arch.py:60] OPTIONS -- DINO -- loss_weight: 1.0
I20240906 13:34:13 18380 dinov2 ssl_meta_arch.py:61] OPTIONS -- DINO -- head_n_prototypes: 65536
I20240906 13:34:13 18380 dinov2 ssl_meta_arch.py:62] OPTIONS -- DINO -- head_bottleneck_dim: 256
I20240906 13:34:13 18380 dinov2 ssl_meta_arch.py:63] OPTIONS -- DINO -- head_hidden_dim: 2048
I20240906 13:34:13 18380 dinov2 ssl_meta_arch.py:75] OPTIONS -- DINO -- applying KOLEO regularization
I20240906 13:34:13 18380 dinov2 ssl_meta_arch.py:85] OPTIONS -- IBOT
I20240906 13:34:13 18380 dinov2 ssl_meta_arch.py:86] OPTIONS -- IBOT -- loss_weight: 1.0
I20240906 13:34:13 18380 dinov2 ssl_meta_arch.py:87] OPTIONS -- IBOT masking -- ibot_mask_ratio_tuple: [0.1, 0.5]
I20240906 13:34:13 18380 dinov2 ssl_meta_arch.py:88] OPTIONS -- IBOT masking -- ibot_mask_sample_probability: 0.5
I20240906 13:34:13 18380 dinov2 ssl_meta_arch.py:111] OPTIONS -- IBOT -- head shared with DINO
I20240906 13:34:13 18380 dinov2 ssl_meta_arch.py:121] Student and Teacher are built: they are both vit_large network.
I20240906 13:34:14 18380 dinov2 train.py:302] Model:
SSLMetaArch(
  (dino_loss): DINOLoss()
  (koleo_loss): KoLeoLoss(
    (pdist): PairwiseDistance()
  )
  (ibot_patch_loss): iBOTPatchLoss()
  (student): ModuleDict(
    (backbone): DinoVisionTransformer(
      (patch_embed): PatchEmbed(
        (proj): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))
        (norm): Identity()
      )
      (blocks): ModuleList(
        (0): BlockChunk(
          (0-5): 6 x NestedTensorBlock(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): DropPath()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): DropPath()
          )
        )
        (1): BlockChunk(
          (0-5): 6 x Identity()
          (6-11): 6 x NestedTensorBlock(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): DropPath()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): DropPath()
          )
        )
        (2): BlockChunk(
          (0-11): 12 x Identity()
          (12-17): 6 x NestedTensorBlock(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): DropPath()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): DropPath()
          )
        )
        (3): BlockChunk(
          (0-17): 18 x Identity()
          (18-23): 6 x NestedTensorBlock(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): DropPath()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): DropPath()
          )
        )
      )
      (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (head): Identity()
    )
    (dino_head): DINOHead(
      (mlp): Sequential(
        (0): Linear(in_features=1024, out_features=2048, bias=True)
        (1): GELU(approximate='none')
        (2): Linear(in_features=2048, out_features=2048, bias=True)
        (3): GELU(approximate='none')
        (4): Linear(in_features=2048, out_features=256, bias=True)
      )
      (last_layer): Linear(in_features=256, out_features=65536, bias=False)
    )
  )
  (teacher): ModuleDict(
    (backbone): DinoVisionTransformer(
      (patch_embed): PatchEmbed(
        (proj): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))
        (norm): Identity()
      )
      (blocks): ModuleList(
        (0): BlockChunk(
          (0-5): 6 x NestedTensorBlock(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): Identity()
          )
        )
        (1): BlockChunk(
          (0-5): 6 x Identity()
          (6-11): 6 x NestedTensorBlock(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): Identity()
          )
        )
        (2): BlockChunk(
          (0-11): 12 x Identity()
          (12-17): 6 x NestedTensorBlock(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): Identity()
          )
        )
        (3): BlockChunk(
          (0-17): 18 x Identity()
          (18-23): 6 x NestedTensorBlock(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): Identity()
          )
        )
      )
      (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (head): Identity()
    )
    (dino_head): DINOHead(
      (mlp): Sequential(
        (0): Linear(in_features=1024, out_features=2048, bias=True)
        (1): GELU(approximate='none')
        (2): Linear(in_features=2048, out_features=2048, bias=True)
        (3): GELU(approximate='none')
        (4): Linear(in_features=2048, out_features=256, bias=True)
      )
      (last_layer): Linear(in_features=256, out_features=65536, bias=False)
    )
  )
)
I20240906 13:35:33 3652 dinov2 config.py:59] git:
  sha: d43546fae6805ceb22618af8cf78469b61d913c7, status: has uncommitted changes, branch: main

I20240906 13:35:33 3652 dinov2 config.py:60] config_file: configs/train/vitl16_short.yaml
eval: 
eval_only: False
no_resume: False
opts: ['train.dataset_path=ImageNet:split=TRAIN:root=C:\\Users\\ISI_UTS\\Siladittya\\iclr2023\\inexpts\\datasets\\ILSVRC\\Data\\CLS-LOC:extra=C:\\Users\\ISI_UTS\\Siladittya\\iclr2023\\inexpts\\datasets\\ILSVRC\\Data\\CLS-LOC', 'train.output_dir=C:\\Users\\ISI_UTS\\Siladittya\\iclr2023\\inexpts\\dinov2_nosetup\\dinov2\\dinov2_nosetup\\dinov2']
output_dir: C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\dinov2_nosetup\dinov2\dinov2_nosetup\dinov2
I20240906 13:35:33 3652 dinov2 config.py:26] sqrt scaling learning rate; base: 0.004, new: 0.001
I20240906 13:35:33 3652 dinov2 config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
dino:
  loss_weight: 1.0
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
  koleo_loss_weight: 0.1
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  separate_head: false
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
train:
  batch_size_per_gpu: 64
  dataset_path: ImageNet:split=TRAIN:root=C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\datasets\ILSVRC\Data\CLS-LOC:extra=C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\datasets\ILSVRC\Data\CLS-LOC
  output_dir: C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\dinov2_nosetup\dinov2\dinov2_nosetup\dinov2
  saveckp_freq: 20
  seed: 0
  num_workers: 0
  OFFICIAL_EPOCH_LENGTH: 1250
  cache_dataset: true
  centering: centering
student:
  arch: vit_large
  patch_size: 16
  drop_path_rate: 0.3
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: mlp
  block_chunks: 4
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
  num_register_tokens: 0
  interpolate_antialias: false
  interpolate_offset: 0.1
teacher:
  momentum_teacher: 0.992
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 30
optim:
  epochs: 100
  weight_decay: 0.04
  weight_decay_end: 0.4
  base_lr: 0.004
  lr: 0.001
  warmup_epochs: 10
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 0.9
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 8
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 224
  local_crops_size: 96
evaluation:
  eval_period_iterations: 12500

I20240906 13:35:33 3652 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20240906 13:35:35 3652 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20240906 13:35:38 3652 dinov2 ssl_meta_arch.py:43] OPTIONS -- architecture : embed_dim: 1024
I20240906 13:35:38 3652 dinov2 ssl_meta_arch.py:58] OPTIONS -- DINO
I20240906 13:35:38 3652 dinov2 ssl_meta_arch.py:60] OPTIONS -- DINO -- loss_weight: 1.0
I20240906 13:35:38 3652 dinov2 ssl_meta_arch.py:61] OPTIONS -- DINO -- head_n_prototypes: 65536
I20240906 13:35:38 3652 dinov2 ssl_meta_arch.py:62] OPTIONS -- DINO -- head_bottleneck_dim: 256
I20240906 13:35:38 3652 dinov2 ssl_meta_arch.py:63] OPTIONS -- DINO -- head_hidden_dim: 2048
I20240906 13:35:38 3652 dinov2 ssl_meta_arch.py:75] OPTIONS -- DINO -- applying KOLEO regularization
I20240906 13:35:38 3652 dinov2 ssl_meta_arch.py:85] OPTIONS -- IBOT
I20240906 13:35:38 3652 dinov2 ssl_meta_arch.py:86] OPTIONS -- IBOT -- loss_weight: 1.0
I20240906 13:35:38 3652 dinov2 ssl_meta_arch.py:87] OPTIONS -- IBOT masking -- ibot_mask_ratio_tuple: [0.1, 0.5]
I20240906 13:35:38 3652 dinov2 ssl_meta_arch.py:88] OPTIONS -- IBOT masking -- ibot_mask_sample_probability: 0.5
I20240906 13:35:38 3652 dinov2 ssl_meta_arch.py:111] OPTIONS -- IBOT -- head shared with DINO
I20240906 13:35:38 3652 dinov2 ssl_meta_arch.py:121] Student and Teacher are built: they are both vit_large network.
I20240906 13:35:39 3652 dinov2 train.py:302] Model:
SSLMetaArch(
  (dino_loss): DINOLoss()
  (koleo_loss): KoLeoLoss(
    (pdist): PairwiseDistance()
  )
  (ibot_patch_loss): iBOTPatchLoss()
  (student): ModuleDict(
    (backbone): DinoVisionTransformer(
      (patch_embed): PatchEmbed(
        (proj): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))
        (norm): Identity()
      )
      (blocks): ModuleList(
        (0): BlockChunk(
          (0-5): 6 x NestedTensorBlock(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): DropPath()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): DropPath()
          )
        )
        (1): BlockChunk(
          (0-5): 6 x Identity()
          (6-11): 6 x NestedTensorBlock(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): DropPath()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): DropPath()
          )
        )
        (2): BlockChunk(
          (0-11): 12 x Identity()
          (12-17): 6 x NestedTensorBlock(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): DropPath()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): DropPath()
          )
        )
        (3): BlockChunk(
          (0-17): 18 x Identity()
          (18-23): 6 x NestedTensorBlock(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): DropPath()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): DropPath()
          )
        )
      )
      (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (head): Identity()
    )
    (dino_head): DINOHead(
      (mlp): Sequential(
        (0): Linear(in_features=1024, out_features=2048, bias=True)
        (1): GELU(approximate='none')
        (2): Linear(in_features=2048, out_features=2048, bias=True)
        (3): GELU(approximate='none')
        (4): Linear(in_features=2048, out_features=256, bias=True)
      )
      (last_layer): Linear(in_features=256, out_features=65536, bias=False)
    )
  )
  (teacher): ModuleDict(
    (backbone): DinoVisionTransformer(
      (patch_embed): PatchEmbed(
        (proj): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))
        (norm): Identity()
      )
      (blocks): ModuleList(
        (0): BlockChunk(
          (0-5): 6 x NestedTensorBlock(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): Identity()
          )
        )
        (1): BlockChunk(
          (0-5): 6 x Identity()
          (6-11): 6 x NestedTensorBlock(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): Identity()
          )
        )
        (2): BlockChunk(
          (0-11): 12 x Identity()
          (12-17): 6 x NestedTensorBlock(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): Identity()
          )
        )
        (3): BlockChunk(
          (0-17): 18 x Identity()
          (18-23): 6 x NestedTensorBlock(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): Identity()
          )
        )
      )
      (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (head): Identity()
    )
    (dino_head): DINOHead(
      (mlp): Sequential(
        (0): Linear(in_features=1024, out_features=2048, bias=True)
        (1): GELU(approximate='none')
        (2): Linear(in_features=2048, out_features=2048, bias=True)
        (3): GELU(approximate='none')
        (4): Linear(in_features=2048, out_features=256, bias=True)
      )
      (last_layer): Linear(in_features=256, out_features=65536, bias=False)
    )
  )
)
I20240906 13:35:39 3652 dinov2 param_groups.py:54] chunked fsdp
I20240906 13:35:39 3652 dinov2 param_groups.py:87] cls_token: lr_multiplier: 0.0717897987691853, wd_multiplier: 1.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] pos_embed: lr_multiplier: 0.0717897987691853, wd_multiplier: 1.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] mask_token: lr_multiplier: 0.0717897987691853, wd_multiplier: 1.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] patch_embed.proj.weight: lr_multiplier: 0.01435795975383706, wd_multiplier: 1.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] patch_embed.proj.bias: lr_multiplier: 0.01435795975383706, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.0.0.norm1.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.0.0.norm1.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.0.0.attn.qkv.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 1.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.0.0.attn.qkv.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.0.0.attn.proj.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 1.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.0.0.attn.proj.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.0.0.ls1.gamma: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.0.0.norm2.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.0.0.norm2.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.0.0.mlp.fc1.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 1.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.0.0.mlp.fc1.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.0.0.mlp.fc2.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 1.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.0.0.mlp.fc2.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.0.0.ls2.gamma: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.0.1.norm1.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.0.1.norm1.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.0.1.attn.qkv.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 1.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.0.1.attn.qkv.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.0.1.attn.proj.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 1.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.0.1.attn.proj.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.0.1.ls1.gamma: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.0.1.norm2.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.0.1.norm2.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.0.1.mlp.fc1.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 1.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.0.1.mlp.fc1.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.0.1.mlp.fc2.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 1.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.0.1.mlp.fc2.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.0.1.ls2.gamma: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.0.2.norm1.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.0.2.norm1.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.0.2.attn.qkv.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 1.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.0.2.attn.qkv.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.0.2.attn.proj.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 1.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.0.2.attn.proj.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.0.2.ls1.gamma: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.0.2.norm2.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.0.2.norm2.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.0.2.mlp.fc1.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 1.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.0.2.mlp.fc1.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.0.2.mlp.fc2.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 1.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.0.2.mlp.fc2.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.0.2.ls2.gamma: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.0.3.norm1.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.0.3.norm1.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.0.3.attn.qkv.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 1.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.0.3.attn.qkv.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.0.3.attn.proj.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 1.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.0.3.attn.proj.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.0.3.ls1.gamma: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.0.3.norm2.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.0.3.norm2.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.0.3.mlp.fc1.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 1.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.0.3.mlp.fc1.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.0.3.mlp.fc2.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 1.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.0.3.mlp.fc2.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.0.3.ls2.gamma: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.0.4.norm1.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.0.4.norm1.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.0.4.attn.qkv.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 1.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.0.4.attn.qkv.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.0.4.attn.proj.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 1.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.0.4.attn.proj.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.0.4.ls1.gamma: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.0.4.norm2.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.0.4.norm2.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.0.4.mlp.fc1.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 1.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.0.4.mlp.fc1.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.0.4.mlp.fc2.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 1.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.0.4.mlp.fc2.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.0.4.ls2.gamma: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.0.5.norm1.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.0.5.norm1.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.0.5.attn.qkv.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 1.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.0.5.attn.qkv.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.0.5.attn.proj.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 1.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.0.5.attn.proj.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.0.5.ls1.gamma: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.0.5.norm2.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.0.5.norm2.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.0.5.mlp.fc1.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 1.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.0.5.mlp.fc1.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.0.5.mlp.fc2.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 1.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.0.5.mlp.fc2.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.0.5.ls2.gamma: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.1.6.norm1.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.1.6.norm1.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.1.6.attn.qkv.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 1.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.1.6.attn.qkv.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.1.6.attn.proj.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 1.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.1.6.attn.proj.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.1.6.ls1.gamma: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.1.6.norm2.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.1.6.norm2.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.1.6.mlp.fc1.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 1.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.1.6.mlp.fc1.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.1.6.mlp.fc2.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 1.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.1.6.mlp.fc2.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.1.6.ls2.gamma: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.1.7.norm1.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.1.7.norm1.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.1.7.attn.qkv.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 1.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.1.7.attn.qkv.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.1.7.attn.proj.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 1.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.1.7.attn.proj.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.1.7.ls1.gamma: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.1.7.norm2.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.1.7.norm2.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.1.7.mlp.fc1.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 1.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.1.7.mlp.fc1.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.1.7.mlp.fc2.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 1.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.1.7.mlp.fc2.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.1.7.ls2.gamma: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.1.8.norm1.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.1.8.norm1.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.1.8.attn.qkv.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 1.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.1.8.attn.qkv.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.1.8.attn.proj.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 1.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.1.8.attn.proj.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.1.8.ls1.gamma: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.1.8.norm2.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.1.8.norm2.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.1.8.mlp.fc1.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 1.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.1.8.mlp.fc1.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.1.8.mlp.fc2.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 1.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.1.8.mlp.fc2.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.1.8.ls2.gamma: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.1.9.norm1.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.1.9.norm1.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.1.9.attn.qkv.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 1.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.1.9.attn.qkv.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.1.9.attn.proj.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 1.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.1.9.attn.proj.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.1.9.ls1.gamma: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.1.9.norm2.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.1.9.norm2.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.1.9.mlp.fc1.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 1.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.1.9.mlp.fc1.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.1.9.mlp.fc2.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 1.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.1.9.mlp.fc2.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.1.9.ls2.gamma: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.1.10.norm1.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.1.10.norm1.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.1.10.attn.qkv.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 1.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.1.10.attn.qkv.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.1.10.attn.proj.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 1.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.1.10.attn.proj.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.1.10.ls1.gamma: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.1.10.norm2.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.1.10.norm2.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.1.10.mlp.fc1.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 1.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.1.10.mlp.fc1.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.1.10.mlp.fc2.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 1.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.1.10.mlp.fc2.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.1.10.ls2.gamma: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.1.11.norm1.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.1.11.norm1.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.1.11.attn.qkv.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 1.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.1.11.attn.qkv.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.1.11.attn.proj.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 1.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.1.11.attn.proj.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.1.11.ls1.gamma: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.1.11.norm2.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.1.11.norm2.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.1.11.mlp.fc1.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 1.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.1.11.mlp.fc1.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.1.11.mlp.fc2.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 1.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.1.11.mlp.fc2.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.1.11.ls2.gamma: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.2.12.norm1.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.2.12.norm1.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.2.12.attn.qkv.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 1.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.2.12.attn.qkv.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.2.12.attn.proj.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 1.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.2.12.attn.proj.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.2.12.ls1.gamma: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.2.12.norm2.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.2.12.norm2.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.2.12.mlp.fc1.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 1.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.2.12.mlp.fc1.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.2.12.mlp.fc2.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 1.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.2.12.mlp.fc2.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.2.12.ls2.gamma: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.2.13.norm1.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.2.13.norm1.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.2.13.attn.qkv.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 1.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.2.13.attn.qkv.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.2.13.attn.proj.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 1.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.2.13.attn.proj.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.2.13.ls1.gamma: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.2.13.norm2.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.2.13.norm2.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.2.13.mlp.fc1.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 1.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.2.13.mlp.fc1.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.2.13.mlp.fc2.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 1.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.2.13.mlp.fc2.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.2.13.ls2.gamma: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.2.14.norm1.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.2.14.norm1.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.2.14.attn.qkv.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 1.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.2.14.attn.qkv.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.2.14.attn.proj.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 1.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.2.14.attn.proj.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.2.14.ls1.gamma: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.2.14.norm2.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.2.14.norm2.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.2.14.mlp.fc1.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 1.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.2.14.mlp.fc1.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.2.14.mlp.fc2.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 1.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.2.14.mlp.fc2.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.2.14.ls2.gamma: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.2.15.norm1.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.2.15.norm1.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.2.15.attn.qkv.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 1.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.2.15.attn.qkv.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.2.15.attn.proj.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 1.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.2.15.attn.proj.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.2.15.ls1.gamma: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.2.15.norm2.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.2.15.norm2.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.2.15.mlp.fc1.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 1.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.2.15.mlp.fc1.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.2.15.mlp.fc2.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 1.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.2.15.mlp.fc2.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.2.15.ls2.gamma: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.2.16.norm1.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.2.16.norm1.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.2.16.attn.qkv.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 1.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.2.16.attn.qkv.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.2.16.attn.proj.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 1.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.2.16.attn.proj.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.2.16.ls1.gamma: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.2.16.norm2.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.2.16.norm2.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.2.16.mlp.fc1.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 1.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.2.16.mlp.fc1.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.2.16.mlp.fc2.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 1.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.2.16.mlp.fc2.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.2.16.ls2.gamma: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.2.17.norm1.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.2.17.norm1.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.2.17.attn.qkv.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 1.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.2.17.attn.qkv.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.2.17.attn.proj.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 1.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.2.17.attn.proj.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.2.17.ls1.gamma: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.2.17.norm2.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.2.17.norm2.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.2.17.mlp.fc1.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 1.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.2.17.mlp.fc1.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.2.17.mlp.fc2.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 1.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.2.17.mlp.fc2.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.2.17.ls2.gamma: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.3.18.norm1.weight: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.3.18.norm1.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.3.18.attn.qkv.weight: lr_multiplier: 0.531441, wd_multiplier: 1.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.3.18.attn.qkv.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.3.18.attn.proj.weight: lr_multiplier: 0.531441, wd_multiplier: 1.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.3.18.attn.proj.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.3.18.ls1.gamma: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.3.18.norm2.weight: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.3.18.norm2.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.3.18.mlp.fc1.weight: lr_multiplier: 0.531441, wd_multiplier: 1.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.3.18.mlp.fc1.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.3.18.mlp.fc2.weight: lr_multiplier: 0.531441, wd_multiplier: 1.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.3.18.mlp.fc2.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.3.18.ls2.gamma: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.3.19.norm1.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.3.19.norm1.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.3.19.attn.qkv.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 1.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.3.19.attn.qkv.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.3.19.attn.proj.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 1.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.3.19.attn.proj.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.3.19.ls1.gamma: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.3.19.norm2.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.3.19.norm2.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.3.19.mlp.fc1.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 1.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.3.19.mlp.fc1.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.3.19.mlp.fc2.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 1.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.3.19.mlp.fc2.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.3.19.ls2.gamma: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.3.20.norm1.weight: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.3.20.norm1.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.3.20.attn.qkv.weight: lr_multiplier: 0.6561, wd_multiplier: 1.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.3.20.attn.qkv.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.3.20.attn.proj.weight: lr_multiplier: 0.6561, wd_multiplier: 1.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.3.20.attn.proj.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.3.20.ls1.gamma: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.3.20.norm2.weight: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.3.20.norm2.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.3.20.mlp.fc1.weight: lr_multiplier: 0.6561, wd_multiplier: 1.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.3.20.mlp.fc1.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.3.20.mlp.fc2.weight: lr_multiplier: 0.6561, wd_multiplier: 1.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.3.20.mlp.fc2.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.3.20.ls2.gamma: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.3.21.norm1.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.3.21.norm1.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.3.21.attn.qkv.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 1.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.3.21.attn.qkv.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.3.21.attn.proj.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 1.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.3.21.attn.proj.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.3.21.ls1.gamma: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.3.21.norm2.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.3.21.norm2.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.3.21.mlp.fc1.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 1.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.3.21.mlp.fc1.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.3.21.mlp.fc2.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 1.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.3.21.mlp.fc2.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.3.21.ls2.gamma: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.3.22.norm1.weight: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.3.22.norm1.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.3.22.attn.qkv.weight: lr_multiplier: 0.81, wd_multiplier: 1.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.3.22.attn.qkv.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.3.22.attn.proj.weight: lr_multiplier: 0.81, wd_multiplier: 1.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.3.22.attn.proj.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.3.22.ls1.gamma: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.3.22.norm2.weight: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.3.22.norm2.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.3.22.mlp.fc1.weight: lr_multiplier: 0.81, wd_multiplier: 1.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.3.22.mlp.fc1.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.3.22.mlp.fc2.weight: lr_multiplier: 0.81, wd_multiplier: 1.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.3.22.mlp.fc2.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.3.22.ls2.gamma: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.3.23.norm1.weight: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.3.23.norm1.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.3.23.attn.qkv.weight: lr_multiplier: 0.9, wd_multiplier: 1.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.3.23.attn.qkv.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.3.23.attn.proj.weight: lr_multiplier: 0.9, wd_multiplier: 1.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.3.23.attn.proj.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.3.23.ls1.gamma: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.3.23.norm2.weight: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.3.23.norm2.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.3.23.mlp.fc1.weight: lr_multiplier: 0.9, wd_multiplier: 1.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.3.23.mlp.fc1.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.3.23.mlp.fc2.weight: lr_multiplier: 0.9, wd_multiplier: 1.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.3.23.mlp.fc2.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] blocks.3.23.ls2.gamma: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] norm.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] norm.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 ssl_meta_arch.py:378] fusing param groups
I20240906 13:35:39 3652 dinov2 param_groups.py:64] else code branch
I20240906 13:35:39 3652 dinov2 param_groups.py:87] mlp.0.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] mlp.0.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] mlp.2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] mlp.2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] mlp.4.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] mlp.4.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] last_layer.weight_g: lr_multiplier: 1.0, wd_multiplier: 1.0
I20240906 13:35:39 3652 dinov2 param_groups.py:87] last_layer.weight_v: lr_multiplier: 1.0, wd_multiplier: 1.0
I20240906 13:35:39 3652 dinov2 ssl_meta_arch.py:378] fusing param groups
I20240906 13:35:39 3652 dinov2 train.py:98] Schedulers ready.
I20240906 13:35:39 3652 dinov2 augmentations.py:34] ###################################
I20240906 13:35:39 3652 dinov2 augmentations.py:35] Using data augmentation parameters:
I20240906 13:35:39 3652 dinov2 augmentations.py:36] global_crops_scale: [0.32, 1.0]
I20240906 13:35:39 3652 dinov2 augmentations.py:37] local_crops_scale: [0.05, 0.32]
I20240906 13:35:39 3652 dinov2 augmentations.py:38] local_crops_number: 8
I20240906 13:35:39 3652 dinov2 augmentations.py:39] global_crops_size: 224
I20240906 13:35:39 3652 dinov2 augmentations.py:40] local_crops_size: 96
I20240906 13:35:39 3652 dinov2 augmentations.py:41] ###################################
I20240906 13:35:39 3652 dinov2 loaders.py:89] using dataset: "ImageNet:split=TRAIN:root=C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\datasets\ILSVRC\Data\CLS-LOC:extra=C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\datasets\ILSVRC\Data\CLS-LOC"
I20240906 13:35:39 3652 dinov2 loaders.py:94] # of dataset samples: 1,281,167
I20240906 13:35:39 3652 dinov2 loaders.py:117] sampler: infinite
I20240906 13:35:39 3652 dinov2 loaders.py:211] using PyTorch data loader
I20240906 13:35:39 3652 dinov2 loaders.py:226] infinite data loader
I20240906 13:35:39 3652 dinov2 train.py:216] Starting training from iteration 0
W20240906 13:35:44 3652 xformers __init__.py:50] A matching Triton is not available, some optimizations will not be enabled.
Error caught was: No module named 'triton'
I20240906 13:37:07 10828 dinov2 config.py:59] git:
  sha: d43546fae6805ceb22618af8cf78469b61d913c7, status: has uncommitted changes, branch: main

I20240906 13:37:07 10828 dinov2 config.py:60] config_file: configs/train/vitl16_short.yaml
eval: 
eval_only: False
no_resume: False
opts: ['train.dataset_path=ImageNet:split=TRAIN:root=C:\\Users\\ISI_UTS\\Siladittya\\iclr2023\\inexpts\\datasets\\ILSVRC\\Data\\CLS-LOC:extra=C:\\Users\\ISI_UTS\\Siladittya\\iclr2023\\inexpts\\datasets\\ILSVRC\\Data\\CLS-LOC', 'train.output_dir=C:\\Users\\ISI_UTS\\Siladittya\\iclr2023\\inexpts\\dinov2_nosetup\\dinov2\\dinov2_nosetup\\dinov2']
output_dir: C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\dinov2_nosetup\dinov2\dinov2_nosetup\dinov2
I20240906 13:37:07 10828 dinov2 config.py:26] sqrt scaling learning rate; base: 0.004, new: 0.001
I20240906 13:37:07 10828 dinov2 config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
dino:
  loss_weight: 1.0
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
  koleo_loss_weight: 0.1
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  separate_head: false
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
train:
  batch_size_per_gpu: 64
  dataset_path: ImageNet:split=TRAIN:root=C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\datasets\ILSVRC\Data\CLS-LOC:extra=C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\datasets\ILSVRC\Data\CLS-LOC
  output_dir: C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\dinov2_nosetup\dinov2\dinov2_nosetup\dinov2
  saveckp_freq: 20
  seed: 0
  num_workers: 0
  OFFICIAL_EPOCH_LENGTH: 1250
  cache_dataset: true
  centering: centering
student:
  arch: vit_large
  patch_size: 16
  drop_path_rate: 0.3
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: mlp
  block_chunks: 4
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
  num_register_tokens: 0
  interpolate_antialias: false
  interpolate_offset: 0.1
teacher:
  momentum_teacher: 0.992
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 30
optim:
  epochs: 100
  weight_decay: 0.04
  weight_decay_end: 0.4
  base_lr: 0.004
  lr: 0.001
  warmup_epochs: 10
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 0.9
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 8
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 224
  local_crops_size: 96
evaluation:
  eval_period_iterations: 12500

I20240906 13:37:07 10828 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20240906 13:37:09 10828 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20240906 13:37:12 10828 dinov2 ssl_meta_arch.py:43] OPTIONS -- architecture : embed_dim: 1024
I20240906 13:37:12 10828 dinov2 ssl_meta_arch.py:58] OPTIONS -- DINO
I20240906 13:37:12 10828 dinov2 ssl_meta_arch.py:60] OPTIONS -- DINO -- loss_weight: 1.0
I20240906 13:37:12 10828 dinov2 ssl_meta_arch.py:61] OPTIONS -- DINO -- head_n_prototypes: 65536
I20240906 13:37:12 10828 dinov2 ssl_meta_arch.py:62] OPTIONS -- DINO -- head_bottleneck_dim: 256
I20240906 13:37:12 10828 dinov2 ssl_meta_arch.py:63] OPTIONS -- DINO -- head_hidden_dim: 2048
I20240906 13:37:12 10828 dinov2 ssl_meta_arch.py:75] OPTIONS -- DINO -- applying KOLEO regularization
I20240906 13:37:12 10828 dinov2 ssl_meta_arch.py:85] OPTIONS -- IBOT
I20240906 13:37:12 10828 dinov2 ssl_meta_arch.py:86] OPTIONS -- IBOT -- loss_weight: 1.0
I20240906 13:37:12 10828 dinov2 ssl_meta_arch.py:87] OPTIONS -- IBOT masking -- ibot_mask_ratio_tuple: [0.1, 0.5]
I20240906 13:37:12 10828 dinov2 ssl_meta_arch.py:88] OPTIONS -- IBOT masking -- ibot_mask_sample_probability: 0.5
I20240906 13:37:12 10828 dinov2 ssl_meta_arch.py:111] OPTIONS -- IBOT -- head shared with DINO
I20240906 13:37:12 10828 dinov2 ssl_meta_arch.py:121] Student and Teacher are built: they are both vit_large network.
I20240906 13:37:13 10828 dinov2 train.py:302] Model:
SSLMetaArch(
  (dino_loss): DINOLoss()
  (koleo_loss): KoLeoLoss(
    (pdist): PairwiseDistance()
  )
  (ibot_patch_loss): iBOTPatchLoss()
  (student): ModuleDict(
    (backbone): DinoVisionTransformer(
      (patch_embed): PatchEmbed(
        (proj): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))
        (norm): Identity()
      )
      (blocks): ModuleList(
        (0): BlockChunk(
          (0-5): 6 x NestedTensorBlock(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): DropPath()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): DropPath()
          )
        )
        (1): BlockChunk(
          (0-5): 6 x Identity()
          (6-11): 6 x NestedTensorBlock(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): DropPath()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): DropPath()
          )
        )
        (2): BlockChunk(
          (0-11): 12 x Identity()
          (12-17): 6 x NestedTensorBlock(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): DropPath()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): DropPath()
          )
        )
        (3): BlockChunk(
          (0-17): 18 x Identity()
          (18-23): 6 x NestedTensorBlock(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): DropPath()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): DropPath()
          )
        )
      )
      (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (head): Identity()
    )
    (dino_head): DINOHead(
      (mlp): Sequential(
        (0): Linear(in_features=1024, out_features=2048, bias=True)
        (1): GELU(approximate='none')
        (2): Linear(in_features=2048, out_features=2048, bias=True)
        (3): GELU(approximate='none')
        (4): Linear(in_features=2048, out_features=256, bias=True)
      )
      (last_layer): Linear(in_features=256, out_features=65536, bias=False)
    )
  )
  (teacher): ModuleDict(
    (backbone): DinoVisionTransformer(
      (patch_embed): PatchEmbed(
        (proj): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))
        (norm): Identity()
      )
      (blocks): ModuleList(
        (0): BlockChunk(
          (0-5): 6 x NestedTensorBlock(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): Identity()
          )
        )
        (1): BlockChunk(
          (0-5): 6 x Identity()
          (6-11): 6 x NestedTensorBlock(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): Identity()
          )
        )
        (2): BlockChunk(
          (0-11): 12 x Identity()
          (12-17): 6 x NestedTensorBlock(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): Identity()
          )
        )
        (3): BlockChunk(
          (0-17): 18 x Identity()
          (18-23): 6 x NestedTensorBlock(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): Identity()
          )
        )
      )
      (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (head): Identity()
    )
    (dino_head): DINOHead(
      (mlp): Sequential(
        (0): Linear(in_features=1024, out_features=2048, bias=True)
        (1): GELU(approximate='none')
        (2): Linear(in_features=2048, out_features=2048, bias=True)
        (3): GELU(approximate='none')
        (4): Linear(in_features=2048, out_features=256, bias=True)
      )
      (last_layer): Linear(in_features=256, out_features=65536, bias=False)
    )
  )
)
I20240906 13:37:13 10828 dinov2 param_groups.py:54] chunked fsdp
I20240906 13:37:13 10828 dinov2 param_groups.py:87] cls_token: lr_multiplier: 0.0717897987691853, wd_multiplier: 1.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] pos_embed: lr_multiplier: 0.0717897987691853, wd_multiplier: 1.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] mask_token: lr_multiplier: 0.0717897987691853, wd_multiplier: 1.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] patch_embed.proj.weight: lr_multiplier: 0.01435795975383706, wd_multiplier: 1.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] patch_embed.proj.bias: lr_multiplier: 0.01435795975383706, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.0.0.norm1.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.0.0.norm1.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.0.0.attn.qkv.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 1.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.0.0.attn.qkv.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.0.0.attn.proj.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 1.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.0.0.attn.proj.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.0.0.ls1.gamma: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.0.0.norm2.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.0.0.norm2.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.0.0.mlp.fc1.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 1.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.0.0.mlp.fc1.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.0.0.mlp.fc2.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 1.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.0.0.mlp.fc2.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.0.0.ls2.gamma: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.0.1.norm1.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.0.1.norm1.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.0.1.attn.qkv.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 1.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.0.1.attn.qkv.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.0.1.attn.proj.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 1.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.0.1.attn.proj.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.0.1.ls1.gamma: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.0.1.norm2.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.0.1.norm2.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.0.1.mlp.fc1.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 1.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.0.1.mlp.fc1.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.0.1.mlp.fc2.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 1.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.0.1.mlp.fc2.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.0.1.ls2.gamma: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.0.2.norm1.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.0.2.norm1.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.0.2.attn.qkv.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 1.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.0.2.attn.qkv.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.0.2.attn.proj.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 1.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.0.2.attn.proj.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.0.2.ls1.gamma: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.0.2.norm2.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.0.2.norm2.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.0.2.mlp.fc1.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 1.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.0.2.mlp.fc1.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.0.2.mlp.fc2.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 1.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.0.2.mlp.fc2.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.0.2.ls2.gamma: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.0.3.norm1.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.0.3.norm1.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.0.3.attn.qkv.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 1.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.0.3.attn.qkv.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.0.3.attn.proj.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 1.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.0.3.attn.proj.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.0.3.ls1.gamma: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.0.3.norm2.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.0.3.norm2.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.0.3.mlp.fc1.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 1.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.0.3.mlp.fc1.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.0.3.mlp.fc2.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 1.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.0.3.mlp.fc2.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.0.3.ls2.gamma: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.0.4.norm1.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.0.4.norm1.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.0.4.attn.qkv.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 1.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.0.4.attn.qkv.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.0.4.attn.proj.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 1.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.0.4.attn.proj.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.0.4.ls1.gamma: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.0.4.norm2.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.0.4.norm2.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.0.4.mlp.fc1.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 1.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.0.4.mlp.fc1.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.0.4.mlp.fc2.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 1.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.0.4.mlp.fc2.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.0.4.ls2.gamma: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.0.5.norm1.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.0.5.norm1.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.0.5.attn.qkv.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 1.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.0.5.attn.qkv.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.0.5.attn.proj.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 1.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.0.5.attn.proj.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.0.5.ls1.gamma: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.0.5.norm2.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.0.5.norm2.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.0.5.mlp.fc1.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 1.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.0.5.mlp.fc1.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.0.5.mlp.fc2.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 1.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.0.5.mlp.fc2.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.0.5.ls2.gamma: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.1.6.norm1.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.1.6.norm1.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.1.6.attn.qkv.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 1.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.1.6.attn.qkv.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.1.6.attn.proj.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 1.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.1.6.attn.proj.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.1.6.ls1.gamma: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.1.6.norm2.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.1.6.norm2.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.1.6.mlp.fc1.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 1.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.1.6.mlp.fc1.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.1.6.mlp.fc2.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 1.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.1.6.mlp.fc2.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.1.6.ls2.gamma: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.1.7.norm1.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.1.7.norm1.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.1.7.attn.qkv.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 1.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.1.7.attn.qkv.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.1.7.attn.proj.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 1.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.1.7.attn.proj.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.1.7.ls1.gamma: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.1.7.norm2.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.1.7.norm2.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.1.7.mlp.fc1.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 1.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.1.7.mlp.fc1.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.1.7.mlp.fc2.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 1.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.1.7.mlp.fc2.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.1.7.ls2.gamma: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.1.8.norm1.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.1.8.norm1.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.1.8.attn.qkv.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 1.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.1.8.attn.qkv.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.1.8.attn.proj.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 1.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.1.8.attn.proj.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.1.8.ls1.gamma: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.1.8.norm2.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.1.8.norm2.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.1.8.mlp.fc1.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 1.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.1.8.mlp.fc1.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.1.8.mlp.fc2.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 1.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.1.8.mlp.fc2.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.1.8.ls2.gamma: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.1.9.norm1.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.1.9.norm1.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.1.9.attn.qkv.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 1.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.1.9.attn.qkv.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.1.9.attn.proj.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 1.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.1.9.attn.proj.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.1.9.ls1.gamma: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.1.9.norm2.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.1.9.norm2.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.1.9.mlp.fc1.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 1.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.1.9.mlp.fc1.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.1.9.mlp.fc2.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 1.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.1.9.mlp.fc2.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.1.9.ls2.gamma: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.1.10.norm1.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.1.10.norm1.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.1.10.attn.qkv.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 1.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.1.10.attn.qkv.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.1.10.attn.proj.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 1.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.1.10.attn.proj.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.1.10.ls1.gamma: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.1.10.norm2.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.1.10.norm2.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.1.10.mlp.fc1.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 1.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.1.10.mlp.fc1.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.1.10.mlp.fc2.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 1.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.1.10.mlp.fc2.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.1.10.ls2.gamma: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.1.11.norm1.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.1.11.norm1.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.1.11.attn.qkv.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 1.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.1.11.attn.qkv.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.1.11.attn.proj.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 1.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.1.11.attn.proj.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.1.11.ls1.gamma: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.1.11.norm2.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.1.11.norm2.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.1.11.mlp.fc1.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 1.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.1.11.mlp.fc1.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.1.11.mlp.fc2.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 1.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.1.11.mlp.fc2.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.1.11.ls2.gamma: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.2.12.norm1.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.2.12.norm1.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.2.12.attn.qkv.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 1.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.2.12.attn.qkv.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.2.12.attn.proj.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 1.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.2.12.attn.proj.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.2.12.ls1.gamma: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.2.12.norm2.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.2.12.norm2.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.2.12.mlp.fc1.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 1.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.2.12.mlp.fc1.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.2.12.mlp.fc2.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 1.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.2.12.mlp.fc2.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.2.12.ls2.gamma: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.2.13.norm1.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.2.13.norm1.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.2.13.attn.qkv.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 1.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.2.13.attn.qkv.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.2.13.attn.proj.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 1.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.2.13.attn.proj.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.2.13.ls1.gamma: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.2.13.norm2.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.2.13.norm2.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.2.13.mlp.fc1.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 1.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.2.13.mlp.fc1.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.2.13.mlp.fc2.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 1.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.2.13.mlp.fc2.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.2.13.ls2.gamma: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.2.14.norm1.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.2.14.norm1.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.2.14.attn.qkv.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 1.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.2.14.attn.qkv.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.2.14.attn.proj.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 1.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.2.14.attn.proj.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.2.14.ls1.gamma: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.2.14.norm2.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.2.14.norm2.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.2.14.mlp.fc1.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 1.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.2.14.mlp.fc1.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.2.14.mlp.fc2.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 1.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.2.14.mlp.fc2.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.2.14.ls2.gamma: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.2.15.norm1.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.2.15.norm1.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.2.15.attn.qkv.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 1.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.2.15.attn.qkv.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.2.15.attn.proj.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 1.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.2.15.attn.proj.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.2.15.ls1.gamma: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.2.15.norm2.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.2.15.norm2.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.2.15.mlp.fc1.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 1.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.2.15.mlp.fc1.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.2.15.mlp.fc2.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 1.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.2.15.mlp.fc2.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.2.15.ls2.gamma: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.2.16.norm1.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.2.16.norm1.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.2.16.attn.qkv.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 1.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.2.16.attn.qkv.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.2.16.attn.proj.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 1.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.2.16.attn.proj.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.2.16.ls1.gamma: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.2.16.norm2.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.2.16.norm2.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.2.16.mlp.fc1.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 1.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.2.16.mlp.fc1.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.2.16.mlp.fc2.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 1.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.2.16.mlp.fc2.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.2.16.ls2.gamma: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.2.17.norm1.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.2.17.norm1.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.2.17.attn.qkv.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 1.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.2.17.attn.qkv.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.2.17.attn.proj.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 1.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.2.17.attn.proj.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.2.17.ls1.gamma: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.2.17.norm2.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.2.17.norm2.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.2.17.mlp.fc1.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 1.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.2.17.mlp.fc1.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.2.17.mlp.fc2.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 1.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.2.17.mlp.fc2.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.2.17.ls2.gamma: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.3.18.norm1.weight: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.3.18.norm1.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.3.18.attn.qkv.weight: lr_multiplier: 0.531441, wd_multiplier: 1.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.3.18.attn.qkv.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.3.18.attn.proj.weight: lr_multiplier: 0.531441, wd_multiplier: 1.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.3.18.attn.proj.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.3.18.ls1.gamma: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.3.18.norm2.weight: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.3.18.norm2.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.3.18.mlp.fc1.weight: lr_multiplier: 0.531441, wd_multiplier: 1.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.3.18.mlp.fc1.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.3.18.mlp.fc2.weight: lr_multiplier: 0.531441, wd_multiplier: 1.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.3.18.mlp.fc2.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.3.18.ls2.gamma: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.3.19.norm1.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.3.19.norm1.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.3.19.attn.qkv.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 1.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.3.19.attn.qkv.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.3.19.attn.proj.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 1.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.3.19.attn.proj.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.3.19.ls1.gamma: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.3.19.norm2.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.3.19.norm2.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.3.19.mlp.fc1.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 1.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.3.19.mlp.fc1.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.3.19.mlp.fc2.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 1.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.3.19.mlp.fc2.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.3.19.ls2.gamma: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.3.20.norm1.weight: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.3.20.norm1.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.3.20.attn.qkv.weight: lr_multiplier: 0.6561, wd_multiplier: 1.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.3.20.attn.qkv.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.3.20.attn.proj.weight: lr_multiplier: 0.6561, wd_multiplier: 1.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.3.20.attn.proj.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.3.20.ls1.gamma: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.3.20.norm2.weight: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.3.20.norm2.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.3.20.mlp.fc1.weight: lr_multiplier: 0.6561, wd_multiplier: 1.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.3.20.mlp.fc1.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.3.20.mlp.fc2.weight: lr_multiplier: 0.6561, wd_multiplier: 1.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.3.20.mlp.fc2.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.3.20.ls2.gamma: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.3.21.norm1.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.3.21.norm1.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.3.21.attn.qkv.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 1.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.3.21.attn.qkv.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.3.21.attn.proj.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 1.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.3.21.attn.proj.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.3.21.ls1.gamma: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.3.21.norm2.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.3.21.norm2.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.3.21.mlp.fc1.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 1.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.3.21.mlp.fc1.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.3.21.mlp.fc2.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 1.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.3.21.mlp.fc2.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.3.21.ls2.gamma: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.3.22.norm1.weight: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.3.22.norm1.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.3.22.attn.qkv.weight: lr_multiplier: 0.81, wd_multiplier: 1.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.3.22.attn.qkv.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.3.22.attn.proj.weight: lr_multiplier: 0.81, wd_multiplier: 1.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.3.22.attn.proj.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.3.22.ls1.gamma: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.3.22.norm2.weight: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.3.22.norm2.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.3.22.mlp.fc1.weight: lr_multiplier: 0.81, wd_multiplier: 1.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.3.22.mlp.fc1.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.3.22.mlp.fc2.weight: lr_multiplier: 0.81, wd_multiplier: 1.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.3.22.mlp.fc2.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.3.22.ls2.gamma: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.3.23.norm1.weight: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.3.23.norm1.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.3.23.attn.qkv.weight: lr_multiplier: 0.9, wd_multiplier: 1.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.3.23.attn.qkv.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.3.23.attn.proj.weight: lr_multiplier: 0.9, wd_multiplier: 1.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.3.23.attn.proj.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.3.23.ls1.gamma: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.3.23.norm2.weight: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.3.23.norm2.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.3.23.mlp.fc1.weight: lr_multiplier: 0.9, wd_multiplier: 1.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.3.23.mlp.fc1.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.3.23.mlp.fc2.weight: lr_multiplier: 0.9, wd_multiplier: 1.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.3.23.mlp.fc2.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] blocks.3.23.ls2.gamma: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] norm.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] norm.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 ssl_meta_arch.py:378] fusing param groups
I20240906 13:37:13 10828 dinov2 param_groups.py:64] else code branch
I20240906 13:37:13 10828 dinov2 param_groups.py:87] mlp.0.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] mlp.0.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] mlp.2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] mlp.2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] mlp.4.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] mlp.4.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] last_layer.weight_g: lr_multiplier: 1.0, wd_multiplier: 1.0
I20240906 13:37:13 10828 dinov2 param_groups.py:87] last_layer.weight_v: lr_multiplier: 1.0, wd_multiplier: 1.0
I20240906 13:37:13 10828 dinov2 ssl_meta_arch.py:378] fusing param groups
I20240906 13:37:13 10828 dinov2 train.py:98] Schedulers ready.
I20240906 13:37:13 10828 dinov2 augmentations.py:34] ###################################
I20240906 13:37:13 10828 dinov2 augmentations.py:35] Using data augmentation parameters:
I20240906 13:37:13 10828 dinov2 augmentations.py:36] global_crops_scale: [0.32, 1.0]
I20240906 13:37:13 10828 dinov2 augmentations.py:37] local_crops_scale: [0.05, 0.32]
I20240906 13:37:13 10828 dinov2 augmentations.py:38] local_crops_number: 8
I20240906 13:37:13 10828 dinov2 augmentations.py:39] global_crops_size: 224
I20240906 13:37:13 10828 dinov2 augmentations.py:40] local_crops_size: 96
I20240906 13:37:13 10828 dinov2 augmentations.py:41] ###################################
I20240906 13:37:13 10828 dinov2 loaders.py:89] using dataset: "ImageNet:split=TRAIN:root=C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\datasets\ILSVRC\Data\CLS-LOC:extra=C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\datasets\ILSVRC\Data\CLS-LOC"
I20240906 13:37:13 10828 dinov2 loaders.py:94] # of dataset samples: 1,281,167
I20240906 13:37:13 10828 dinov2 loaders.py:117] sampler: infinite
I20240906 13:37:13 10828 dinov2 loaders.py:211] using PyTorch data loader
I20240906 13:37:13 10828 dinov2 loaders.py:226] infinite data loader
I20240906 13:37:13 10828 dinov2 train.py:216] Starting training from iteration 0
I20240906 13:37:36 17120 dinov2 config.py:59] git:
  sha: d43546fae6805ceb22618af8cf78469b61d913c7, status: has uncommitted changes, branch: main

I20240906 13:37:36 17120 dinov2 config.py:60] config_file: configs/train/vitl16_short.yaml
eval: 
eval_only: False
no_resume: False
opts: ['train.dataset_path=ImageNet:split=TRAIN:root=C:\\Users\\ISI_UTS\\Siladittya\\iclr2023\\inexpts\\datasets\\ILSVRC\\Data\\CLS-LOC:extra=C:\\Users\\ISI_UTS\\Siladittya\\iclr2023\\inexpts\\datasets\\ILSVRC\\Data\\CLS-LOC', 'train.output_dir=C:\\Users\\ISI_UTS\\Siladittya\\iclr2023\\inexpts\\dinov2_nosetup\\dinov2\\dinov2_nosetup\\dinov2']
output_dir: C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\dinov2_nosetup\dinov2\dinov2_nosetup\dinov2
I20240906 13:37:36 17120 dinov2 config.py:26] sqrt scaling learning rate; base: 0.004, new: 0.001
I20240906 13:37:36 17120 dinov2 config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
dino:
  loss_weight: 1.0
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
  koleo_loss_weight: 0.1
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  separate_head: false
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
train:
  batch_size_per_gpu: 64
  dataset_path: ImageNet:split=TRAIN:root=C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\datasets\ILSVRC\Data\CLS-LOC:extra=C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\datasets\ILSVRC\Data\CLS-LOC
  output_dir: C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\dinov2_nosetup\dinov2\dinov2_nosetup\dinov2
  saveckp_freq: 20
  seed: 0
  num_workers: 0
  OFFICIAL_EPOCH_LENGTH: 1250
  cache_dataset: true
  centering: centering
student:
  arch: vit_large
  patch_size: 16
  drop_path_rate: 0.3
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: mlp
  block_chunks: 4
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
  num_register_tokens: 0
  interpolate_antialias: false
  interpolate_offset: 0.1
teacher:
  momentum_teacher: 0.992
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 30
optim:
  epochs: 100
  weight_decay: 0.04
  weight_decay_end: 0.4
  base_lr: 0.004
  lr: 0.001
  warmup_epochs: 10
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 0.9
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 8
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 224
  local_crops_size: 96
evaluation:
  eval_period_iterations: 12500

I20240906 13:37:36 17120 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20240906 13:37:38 17120 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20240906 13:37:41 17120 dinov2 ssl_meta_arch.py:43] OPTIONS -- architecture : embed_dim: 1024
I20240906 13:37:41 17120 dinov2 ssl_meta_arch.py:58] OPTIONS -- DINO
I20240906 13:37:41 17120 dinov2 ssl_meta_arch.py:60] OPTIONS -- DINO -- loss_weight: 1.0
I20240906 13:37:41 17120 dinov2 ssl_meta_arch.py:61] OPTIONS -- DINO -- head_n_prototypes: 65536
I20240906 13:37:41 17120 dinov2 ssl_meta_arch.py:62] OPTIONS -- DINO -- head_bottleneck_dim: 256
I20240906 13:37:41 17120 dinov2 ssl_meta_arch.py:63] OPTIONS -- DINO -- head_hidden_dim: 2048
I20240906 13:37:41 17120 dinov2 ssl_meta_arch.py:75] OPTIONS -- DINO -- applying KOLEO regularization
I20240906 13:37:41 17120 dinov2 ssl_meta_arch.py:85] OPTIONS -- IBOT
I20240906 13:37:41 17120 dinov2 ssl_meta_arch.py:86] OPTIONS -- IBOT -- loss_weight: 1.0
I20240906 13:37:41 17120 dinov2 ssl_meta_arch.py:87] OPTIONS -- IBOT masking -- ibot_mask_ratio_tuple: [0.1, 0.5]
I20240906 13:37:41 17120 dinov2 ssl_meta_arch.py:88] OPTIONS -- IBOT masking -- ibot_mask_sample_probability: 0.5
I20240906 13:37:41 17120 dinov2 ssl_meta_arch.py:111] OPTIONS -- IBOT -- head shared with DINO
I20240906 13:37:41 17120 dinov2 ssl_meta_arch.py:121] Student and Teacher are built: they are both vit_large network.
I20240906 13:37:42 17120 dinov2 train.py:302] Model:
SSLMetaArch(
  (dino_loss): DINOLoss()
  (koleo_loss): KoLeoLoss(
    (pdist): PairwiseDistance()
  )
  (ibot_patch_loss): iBOTPatchLoss()
  (student): ModuleDict(
    (backbone): DinoVisionTransformer(
      (patch_embed): PatchEmbed(
        (proj): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))
        (norm): Identity()
      )
      (blocks): ModuleList(
        (0): BlockChunk(
          (0-5): 6 x NestedTensorBlock(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): DropPath()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): DropPath()
          )
        )
        (1): BlockChunk(
          (0-5): 6 x Identity()
          (6-11): 6 x NestedTensorBlock(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): DropPath()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): DropPath()
          )
        )
        (2): BlockChunk(
          (0-11): 12 x Identity()
          (12-17): 6 x NestedTensorBlock(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): DropPath()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): DropPath()
          )
        )
        (3): BlockChunk(
          (0-17): 18 x Identity()
          (18-23): 6 x NestedTensorBlock(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): DropPath()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): DropPath()
          )
        )
      )
      (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (head): Identity()
    )
    (dino_head): DINOHead(
      (mlp): Sequential(
        (0): Linear(in_features=1024, out_features=2048, bias=True)
        (1): GELU(approximate='none')
        (2): Linear(in_features=2048, out_features=2048, bias=True)
        (3): GELU(approximate='none')
        (4): Linear(in_features=2048, out_features=256, bias=True)
      )
      (last_layer): Linear(in_features=256, out_features=65536, bias=False)
    )
  )
  (teacher): ModuleDict(
    (backbone): DinoVisionTransformer(
      (patch_embed): PatchEmbed(
        (proj): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))
        (norm): Identity()
      )
      (blocks): ModuleList(
        (0): BlockChunk(
          (0-5): 6 x NestedTensorBlock(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): Identity()
          )
        )
        (1): BlockChunk(
          (0-5): 6 x Identity()
          (6-11): 6 x NestedTensorBlock(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): Identity()
          )
        )
        (2): BlockChunk(
          (0-11): 12 x Identity()
          (12-17): 6 x NestedTensorBlock(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): Identity()
          )
        )
        (3): BlockChunk(
          (0-17): 18 x Identity()
          (18-23): 6 x NestedTensorBlock(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): Identity()
          )
        )
      )
      (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (head): Identity()
    )
    (dino_head): DINOHead(
      (mlp): Sequential(
        (0): Linear(in_features=1024, out_features=2048, bias=True)
        (1): GELU(approximate='none')
        (2): Linear(in_features=2048, out_features=2048, bias=True)
        (3): GELU(approximate='none')
        (4): Linear(in_features=2048, out_features=256, bias=True)
      )
      (last_layer): Linear(in_features=256, out_features=65536, bias=False)
    )
  )
)
I20240906 13:37:42 17120 dinov2 param_groups.py:54] chunked fsdp
I20240906 13:37:42 17120 dinov2 param_groups.py:87] cls_token: lr_multiplier: 0.0717897987691853, wd_multiplier: 1.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] pos_embed: lr_multiplier: 0.0717897987691853, wd_multiplier: 1.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] mask_token: lr_multiplier: 0.0717897987691853, wd_multiplier: 1.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] patch_embed.proj.weight: lr_multiplier: 0.01435795975383706, wd_multiplier: 1.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] patch_embed.proj.bias: lr_multiplier: 0.01435795975383706, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.0.0.norm1.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.0.0.norm1.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.0.0.attn.qkv.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 1.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.0.0.attn.qkv.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.0.0.attn.proj.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 1.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.0.0.attn.proj.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.0.0.ls1.gamma: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.0.0.norm2.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.0.0.norm2.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.0.0.mlp.fc1.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 1.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.0.0.mlp.fc1.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.0.0.mlp.fc2.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 1.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.0.0.mlp.fc2.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.0.0.ls2.gamma: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.0.1.norm1.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.0.1.norm1.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.0.1.attn.qkv.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 1.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.0.1.attn.qkv.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.0.1.attn.proj.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 1.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.0.1.attn.proj.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.0.1.ls1.gamma: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.0.1.norm2.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.0.1.norm2.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.0.1.mlp.fc1.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 1.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.0.1.mlp.fc1.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.0.1.mlp.fc2.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 1.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.0.1.mlp.fc2.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.0.1.ls2.gamma: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.0.2.norm1.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.0.2.norm1.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.0.2.attn.qkv.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 1.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.0.2.attn.qkv.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.0.2.attn.proj.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 1.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.0.2.attn.proj.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.0.2.ls1.gamma: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.0.2.norm2.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.0.2.norm2.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.0.2.mlp.fc1.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 1.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.0.2.mlp.fc1.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.0.2.mlp.fc2.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 1.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.0.2.mlp.fc2.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.0.2.ls2.gamma: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.0.3.norm1.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.0.3.norm1.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.0.3.attn.qkv.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 1.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.0.3.attn.qkv.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.0.3.attn.proj.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 1.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.0.3.attn.proj.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.0.3.ls1.gamma: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.0.3.norm2.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.0.3.norm2.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.0.3.mlp.fc1.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 1.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.0.3.mlp.fc1.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.0.3.mlp.fc2.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 1.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.0.3.mlp.fc2.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.0.3.ls2.gamma: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.0.4.norm1.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.0.4.norm1.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.0.4.attn.qkv.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 1.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.0.4.attn.qkv.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.0.4.attn.proj.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 1.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.0.4.attn.proj.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.0.4.ls1.gamma: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.0.4.norm2.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.0.4.norm2.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.0.4.mlp.fc1.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 1.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.0.4.mlp.fc1.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.0.4.mlp.fc2.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 1.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.0.4.mlp.fc2.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.0.4.ls2.gamma: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.0.5.norm1.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.0.5.norm1.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.0.5.attn.qkv.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 1.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.0.5.attn.qkv.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.0.5.attn.proj.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 1.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.0.5.attn.proj.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.0.5.ls1.gamma: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.0.5.norm2.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.0.5.norm2.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.0.5.mlp.fc1.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 1.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.0.5.mlp.fc1.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.0.5.mlp.fc2.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 1.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.0.5.mlp.fc2.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.0.5.ls2.gamma: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.1.6.norm1.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.1.6.norm1.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.1.6.attn.qkv.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 1.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.1.6.attn.qkv.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.1.6.attn.proj.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 1.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.1.6.attn.proj.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.1.6.ls1.gamma: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.1.6.norm2.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.1.6.norm2.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.1.6.mlp.fc1.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 1.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.1.6.mlp.fc1.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.1.6.mlp.fc2.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 1.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.1.6.mlp.fc2.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.1.6.ls2.gamma: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.1.7.norm1.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.1.7.norm1.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.1.7.attn.qkv.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 1.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.1.7.attn.qkv.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.1.7.attn.proj.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 1.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.1.7.attn.proj.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.1.7.ls1.gamma: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.1.7.norm2.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.1.7.norm2.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.1.7.mlp.fc1.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 1.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.1.7.mlp.fc1.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.1.7.mlp.fc2.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 1.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.1.7.mlp.fc2.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.1.7.ls2.gamma: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.1.8.norm1.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.1.8.norm1.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.1.8.attn.qkv.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 1.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.1.8.attn.qkv.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.1.8.attn.proj.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 1.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.1.8.attn.proj.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.1.8.ls1.gamma: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.1.8.norm2.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.1.8.norm2.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.1.8.mlp.fc1.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 1.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.1.8.mlp.fc1.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.1.8.mlp.fc2.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 1.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.1.8.mlp.fc2.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.1.8.ls2.gamma: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.1.9.norm1.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.1.9.norm1.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.1.9.attn.qkv.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 1.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.1.9.attn.qkv.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.1.9.attn.proj.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 1.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.1.9.attn.proj.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.1.9.ls1.gamma: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.1.9.norm2.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.1.9.norm2.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.1.9.mlp.fc1.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 1.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.1.9.mlp.fc1.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.1.9.mlp.fc2.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 1.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.1.9.mlp.fc2.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.1.9.ls2.gamma: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.1.10.norm1.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.1.10.norm1.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.1.10.attn.qkv.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 1.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.1.10.attn.qkv.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.1.10.attn.proj.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 1.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.1.10.attn.proj.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.1.10.ls1.gamma: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.1.10.norm2.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.1.10.norm2.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.1.10.mlp.fc1.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 1.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.1.10.mlp.fc1.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.1.10.mlp.fc2.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 1.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.1.10.mlp.fc2.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.1.10.ls2.gamma: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.1.11.norm1.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.1.11.norm1.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.1.11.attn.qkv.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 1.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.1.11.attn.qkv.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.1.11.attn.proj.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 1.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.1.11.attn.proj.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.1.11.ls1.gamma: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.1.11.norm2.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.1.11.norm2.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.1.11.mlp.fc1.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 1.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.1.11.mlp.fc1.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.1.11.mlp.fc2.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 1.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.1.11.mlp.fc2.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.1.11.ls2.gamma: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.2.12.norm1.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.2.12.norm1.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.2.12.attn.qkv.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 1.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.2.12.attn.qkv.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.2.12.attn.proj.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 1.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.2.12.attn.proj.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.2.12.ls1.gamma: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.2.12.norm2.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.2.12.norm2.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.2.12.mlp.fc1.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 1.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.2.12.mlp.fc1.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.2.12.mlp.fc2.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 1.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.2.12.mlp.fc2.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.2.12.ls2.gamma: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.2.13.norm1.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.2.13.norm1.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.2.13.attn.qkv.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 1.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.2.13.attn.qkv.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.2.13.attn.proj.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 1.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.2.13.attn.proj.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.2.13.ls1.gamma: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.2.13.norm2.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.2.13.norm2.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.2.13.mlp.fc1.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 1.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.2.13.mlp.fc1.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.2.13.mlp.fc2.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 1.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.2.13.mlp.fc2.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.2.13.ls2.gamma: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.2.14.norm1.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.2.14.norm1.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.2.14.attn.qkv.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 1.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.2.14.attn.qkv.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.2.14.attn.proj.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 1.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.2.14.attn.proj.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.2.14.ls1.gamma: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.2.14.norm2.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.2.14.norm2.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.2.14.mlp.fc1.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 1.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.2.14.mlp.fc1.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.2.14.mlp.fc2.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 1.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.2.14.mlp.fc2.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.2.14.ls2.gamma: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.2.15.norm1.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.2.15.norm1.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.2.15.attn.qkv.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 1.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.2.15.attn.qkv.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.2.15.attn.proj.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 1.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.2.15.attn.proj.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.2.15.ls1.gamma: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.2.15.norm2.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.2.15.norm2.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.2.15.mlp.fc1.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 1.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.2.15.mlp.fc1.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.2.15.mlp.fc2.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 1.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.2.15.mlp.fc2.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.2.15.ls2.gamma: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.2.16.norm1.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.2.16.norm1.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.2.16.attn.qkv.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 1.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.2.16.attn.qkv.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.2.16.attn.proj.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 1.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.2.16.attn.proj.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.2.16.ls1.gamma: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.2.16.norm2.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.2.16.norm2.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.2.16.mlp.fc1.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 1.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.2.16.mlp.fc1.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.2.16.mlp.fc2.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 1.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.2.16.mlp.fc2.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.2.16.ls2.gamma: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.2.17.norm1.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.2.17.norm1.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.2.17.attn.qkv.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 1.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.2.17.attn.qkv.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.2.17.attn.proj.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 1.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.2.17.attn.proj.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.2.17.ls1.gamma: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.2.17.norm2.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.2.17.norm2.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.2.17.mlp.fc1.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 1.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.2.17.mlp.fc1.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.2.17.mlp.fc2.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 1.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.2.17.mlp.fc2.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.2.17.ls2.gamma: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.3.18.norm1.weight: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.3.18.norm1.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.3.18.attn.qkv.weight: lr_multiplier: 0.531441, wd_multiplier: 1.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.3.18.attn.qkv.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.3.18.attn.proj.weight: lr_multiplier: 0.531441, wd_multiplier: 1.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.3.18.attn.proj.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.3.18.ls1.gamma: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.3.18.norm2.weight: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.3.18.norm2.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.3.18.mlp.fc1.weight: lr_multiplier: 0.531441, wd_multiplier: 1.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.3.18.mlp.fc1.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.3.18.mlp.fc2.weight: lr_multiplier: 0.531441, wd_multiplier: 1.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.3.18.mlp.fc2.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.3.18.ls2.gamma: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.3.19.norm1.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.3.19.norm1.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.3.19.attn.qkv.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 1.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.3.19.attn.qkv.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.3.19.attn.proj.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 1.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.3.19.attn.proj.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.3.19.ls1.gamma: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.3.19.norm2.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.3.19.norm2.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.3.19.mlp.fc1.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 1.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.3.19.mlp.fc1.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.3.19.mlp.fc2.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 1.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.3.19.mlp.fc2.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.3.19.ls2.gamma: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.3.20.norm1.weight: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.3.20.norm1.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.3.20.attn.qkv.weight: lr_multiplier: 0.6561, wd_multiplier: 1.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.3.20.attn.qkv.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.3.20.attn.proj.weight: lr_multiplier: 0.6561, wd_multiplier: 1.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.3.20.attn.proj.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.3.20.ls1.gamma: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.3.20.norm2.weight: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.3.20.norm2.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.3.20.mlp.fc1.weight: lr_multiplier: 0.6561, wd_multiplier: 1.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.3.20.mlp.fc1.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.3.20.mlp.fc2.weight: lr_multiplier: 0.6561, wd_multiplier: 1.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.3.20.mlp.fc2.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.3.20.ls2.gamma: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.3.21.norm1.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.3.21.norm1.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.3.21.attn.qkv.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 1.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.3.21.attn.qkv.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.3.21.attn.proj.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 1.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.3.21.attn.proj.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.3.21.ls1.gamma: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.3.21.norm2.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.3.21.norm2.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.3.21.mlp.fc1.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 1.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.3.21.mlp.fc1.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.3.21.mlp.fc2.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 1.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.3.21.mlp.fc2.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.3.21.ls2.gamma: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.3.22.norm1.weight: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.3.22.norm1.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.3.22.attn.qkv.weight: lr_multiplier: 0.81, wd_multiplier: 1.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.3.22.attn.qkv.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.3.22.attn.proj.weight: lr_multiplier: 0.81, wd_multiplier: 1.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.3.22.attn.proj.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.3.22.ls1.gamma: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.3.22.norm2.weight: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.3.22.norm2.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.3.22.mlp.fc1.weight: lr_multiplier: 0.81, wd_multiplier: 1.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.3.22.mlp.fc1.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.3.22.mlp.fc2.weight: lr_multiplier: 0.81, wd_multiplier: 1.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.3.22.mlp.fc2.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.3.22.ls2.gamma: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.3.23.norm1.weight: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.3.23.norm1.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.3.23.attn.qkv.weight: lr_multiplier: 0.9, wd_multiplier: 1.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.3.23.attn.qkv.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.3.23.attn.proj.weight: lr_multiplier: 0.9, wd_multiplier: 1.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.3.23.attn.proj.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.3.23.ls1.gamma: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.3.23.norm2.weight: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.3.23.norm2.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.3.23.mlp.fc1.weight: lr_multiplier: 0.9, wd_multiplier: 1.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.3.23.mlp.fc1.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.3.23.mlp.fc2.weight: lr_multiplier: 0.9, wd_multiplier: 1.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.3.23.mlp.fc2.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] blocks.3.23.ls2.gamma: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] norm.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] norm.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 ssl_meta_arch.py:378] fusing param groups
I20240906 13:37:42 17120 dinov2 param_groups.py:64] else code branch
I20240906 13:37:42 17120 dinov2 param_groups.py:87] mlp.0.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] mlp.0.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] mlp.2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] mlp.2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] mlp.4.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] mlp.4.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] last_layer.weight_g: lr_multiplier: 1.0, wd_multiplier: 1.0
I20240906 13:37:42 17120 dinov2 param_groups.py:87] last_layer.weight_v: lr_multiplier: 1.0, wd_multiplier: 1.0
I20240906 13:37:42 17120 dinov2 ssl_meta_arch.py:378] fusing param groups
I20240906 13:37:42 17120 dinov2 train.py:98] Schedulers ready.
I20240906 13:37:42 17120 dinov2 augmentations.py:34] ###################################
I20240906 13:37:42 17120 dinov2 augmentations.py:35] Using data augmentation parameters:
I20240906 13:37:42 17120 dinov2 augmentations.py:36] global_crops_scale: [0.32, 1.0]
I20240906 13:37:42 17120 dinov2 augmentations.py:37] local_crops_scale: [0.05, 0.32]
I20240906 13:37:42 17120 dinov2 augmentations.py:38] local_crops_number: 8
I20240906 13:37:42 17120 dinov2 augmentations.py:39] global_crops_size: 224
I20240906 13:37:42 17120 dinov2 augmentations.py:40] local_crops_size: 96
I20240906 13:37:42 17120 dinov2 augmentations.py:41] ###################################
I20240906 13:37:42 17120 dinov2 loaders.py:89] using dataset: "ImageNet:split=TRAIN:root=C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\datasets\ILSVRC\Data\CLS-LOC:extra=C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\datasets\ILSVRC\Data\CLS-LOC"
I20240906 13:37:42 17120 dinov2 loaders.py:94] # of dataset samples: 1,281,167
I20240906 13:37:42 17120 dinov2 loaders.py:117] sampler: infinite
I20240906 13:37:42 17120 dinov2 loaders.py:211] using PyTorch data loader
I20240906 13:37:42 17120 dinov2 loaders.py:226] infinite data loader
I20240906 13:37:42 17120 dinov2 train.py:216] Starting training from iteration 0
W20240906 13:37:46 17120 xformers __init__.py:50] A matching Triton is not available, some optimizations will not be enabled.
Error caught was: No module named 'triton'
I20240906 13:38:55 15556 dinov2 config.py:59] git:
  sha: d43546fae6805ceb22618af8cf78469b61d913c7, status: has uncommitted changes, branch: main

I20240906 13:38:55 15556 dinov2 config.py:60] config_file: configs/train/vitl16_short.yaml
eval: 
eval_only: False
no_resume: False
opts: ['train.dataset_path=ImageNet:split=TRAIN:root=C:\\Users\\ISI_UTS\\Siladittya\\iclr2023\\inexpts\\datasets\\ILSVRC\\Data\\CLS-LOC:extra=C:\\Users\\ISI_UTS\\Siladittya\\iclr2023\\inexpts\\datasets\\ILSVRC\\Data\\CLS-LOC', 'train.output_dir=C:\\Users\\ISI_UTS\\Siladittya\\iclr2023\\inexpts\\dinov2_nosetup\\dinov2\\dinov2_nosetup\\dinov2']
output_dir: C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\dinov2_nosetup\dinov2\dinov2_nosetup\dinov2
I20240906 13:38:55 15556 dinov2 config.py:26] sqrt scaling learning rate; base: 0.004, new: 0.001
I20240906 13:38:55 15556 dinov2 config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
dino:
  loss_weight: 1.0
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
  koleo_loss_weight: 0.1
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  separate_head: false
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
train:
  batch_size_per_gpu: 64
  dataset_path: ImageNet:split=TRAIN:root=C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\datasets\ILSVRC\Data\CLS-LOC:extra=C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\datasets\ILSVRC\Data\CLS-LOC
  output_dir: C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\dinov2_nosetup\dinov2\dinov2_nosetup\dinov2
  saveckp_freq: 20
  seed: 0
  num_workers: 0
  OFFICIAL_EPOCH_LENGTH: 1250
  cache_dataset: true
  centering: centering
student:
  arch: vit_large
  patch_size: 16
  drop_path_rate: 0.3
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: mlp
  block_chunks: 4
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
  num_register_tokens: 0
  interpolate_antialias: false
  interpolate_offset: 0.1
teacher:
  momentum_teacher: 0.992
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 30
optim:
  epochs: 100
  weight_decay: 0.04
  weight_decay_end: 0.4
  base_lr: 0.004
  lr: 0.001
  warmup_epochs: 10
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 0.9
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 8
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 224
  local_crops_size: 96
evaluation:
  eval_period_iterations: 12500

I20240906 13:38:55 15556 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20240906 13:38:57 15556 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20240906 13:39:00 15556 dinov2 ssl_meta_arch.py:43] OPTIONS -- architecture : embed_dim: 1024
I20240906 13:39:00 15556 dinov2 ssl_meta_arch.py:58] OPTIONS -- DINO
I20240906 13:39:00 15556 dinov2 ssl_meta_arch.py:60] OPTIONS -- DINO -- loss_weight: 1.0
I20240906 13:39:00 15556 dinov2 ssl_meta_arch.py:61] OPTIONS -- DINO -- head_n_prototypes: 65536
I20240906 13:39:00 15556 dinov2 ssl_meta_arch.py:62] OPTIONS -- DINO -- head_bottleneck_dim: 256
I20240906 13:39:00 15556 dinov2 ssl_meta_arch.py:63] OPTIONS -- DINO -- head_hidden_dim: 2048
I20240906 13:39:00 15556 dinov2 ssl_meta_arch.py:75] OPTIONS -- DINO -- applying KOLEO regularization
I20240906 13:39:00 15556 dinov2 ssl_meta_arch.py:85] OPTIONS -- IBOT
I20240906 13:39:00 15556 dinov2 ssl_meta_arch.py:86] OPTIONS -- IBOT -- loss_weight: 1.0
I20240906 13:39:00 15556 dinov2 ssl_meta_arch.py:87] OPTIONS -- IBOT masking -- ibot_mask_ratio_tuple: [0.1, 0.5]
I20240906 13:39:00 15556 dinov2 ssl_meta_arch.py:88] OPTIONS -- IBOT masking -- ibot_mask_sample_probability: 0.5
I20240906 13:39:00 15556 dinov2 ssl_meta_arch.py:111] OPTIONS -- IBOT -- head shared with DINO
I20240906 13:39:00 15556 dinov2 ssl_meta_arch.py:121] Student and Teacher are built: they are both vit_large network.
I20240906 13:39:01 15556 dinov2 train.py:302] Model:
SSLMetaArch(
  (dino_loss): DINOLoss()
  (koleo_loss): KoLeoLoss(
    (pdist): PairwiseDistance()
  )
  (ibot_patch_loss): iBOTPatchLoss()
  (student): ModuleDict(
    (backbone): DinoVisionTransformer(
      (patch_embed): PatchEmbed(
        (proj): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))
        (norm): Identity()
      )
      (blocks): ModuleList(
        (0): BlockChunk(
          (0-5): 6 x NestedTensorBlock(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): DropPath()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): DropPath()
          )
        )
        (1): BlockChunk(
          (0-5): 6 x Identity()
          (6-11): 6 x NestedTensorBlock(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): DropPath()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): DropPath()
          )
        )
        (2): BlockChunk(
          (0-11): 12 x Identity()
          (12-17): 6 x NestedTensorBlock(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): DropPath()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): DropPath()
          )
        )
        (3): BlockChunk(
          (0-17): 18 x Identity()
          (18-23): 6 x NestedTensorBlock(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): DropPath()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): DropPath()
          )
        )
      )
      (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (head): Identity()
    )
    (dino_head): DINOHead(
      (mlp): Sequential(
        (0): Linear(in_features=1024, out_features=2048, bias=True)
        (1): GELU(approximate='none')
        (2): Linear(in_features=2048, out_features=2048, bias=True)
        (3): GELU(approximate='none')
        (4): Linear(in_features=2048, out_features=256, bias=True)
      )
      (last_layer): Linear(in_features=256, out_features=65536, bias=False)
    )
  )
  (teacher): ModuleDict(
    (backbone): DinoVisionTransformer(
      (patch_embed): PatchEmbed(
        (proj): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))
        (norm): Identity()
      )
      (blocks): ModuleList(
        (0): BlockChunk(
          (0-5): 6 x NestedTensorBlock(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): Identity()
          )
        )
        (1): BlockChunk(
          (0-5): 6 x Identity()
          (6-11): 6 x NestedTensorBlock(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): Identity()
          )
        )
        (2): BlockChunk(
          (0-11): 12 x Identity()
          (12-17): 6 x NestedTensorBlock(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): Identity()
          )
        )
        (3): BlockChunk(
          (0-17): 18 x Identity()
          (18-23): 6 x NestedTensorBlock(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): Identity()
          )
        )
      )
      (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (head): Identity()
    )
    (dino_head): DINOHead(
      (mlp): Sequential(
        (0): Linear(in_features=1024, out_features=2048, bias=True)
        (1): GELU(approximate='none')
        (2): Linear(in_features=2048, out_features=2048, bias=True)
        (3): GELU(approximate='none')
        (4): Linear(in_features=2048, out_features=256, bias=True)
      )
      (last_layer): Linear(in_features=256, out_features=65536, bias=False)
    )
  )
)
I20240906 13:39:01 15556 dinov2 param_groups.py:54] chunked fsdp
I20240906 13:39:01 15556 dinov2 param_groups.py:87] cls_token: lr_multiplier: 0.0717897987691853, wd_multiplier: 1.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] pos_embed: lr_multiplier: 0.0717897987691853, wd_multiplier: 1.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] mask_token: lr_multiplier: 0.0717897987691853, wd_multiplier: 1.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] patch_embed.proj.weight: lr_multiplier: 0.01435795975383706, wd_multiplier: 1.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] patch_embed.proj.bias: lr_multiplier: 0.01435795975383706, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.0.0.norm1.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.0.0.norm1.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.0.0.attn.qkv.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 1.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.0.0.attn.qkv.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.0.0.attn.proj.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 1.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.0.0.attn.proj.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.0.0.ls1.gamma: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.0.0.norm2.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.0.0.norm2.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.0.0.mlp.fc1.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 1.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.0.0.mlp.fc1.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.0.0.mlp.fc2.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 1.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.0.0.mlp.fc2.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.0.0.ls2.gamma: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.0.1.norm1.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.0.1.norm1.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.0.1.attn.qkv.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 1.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.0.1.attn.qkv.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.0.1.attn.proj.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 1.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.0.1.attn.proj.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.0.1.ls1.gamma: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.0.1.norm2.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.0.1.norm2.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.0.1.mlp.fc1.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 1.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.0.1.mlp.fc1.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.0.1.mlp.fc2.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 1.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.0.1.mlp.fc2.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.0.1.ls2.gamma: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.0.2.norm1.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.0.2.norm1.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.0.2.attn.qkv.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 1.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.0.2.attn.qkv.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.0.2.attn.proj.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 1.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.0.2.attn.proj.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.0.2.ls1.gamma: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.0.2.norm2.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.0.2.norm2.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.0.2.mlp.fc1.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 1.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.0.2.mlp.fc1.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.0.2.mlp.fc2.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 1.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.0.2.mlp.fc2.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.0.2.ls2.gamma: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.0.3.norm1.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.0.3.norm1.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.0.3.attn.qkv.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 1.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.0.3.attn.qkv.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.0.3.attn.proj.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 1.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.0.3.attn.proj.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.0.3.ls1.gamma: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.0.3.norm2.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.0.3.norm2.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.0.3.mlp.fc1.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 1.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.0.3.mlp.fc1.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.0.3.mlp.fc2.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 1.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.0.3.mlp.fc2.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.0.3.ls2.gamma: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.0.4.norm1.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.0.4.norm1.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.0.4.attn.qkv.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 1.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.0.4.attn.qkv.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.0.4.attn.proj.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 1.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.0.4.attn.proj.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.0.4.ls1.gamma: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.0.4.norm2.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.0.4.norm2.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.0.4.mlp.fc1.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 1.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.0.4.mlp.fc1.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.0.4.mlp.fc2.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 1.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.0.4.mlp.fc2.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.0.4.ls2.gamma: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.0.5.norm1.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.0.5.norm1.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.0.5.attn.qkv.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 1.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.0.5.attn.qkv.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.0.5.attn.proj.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 1.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.0.5.attn.proj.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.0.5.ls1.gamma: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.0.5.norm2.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.0.5.norm2.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.0.5.mlp.fc1.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 1.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.0.5.mlp.fc1.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.0.5.mlp.fc2.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 1.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.0.5.mlp.fc2.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.0.5.ls2.gamma: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.1.6.norm1.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.1.6.norm1.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.1.6.attn.qkv.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 1.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.1.6.attn.qkv.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.1.6.attn.proj.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 1.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.1.6.attn.proj.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.1.6.ls1.gamma: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.1.6.norm2.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.1.6.norm2.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.1.6.mlp.fc1.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 1.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.1.6.mlp.fc1.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.1.6.mlp.fc2.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 1.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.1.6.mlp.fc2.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.1.6.ls2.gamma: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.1.7.norm1.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.1.7.norm1.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.1.7.attn.qkv.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 1.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.1.7.attn.qkv.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.1.7.attn.proj.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 1.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.1.7.attn.proj.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.1.7.ls1.gamma: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.1.7.norm2.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.1.7.norm2.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.1.7.mlp.fc1.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 1.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.1.7.mlp.fc1.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.1.7.mlp.fc2.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 1.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.1.7.mlp.fc2.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.1.7.ls2.gamma: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.1.8.norm1.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.1.8.norm1.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.1.8.attn.qkv.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 1.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.1.8.attn.qkv.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.1.8.attn.proj.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 1.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.1.8.attn.proj.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.1.8.ls1.gamma: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.1.8.norm2.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.1.8.norm2.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.1.8.mlp.fc1.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 1.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.1.8.mlp.fc1.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.1.8.mlp.fc2.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 1.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.1.8.mlp.fc2.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.1.8.ls2.gamma: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.1.9.norm1.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.1.9.norm1.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.1.9.attn.qkv.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 1.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.1.9.attn.qkv.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.1.9.attn.proj.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 1.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.1.9.attn.proj.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.1.9.ls1.gamma: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.1.9.norm2.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.1.9.norm2.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.1.9.mlp.fc1.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 1.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.1.9.mlp.fc1.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.1.9.mlp.fc2.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 1.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.1.9.mlp.fc2.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.1.9.ls2.gamma: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.1.10.norm1.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.1.10.norm1.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.1.10.attn.qkv.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 1.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.1.10.attn.qkv.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.1.10.attn.proj.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 1.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.1.10.attn.proj.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.1.10.ls1.gamma: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.1.10.norm2.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.1.10.norm2.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.1.10.mlp.fc1.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 1.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.1.10.mlp.fc1.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.1.10.mlp.fc2.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 1.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.1.10.mlp.fc2.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.1.10.ls2.gamma: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.1.11.norm1.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.1.11.norm1.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.1.11.attn.qkv.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 1.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.1.11.attn.qkv.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.1.11.attn.proj.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 1.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.1.11.attn.proj.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.1.11.ls1.gamma: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.1.11.norm2.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.1.11.norm2.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.1.11.mlp.fc1.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 1.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.1.11.mlp.fc1.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.1.11.mlp.fc2.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 1.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.1.11.mlp.fc2.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.1.11.ls2.gamma: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.2.12.norm1.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.2.12.norm1.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.2.12.attn.qkv.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 1.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.2.12.attn.qkv.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.2.12.attn.proj.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 1.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.2.12.attn.proj.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.2.12.ls1.gamma: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.2.12.norm2.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.2.12.norm2.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.2.12.mlp.fc1.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 1.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.2.12.mlp.fc1.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.2.12.mlp.fc2.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 1.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.2.12.mlp.fc2.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.2.12.ls2.gamma: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.2.13.norm1.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.2.13.norm1.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.2.13.attn.qkv.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 1.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.2.13.attn.qkv.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.2.13.attn.proj.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 1.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.2.13.attn.proj.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.2.13.ls1.gamma: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.2.13.norm2.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.2.13.norm2.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.2.13.mlp.fc1.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 1.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.2.13.mlp.fc1.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.2.13.mlp.fc2.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 1.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.2.13.mlp.fc2.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.2.13.ls2.gamma: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.2.14.norm1.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.2.14.norm1.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.2.14.attn.qkv.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 1.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.2.14.attn.qkv.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.2.14.attn.proj.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 1.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.2.14.attn.proj.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.2.14.ls1.gamma: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.2.14.norm2.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.2.14.norm2.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.2.14.mlp.fc1.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 1.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.2.14.mlp.fc1.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.2.14.mlp.fc2.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 1.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.2.14.mlp.fc2.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.2.14.ls2.gamma: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.2.15.norm1.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.2.15.norm1.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.2.15.attn.qkv.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 1.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.2.15.attn.qkv.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.2.15.attn.proj.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 1.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.2.15.attn.proj.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.2.15.ls1.gamma: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.2.15.norm2.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.2.15.norm2.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.2.15.mlp.fc1.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 1.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.2.15.mlp.fc1.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.2.15.mlp.fc2.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 1.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.2.15.mlp.fc2.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.2.15.ls2.gamma: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.2.16.norm1.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.2.16.norm1.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.2.16.attn.qkv.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 1.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.2.16.attn.qkv.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.2.16.attn.proj.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 1.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.2.16.attn.proj.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.2.16.ls1.gamma: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.2.16.norm2.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.2.16.norm2.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.2.16.mlp.fc1.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 1.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.2.16.mlp.fc1.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.2.16.mlp.fc2.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 1.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.2.16.mlp.fc2.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.2.16.ls2.gamma: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.2.17.norm1.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.2.17.norm1.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.2.17.attn.qkv.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 1.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.2.17.attn.qkv.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.2.17.attn.proj.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 1.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.2.17.attn.proj.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.2.17.ls1.gamma: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.2.17.norm2.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.2.17.norm2.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.2.17.mlp.fc1.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 1.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.2.17.mlp.fc1.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.2.17.mlp.fc2.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 1.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.2.17.mlp.fc2.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.2.17.ls2.gamma: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.3.18.norm1.weight: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.3.18.norm1.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.3.18.attn.qkv.weight: lr_multiplier: 0.531441, wd_multiplier: 1.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.3.18.attn.qkv.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.3.18.attn.proj.weight: lr_multiplier: 0.531441, wd_multiplier: 1.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.3.18.attn.proj.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.3.18.ls1.gamma: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.3.18.norm2.weight: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.3.18.norm2.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.3.18.mlp.fc1.weight: lr_multiplier: 0.531441, wd_multiplier: 1.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.3.18.mlp.fc1.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.3.18.mlp.fc2.weight: lr_multiplier: 0.531441, wd_multiplier: 1.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.3.18.mlp.fc2.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.3.18.ls2.gamma: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.3.19.norm1.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.3.19.norm1.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.3.19.attn.qkv.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 1.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.3.19.attn.qkv.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.3.19.attn.proj.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 1.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.3.19.attn.proj.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.3.19.ls1.gamma: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.3.19.norm2.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.3.19.norm2.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.3.19.mlp.fc1.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 1.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.3.19.mlp.fc1.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.3.19.mlp.fc2.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 1.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.3.19.mlp.fc2.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.3.19.ls2.gamma: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.3.20.norm1.weight: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.3.20.norm1.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.3.20.attn.qkv.weight: lr_multiplier: 0.6561, wd_multiplier: 1.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.3.20.attn.qkv.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.3.20.attn.proj.weight: lr_multiplier: 0.6561, wd_multiplier: 1.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.3.20.attn.proj.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.3.20.ls1.gamma: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.3.20.norm2.weight: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.3.20.norm2.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.3.20.mlp.fc1.weight: lr_multiplier: 0.6561, wd_multiplier: 1.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.3.20.mlp.fc1.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.3.20.mlp.fc2.weight: lr_multiplier: 0.6561, wd_multiplier: 1.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.3.20.mlp.fc2.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.3.20.ls2.gamma: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.3.21.norm1.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.3.21.norm1.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.3.21.attn.qkv.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 1.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.3.21.attn.qkv.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.3.21.attn.proj.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 1.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.3.21.attn.proj.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.3.21.ls1.gamma: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.3.21.norm2.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.3.21.norm2.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.3.21.mlp.fc1.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 1.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.3.21.mlp.fc1.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.3.21.mlp.fc2.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 1.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.3.21.mlp.fc2.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.3.21.ls2.gamma: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.3.22.norm1.weight: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.3.22.norm1.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.3.22.attn.qkv.weight: lr_multiplier: 0.81, wd_multiplier: 1.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.3.22.attn.qkv.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.3.22.attn.proj.weight: lr_multiplier: 0.81, wd_multiplier: 1.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.3.22.attn.proj.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.3.22.ls1.gamma: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.3.22.norm2.weight: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.3.22.norm2.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.3.22.mlp.fc1.weight: lr_multiplier: 0.81, wd_multiplier: 1.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.3.22.mlp.fc1.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.3.22.mlp.fc2.weight: lr_multiplier: 0.81, wd_multiplier: 1.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.3.22.mlp.fc2.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.3.22.ls2.gamma: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.3.23.norm1.weight: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.3.23.norm1.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.3.23.attn.qkv.weight: lr_multiplier: 0.9, wd_multiplier: 1.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.3.23.attn.qkv.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.3.23.attn.proj.weight: lr_multiplier: 0.9, wd_multiplier: 1.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.3.23.attn.proj.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.3.23.ls1.gamma: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.3.23.norm2.weight: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.3.23.norm2.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.3.23.mlp.fc1.weight: lr_multiplier: 0.9, wd_multiplier: 1.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.3.23.mlp.fc1.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.3.23.mlp.fc2.weight: lr_multiplier: 0.9, wd_multiplier: 1.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.3.23.mlp.fc2.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] blocks.3.23.ls2.gamma: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] norm.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] norm.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 ssl_meta_arch.py:378] fusing param groups
I20240906 13:39:01 15556 dinov2 param_groups.py:64] else code branch
I20240906 13:39:01 15556 dinov2 param_groups.py:87] mlp.0.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] mlp.0.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] mlp.2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] mlp.2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] mlp.4.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] mlp.4.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] last_layer.weight_g: lr_multiplier: 1.0, wd_multiplier: 1.0
I20240906 13:39:01 15556 dinov2 param_groups.py:87] last_layer.weight_v: lr_multiplier: 1.0, wd_multiplier: 1.0
I20240906 13:39:01 15556 dinov2 ssl_meta_arch.py:378] fusing param groups
I20240906 13:39:01 15556 dinov2 train.py:98] Schedulers ready.
I20240906 13:39:01 15556 dinov2 augmentations.py:34] ###################################
I20240906 13:39:01 15556 dinov2 augmentations.py:35] Using data augmentation parameters:
I20240906 13:39:01 15556 dinov2 augmentations.py:36] global_crops_scale: [0.32, 1.0]
I20240906 13:39:01 15556 dinov2 augmentations.py:37] local_crops_scale: [0.05, 0.32]
I20240906 13:39:01 15556 dinov2 augmentations.py:38] local_crops_number: 8
I20240906 13:39:01 15556 dinov2 augmentations.py:39] global_crops_size: 224
I20240906 13:39:01 15556 dinov2 augmentations.py:40] local_crops_size: 96
I20240906 13:39:01 15556 dinov2 augmentations.py:41] ###################################
I20240906 13:39:01 15556 dinov2 loaders.py:89] using dataset: "ImageNet:split=TRAIN:root=C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\datasets\ILSVRC\Data\CLS-LOC:extra=C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\datasets\ILSVRC\Data\CLS-LOC"
I20240906 13:39:01 15556 dinov2 loaders.py:94] # of dataset samples: 1,281,167
I20240906 13:39:01 15556 dinov2 loaders.py:117] sampler: infinite
I20240906 13:39:01 15556 dinov2 loaders.py:211] using PyTorch data loader
I20240906 13:39:01 15556 dinov2 loaders.py:226] infinite data loader
I20240906 13:39:01 15556 dinov2 train.py:216] Starting training from iteration 0
I20240906 13:39:42 2180 dinov2 config.py:59] git:
  sha: d43546fae6805ceb22618af8cf78469b61d913c7, status: has uncommitted changes, branch: main

I20240906 13:39:42 2180 dinov2 config.py:60] config_file: configs/train/vitl16_short.yaml
eval: 
eval_only: False
no_resume: False
opts: ['train.dataset_path=ImageNet:split=TRAIN:root=C:\\Users\\ISI_UTS\\Siladittya\\iclr2023\\inexpts\\datasets\\ILSVRC\\Data\\CLS-LOC:extra=C:\\Users\\ISI_UTS\\Siladittya\\iclr2023\\inexpts\\datasets\\ILSVRC\\Data\\CLS-LOC', 'train.output_dir=C:\\Users\\ISI_UTS\\Siladittya\\iclr2023\\inexpts\\dinov2_nosetup\\dinov2\\dinov2_nosetup\\dinov2']
output_dir: C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\dinov2_nosetup\dinov2\dinov2_nosetup\dinov2
I20240906 13:39:42 2180 dinov2 config.py:26] sqrt scaling learning rate; base: 0.004, new: 0.001
I20240906 13:39:42 2180 dinov2 config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
dino:
  loss_weight: 1.0
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
  koleo_loss_weight: 0.1
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  separate_head: false
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
train:
  batch_size_per_gpu: 64
  dataset_path: ImageNet:split=TRAIN:root=C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\datasets\ILSVRC\Data\CLS-LOC:extra=C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\datasets\ILSVRC\Data\CLS-LOC
  output_dir: C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\dinov2_nosetup\dinov2\dinov2_nosetup\dinov2
  saveckp_freq: 20
  seed: 0
  num_workers: 0
  OFFICIAL_EPOCH_LENGTH: 1250
  cache_dataset: true
  centering: centering
student:
  arch: vit_large
  patch_size: 16
  drop_path_rate: 0.3
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: mlp
  block_chunks: 4
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
  num_register_tokens: 0
  interpolate_antialias: false
  interpolate_offset: 0.1
teacher:
  momentum_teacher: 0.992
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 30
optim:
  epochs: 100
  weight_decay: 0.04
  weight_decay_end: 0.4
  base_lr: 0.004
  lr: 0.001
  warmup_epochs: 10
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 0.9
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 8
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 224
  local_crops_size: 96
evaluation:
  eval_period_iterations: 12500

I20240906 13:39:42 2180 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20240906 13:39:44 2180 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20240906 13:39:47 2180 dinov2 ssl_meta_arch.py:43] OPTIONS -- architecture : embed_dim: 1024
I20240906 13:39:47 2180 dinov2 ssl_meta_arch.py:58] OPTIONS -- DINO
I20240906 13:39:47 2180 dinov2 ssl_meta_arch.py:60] OPTIONS -- DINO -- loss_weight: 1.0
I20240906 13:39:47 2180 dinov2 ssl_meta_arch.py:61] OPTIONS -- DINO -- head_n_prototypes: 65536
I20240906 13:39:47 2180 dinov2 ssl_meta_arch.py:62] OPTIONS -- DINO -- head_bottleneck_dim: 256
I20240906 13:39:47 2180 dinov2 ssl_meta_arch.py:63] OPTIONS -- DINO -- head_hidden_dim: 2048
I20240906 13:39:47 2180 dinov2 ssl_meta_arch.py:75] OPTIONS -- DINO -- applying KOLEO regularization
I20240906 13:39:47 2180 dinov2 ssl_meta_arch.py:85] OPTIONS -- IBOT
I20240906 13:39:47 2180 dinov2 ssl_meta_arch.py:86] OPTIONS -- IBOT -- loss_weight: 1.0
I20240906 13:39:47 2180 dinov2 ssl_meta_arch.py:87] OPTIONS -- IBOT masking -- ibot_mask_ratio_tuple: [0.1, 0.5]
I20240906 13:39:47 2180 dinov2 ssl_meta_arch.py:88] OPTIONS -- IBOT masking -- ibot_mask_sample_probability: 0.5
I20240906 13:39:47 2180 dinov2 ssl_meta_arch.py:111] OPTIONS -- IBOT -- head shared with DINO
I20240906 13:39:47 2180 dinov2 ssl_meta_arch.py:121] Student and Teacher are built: they are both vit_large network.
I20240906 13:39:48 2180 dinov2 train.py:302] Model:
SSLMetaArch(
  (dino_loss): DINOLoss()
  (koleo_loss): KoLeoLoss(
    (pdist): PairwiseDistance()
  )
  (ibot_patch_loss): iBOTPatchLoss()
  (student): ModuleDict(
    (backbone): DinoVisionTransformer(
      (patch_embed): PatchEmbed(
        (proj): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))
        (norm): Identity()
      )
      (blocks): ModuleList(
        (0): BlockChunk(
          (0-5): 6 x NestedTensorBlock(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): DropPath()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): DropPath()
          )
        )
        (1): BlockChunk(
          (0-5): 6 x Identity()
          (6-11): 6 x NestedTensorBlock(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): DropPath()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): DropPath()
          )
        )
        (2): BlockChunk(
          (0-11): 12 x Identity()
          (12-17): 6 x NestedTensorBlock(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): DropPath()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): DropPath()
          )
        )
        (3): BlockChunk(
          (0-17): 18 x Identity()
          (18-23): 6 x NestedTensorBlock(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): DropPath()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): DropPath()
          )
        )
      )
      (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (head): Identity()
    )
    (dino_head): DINOHead(
      (mlp): Sequential(
        (0): Linear(in_features=1024, out_features=2048, bias=True)
        (1): GELU(approximate='none')
        (2): Linear(in_features=2048, out_features=2048, bias=True)
        (3): GELU(approximate='none')
        (4): Linear(in_features=2048, out_features=256, bias=True)
      )
      (last_layer): Linear(in_features=256, out_features=65536, bias=False)
    )
  )
  (teacher): ModuleDict(
    (backbone): DinoVisionTransformer(
      (patch_embed): PatchEmbed(
        (proj): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))
        (norm): Identity()
      )
      (blocks): ModuleList(
        (0): BlockChunk(
          (0-5): 6 x NestedTensorBlock(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): Identity()
          )
        )
        (1): BlockChunk(
          (0-5): 6 x Identity()
          (6-11): 6 x NestedTensorBlock(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): Identity()
          )
        )
        (2): BlockChunk(
          (0-11): 12 x Identity()
          (12-17): 6 x NestedTensorBlock(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): Identity()
          )
        )
        (3): BlockChunk(
          (0-17): 18 x Identity()
          (18-23): 6 x NestedTensorBlock(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): Identity()
          )
        )
      )
      (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (head): Identity()
    )
    (dino_head): DINOHead(
      (mlp): Sequential(
        (0): Linear(in_features=1024, out_features=2048, bias=True)
        (1): GELU(approximate='none')
        (2): Linear(in_features=2048, out_features=2048, bias=True)
        (3): GELU(approximate='none')
        (4): Linear(in_features=2048, out_features=256, bias=True)
      )
      (last_layer): Linear(in_features=256, out_features=65536, bias=False)
    )
  )
)
I20240906 13:39:48 2180 dinov2 param_groups.py:54] chunked fsdp
I20240906 13:39:48 2180 dinov2 param_groups.py:87] cls_token: lr_multiplier: 0.0717897987691853, wd_multiplier: 1.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] pos_embed: lr_multiplier: 0.0717897987691853, wd_multiplier: 1.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] mask_token: lr_multiplier: 0.0717897987691853, wd_multiplier: 1.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] patch_embed.proj.weight: lr_multiplier: 0.01435795975383706, wd_multiplier: 1.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] patch_embed.proj.bias: lr_multiplier: 0.01435795975383706, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.0.0.norm1.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.0.0.norm1.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.0.0.attn.qkv.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 1.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.0.0.attn.qkv.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.0.0.attn.proj.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 1.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.0.0.attn.proj.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.0.0.ls1.gamma: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.0.0.norm2.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.0.0.norm2.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.0.0.mlp.fc1.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 1.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.0.0.mlp.fc1.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.0.0.mlp.fc2.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 1.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.0.0.mlp.fc2.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.0.0.ls2.gamma: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.0.1.norm1.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.0.1.norm1.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.0.1.attn.qkv.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 1.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.0.1.attn.qkv.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.0.1.attn.proj.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 1.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.0.1.attn.proj.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.0.1.ls1.gamma: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.0.1.norm2.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.0.1.norm2.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.0.1.mlp.fc1.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 1.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.0.1.mlp.fc1.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.0.1.mlp.fc2.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 1.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.0.1.mlp.fc2.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.0.1.ls2.gamma: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.0.2.norm1.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.0.2.norm1.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.0.2.attn.qkv.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 1.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.0.2.attn.qkv.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.0.2.attn.proj.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 1.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.0.2.attn.proj.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.0.2.ls1.gamma: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.0.2.norm2.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.0.2.norm2.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.0.2.mlp.fc1.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 1.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.0.2.mlp.fc1.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.0.2.mlp.fc2.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 1.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.0.2.mlp.fc2.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.0.2.ls2.gamma: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.0.3.norm1.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.0.3.norm1.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.0.3.attn.qkv.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 1.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.0.3.attn.qkv.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.0.3.attn.proj.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 1.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.0.3.attn.proj.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.0.3.ls1.gamma: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.0.3.norm2.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.0.3.norm2.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.0.3.mlp.fc1.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 1.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.0.3.mlp.fc1.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.0.3.mlp.fc2.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 1.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.0.3.mlp.fc2.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.0.3.ls2.gamma: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.0.4.norm1.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.0.4.norm1.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.0.4.attn.qkv.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 1.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.0.4.attn.qkv.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.0.4.attn.proj.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 1.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.0.4.attn.proj.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.0.4.ls1.gamma: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.0.4.norm2.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.0.4.norm2.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.0.4.mlp.fc1.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 1.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.0.4.mlp.fc1.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.0.4.mlp.fc2.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 1.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.0.4.mlp.fc2.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.0.4.ls2.gamma: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.0.5.norm1.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.0.5.norm1.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.0.5.attn.qkv.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 1.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.0.5.attn.qkv.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.0.5.attn.proj.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 1.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.0.5.attn.proj.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.0.5.ls1.gamma: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.0.5.norm2.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.0.5.norm2.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.0.5.mlp.fc1.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 1.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.0.5.mlp.fc1.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.0.5.mlp.fc2.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 1.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.0.5.mlp.fc2.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.0.5.ls2.gamma: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.1.6.norm1.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.1.6.norm1.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.1.6.attn.qkv.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 1.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.1.6.attn.qkv.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.1.6.attn.proj.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 1.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.1.6.attn.proj.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.1.6.ls1.gamma: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.1.6.norm2.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.1.6.norm2.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.1.6.mlp.fc1.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 1.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.1.6.mlp.fc1.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.1.6.mlp.fc2.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 1.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.1.6.mlp.fc2.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.1.6.ls2.gamma: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.1.7.norm1.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.1.7.norm1.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.1.7.attn.qkv.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 1.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.1.7.attn.qkv.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.1.7.attn.proj.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 1.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.1.7.attn.proj.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.1.7.ls1.gamma: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.1.7.norm2.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.1.7.norm2.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.1.7.mlp.fc1.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 1.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.1.7.mlp.fc1.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.1.7.mlp.fc2.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 1.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.1.7.mlp.fc2.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.1.7.ls2.gamma: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.1.8.norm1.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.1.8.norm1.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.1.8.attn.qkv.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 1.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.1.8.attn.qkv.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.1.8.attn.proj.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 1.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.1.8.attn.proj.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.1.8.ls1.gamma: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.1.8.norm2.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.1.8.norm2.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.1.8.mlp.fc1.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 1.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.1.8.mlp.fc1.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.1.8.mlp.fc2.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 1.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.1.8.mlp.fc2.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.1.8.ls2.gamma: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.1.9.norm1.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.1.9.norm1.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.1.9.attn.qkv.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 1.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.1.9.attn.qkv.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.1.9.attn.proj.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 1.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.1.9.attn.proj.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.1.9.ls1.gamma: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.1.9.norm2.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.1.9.norm2.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.1.9.mlp.fc1.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 1.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.1.9.mlp.fc1.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.1.9.mlp.fc2.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 1.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.1.9.mlp.fc2.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.1.9.ls2.gamma: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.1.10.norm1.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.1.10.norm1.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.1.10.attn.qkv.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 1.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.1.10.attn.qkv.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.1.10.attn.proj.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 1.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.1.10.attn.proj.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.1.10.ls1.gamma: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.1.10.norm2.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.1.10.norm2.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.1.10.mlp.fc1.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 1.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.1.10.mlp.fc1.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.1.10.mlp.fc2.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 1.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.1.10.mlp.fc2.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.1.10.ls2.gamma: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.1.11.norm1.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.1.11.norm1.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.1.11.attn.qkv.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 1.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.1.11.attn.qkv.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.1.11.attn.proj.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 1.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.1.11.attn.proj.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.1.11.ls1.gamma: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.1.11.norm2.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.1.11.norm2.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.1.11.mlp.fc1.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 1.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.1.11.mlp.fc1.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.1.11.mlp.fc2.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 1.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.1.11.mlp.fc2.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.1.11.ls2.gamma: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.2.12.norm1.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.2.12.norm1.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.2.12.attn.qkv.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 1.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.2.12.attn.qkv.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.2.12.attn.proj.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 1.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.2.12.attn.proj.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.2.12.ls1.gamma: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.2.12.norm2.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.2.12.norm2.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.2.12.mlp.fc1.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 1.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.2.12.mlp.fc1.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.2.12.mlp.fc2.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 1.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.2.12.mlp.fc2.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.2.12.ls2.gamma: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.2.13.norm1.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.2.13.norm1.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.2.13.attn.qkv.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 1.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.2.13.attn.qkv.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.2.13.attn.proj.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 1.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.2.13.attn.proj.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.2.13.ls1.gamma: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.2.13.norm2.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.2.13.norm2.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.2.13.mlp.fc1.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 1.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.2.13.mlp.fc1.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.2.13.mlp.fc2.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 1.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.2.13.mlp.fc2.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.2.13.ls2.gamma: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.2.14.norm1.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.2.14.norm1.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.2.14.attn.qkv.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 1.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.2.14.attn.qkv.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.2.14.attn.proj.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 1.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.2.14.attn.proj.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.2.14.ls1.gamma: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.2.14.norm2.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.2.14.norm2.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.2.14.mlp.fc1.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 1.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.2.14.mlp.fc1.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.2.14.mlp.fc2.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 1.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.2.14.mlp.fc2.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.2.14.ls2.gamma: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.2.15.norm1.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.2.15.norm1.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.2.15.attn.qkv.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 1.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.2.15.attn.qkv.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.2.15.attn.proj.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 1.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.2.15.attn.proj.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.2.15.ls1.gamma: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.2.15.norm2.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.2.15.norm2.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.2.15.mlp.fc1.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 1.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.2.15.mlp.fc1.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.2.15.mlp.fc2.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 1.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.2.15.mlp.fc2.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.2.15.ls2.gamma: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.2.16.norm1.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.2.16.norm1.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.2.16.attn.qkv.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 1.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.2.16.attn.qkv.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.2.16.attn.proj.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 1.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.2.16.attn.proj.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.2.16.ls1.gamma: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.2.16.norm2.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.2.16.norm2.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.2.16.mlp.fc1.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 1.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.2.16.mlp.fc1.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.2.16.mlp.fc2.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 1.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.2.16.mlp.fc2.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.2.16.ls2.gamma: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.2.17.norm1.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.2.17.norm1.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.2.17.attn.qkv.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 1.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.2.17.attn.qkv.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.2.17.attn.proj.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 1.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.2.17.attn.proj.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.2.17.ls1.gamma: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.2.17.norm2.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.2.17.norm2.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.2.17.mlp.fc1.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 1.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.2.17.mlp.fc1.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.2.17.mlp.fc2.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 1.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.2.17.mlp.fc2.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.2.17.ls2.gamma: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.3.18.norm1.weight: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.3.18.norm1.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.3.18.attn.qkv.weight: lr_multiplier: 0.531441, wd_multiplier: 1.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.3.18.attn.qkv.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.3.18.attn.proj.weight: lr_multiplier: 0.531441, wd_multiplier: 1.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.3.18.attn.proj.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.3.18.ls1.gamma: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.3.18.norm2.weight: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.3.18.norm2.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.3.18.mlp.fc1.weight: lr_multiplier: 0.531441, wd_multiplier: 1.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.3.18.mlp.fc1.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.3.18.mlp.fc2.weight: lr_multiplier: 0.531441, wd_multiplier: 1.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.3.18.mlp.fc2.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.3.18.ls2.gamma: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.3.19.norm1.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.3.19.norm1.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.3.19.attn.qkv.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 1.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.3.19.attn.qkv.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.3.19.attn.proj.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 1.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.3.19.attn.proj.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.3.19.ls1.gamma: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.3.19.norm2.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.3.19.norm2.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.3.19.mlp.fc1.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 1.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.3.19.mlp.fc1.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.3.19.mlp.fc2.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 1.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.3.19.mlp.fc2.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.3.19.ls2.gamma: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.3.20.norm1.weight: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.3.20.norm1.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.3.20.attn.qkv.weight: lr_multiplier: 0.6561, wd_multiplier: 1.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.3.20.attn.qkv.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.3.20.attn.proj.weight: lr_multiplier: 0.6561, wd_multiplier: 1.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.3.20.attn.proj.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.3.20.ls1.gamma: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.3.20.norm2.weight: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.3.20.norm2.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.3.20.mlp.fc1.weight: lr_multiplier: 0.6561, wd_multiplier: 1.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.3.20.mlp.fc1.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.3.20.mlp.fc2.weight: lr_multiplier: 0.6561, wd_multiplier: 1.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.3.20.mlp.fc2.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.3.20.ls2.gamma: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.3.21.norm1.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.3.21.norm1.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.3.21.attn.qkv.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 1.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.3.21.attn.qkv.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.3.21.attn.proj.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 1.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.3.21.attn.proj.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.3.21.ls1.gamma: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.3.21.norm2.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.3.21.norm2.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.3.21.mlp.fc1.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 1.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.3.21.mlp.fc1.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.3.21.mlp.fc2.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 1.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.3.21.mlp.fc2.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.3.21.ls2.gamma: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.3.22.norm1.weight: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.3.22.norm1.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.3.22.attn.qkv.weight: lr_multiplier: 0.81, wd_multiplier: 1.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.3.22.attn.qkv.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.3.22.attn.proj.weight: lr_multiplier: 0.81, wd_multiplier: 1.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.3.22.attn.proj.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.3.22.ls1.gamma: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.3.22.norm2.weight: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.3.22.norm2.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.3.22.mlp.fc1.weight: lr_multiplier: 0.81, wd_multiplier: 1.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.3.22.mlp.fc1.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.3.22.mlp.fc2.weight: lr_multiplier: 0.81, wd_multiplier: 1.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.3.22.mlp.fc2.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.3.22.ls2.gamma: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.3.23.norm1.weight: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.3.23.norm1.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.3.23.attn.qkv.weight: lr_multiplier: 0.9, wd_multiplier: 1.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.3.23.attn.qkv.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.3.23.attn.proj.weight: lr_multiplier: 0.9, wd_multiplier: 1.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.3.23.attn.proj.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.3.23.ls1.gamma: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.3.23.norm2.weight: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.3.23.norm2.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.3.23.mlp.fc1.weight: lr_multiplier: 0.9, wd_multiplier: 1.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.3.23.mlp.fc1.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.3.23.mlp.fc2.weight: lr_multiplier: 0.9, wd_multiplier: 1.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.3.23.mlp.fc2.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] blocks.3.23.ls2.gamma: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] norm.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] norm.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 ssl_meta_arch.py:378] fusing param groups
I20240906 13:39:48 2180 dinov2 param_groups.py:64] else code branch
I20240906 13:39:48 2180 dinov2 param_groups.py:87] mlp.0.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] mlp.0.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] mlp.2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] mlp.2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] mlp.4.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] mlp.4.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] last_layer.weight_g: lr_multiplier: 1.0, wd_multiplier: 1.0
I20240906 13:39:48 2180 dinov2 param_groups.py:87] last_layer.weight_v: lr_multiplier: 1.0, wd_multiplier: 1.0
I20240906 13:39:48 2180 dinov2 ssl_meta_arch.py:378] fusing param groups
I20240906 13:39:48 2180 dinov2 train.py:98] Schedulers ready.
I20240906 13:39:48 2180 dinov2 augmentations.py:34] ###################################
I20240906 13:39:48 2180 dinov2 augmentations.py:35] Using data augmentation parameters:
I20240906 13:39:48 2180 dinov2 augmentations.py:36] global_crops_scale: [0.32, 1.0]
I20240906 13:39:48 2180 dinov2 augmentations.py:37] local_crops_scale: [0.05, 0.32]
I20240906 13:39:48 2180 dinov2 augmentations.py:38] local_crops_number: 8
I20240906 13:39:48 2180 dinov2 augmentations.py:39] global_crops_size: 224
I20240906 13:39:48 2180 dinov2 augmentations.py:40] local_crops_size: 96
I20240906 13:39:48 2180 dinov2 augmentations.py:41] ###################################
I20240906 13:39:48 2180 dinov2 loaders.py:89] using dataset: "ImageNet:split=TRAIN:root=C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\datasets\ILSVRC\Data\CLS-LOC:extra=C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\datasets\ILSVRC\Data\CLS-LOC"
I20240906 13:39:48 2180 dinov2 loaders.py:94] # of dataset samples: 1,281,167
I20240906 13:39:48 2180 dinov2 loaders.py:117] sampler: infinite
I20240906 13:39:48 2180 dinov2 loaders.py:211] using PyTorch data loader
I20240906 13:39:48 2180 dinov2 loaders.py:226] infinite data loader
I20240906 13:39:48 2180 dinov2 train.py:216] Starting training from iteration 0
W20240906 13:39:52 2180 xformers __init__.py:50] A matching Triton is not available, some optimizations will not be enabled.
Error caught was: No module named 'triton'
I20240906 13:41:11 17136 dinov2 config.py:59] git:
  sha: d43546fae6805ceb22618af8cf78469b61d913c7, status: has uncommitted changes, branch: main

I20240906 13:41:11 17136 dinov2 config.py:60] config_file: configs/train/vitl16_short.yaml
eval: 
eval_only: False
no_resume: False
opts: ['train.dataset_path=ImageNet:split=TRAIN:root=C:\\Users\\ISI_UTS\\Siladittya\\iclr2023\\inexpts\\datasets\\ILSVRC\\Data\\CLS-LOC:extra=C:\\Users\\ISI_UTS\\Siladittya\\iclr2023\\inexpts\\datasets\\ILSVRC\\Data\\CLS-LOC', 'train.output_dir=C:\\Users\\ISI_UTS\\Siladittya\\iclr2023\\inexpts\\dinov2_nosetup\\dinov2\\dinov2_nosetup\\dinov2']
output_dir: C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\dinov2_nosetup\dinov2\dinov2_nosetup\dinov2
I20240906 13:41:11 17136 dinov2 config.py:26] sqrt scaling learning rate; base: 0.004, new: 0.001
I20240906 13:41:11 17136 dinov2 config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
dino:
  loss_weight: 1.0
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
  koleo_loss_weight: 0.1
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  separate_head: false
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
train:
  batch_size_per_gpu: 64
  dataset_path: ImageNet:split=TRAIN:root=C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\datasets\ILSVRC\Data\CLS-LOC:extra=C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\datasets\ILSVRC\Data\CLS-LOC
  output_dir: C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\dinov2_nosetup\dinov2\dinov2_nosetup\dinov2
  saveckp_freq: 20
  seed: 0
  num_workers: 0
  OFFICIAL_EPOCH_LENGTH: 1250
  cache_dataset: true
  centering: centering
student:
  arch: vit_large
  patch_size: 16
  drop_path_rate: 0.3
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: mlp
  block_chunks: 4
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
  num_register_tokens: 0
  interpolate_antialias: false
  interpolate_offset: 0.1
teacher:
  momentum_teacher: 0.992
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 30
optim:
  epochs: 100
  weight_decay: 0.04
  weight_decay_end: 0.4
  base_lr: 0.004
  lr: 0.001
  warmup_epochs: 10
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 0.9
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 8
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 224
  local_crops_size: 96
evaluation:
  eval_period_iterations: 12500

I20240906 13:41:11 17136 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20240906 13:41:14 17136 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20240906 13:41:16 17136 dinov2 ssl_meta_arch.py:43] OPTIONS -- architecture : embed_dim: 1024
I20240906 13:41:16 17136 dinov2 ssl_meta_arch.py:58] OPTIONS -- DINO
I20240906 13:41:16 17136 dinov2 ssl_meta_arch.py:60] OPTIONS -- DINO -- loss_weight: 1.0
I20240906 13:41:16 17136 dinov2 ssl_meta_arch.py:61] OPTIONS -- DINO -- head_n_prototypes: 65536
I20240906 13:41:16 17136 dinov2 ssl_meta_arch.py:62] OPTIONS -- DINO -- head_bottleneck_dim: 256
I20240906 13:41:16 17136 dinov2 ssl_meta_arch.py:63] OPTIONS -- DINO -- head_hidden_dim: 2048
I20240906 13:41:16 17136 dinov2 ssl_meta_arch.py:75] OPTIONS -- DINO -- applying KOLEO regularization
I20240906 13:41:16 17136 dinov2 ssl_meta_arch.py:85] OPTIONS -- IBOT
I20240906 13:41:16 17136 dinov2 ssl_meta_arch.py:86] OPTIONS -- IBOT -- loss_weight: 1.0
I20240906 13:41:16 17136 dinov2 ssl_meta_arch.py:87] OPTIONS -- IBOT masking -- ibot_mask_ratio_tuple: [0.1, 0.5]
I20240906 13:41:16 17136 dinov2 ssl_meta_arch.py:88] OPTIONS -- IBOT masking -- ibot_mask_sample_probability: 0.5
I20240906 13:41:16 17136 dinov2 ssl_meta_arch.py:111] OPTIONS -- IBOT -- head shared with DINO
I20240906 13:41:16 17136 dinov2 ssl_meta_arch.py:121] Student and Teacher are built: they are both vit_large network.
I20240906 13:41:17 17136 dinov2 train.py:302] Model:
SSLMetaArch(
  (dino_loss): DINOLoss()
  (koleo_loss): KoLeoLoss(
    (pdist): PairwiseDistance()
  )
  (ibot_patch_loss): iBOTPatchLoss()
  (student): ModuleDict(
    (backbone): DinoVisionTransformer(
      (patch_embed): PatchEmbed(
        (proj): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))
        (norm): Identity()
      )
      (blocks): ModuleList(
        (0): BlockChunk(
          (0-5): 6 x NestedTensorBlock(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): DropPath()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): DropPath()
          )
        )
        (1): BlockChunk(
          (0-5): 6 x Identity()
          (6-11): 6 x NestedTensorBlock(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): DropPath()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): DropPath()
          )
        )
        (2): BlockChunk(
          (0-11): 12 x Identity()
          (12-17): 6 x NestedTensorBlock(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): DropPath()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): DropPath()
          )
        )
        (3): BlockChunk(
          (0-17): 18 x Identity()
          (18-23): 6 x NestedTensorBlock(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): DropPath()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): DropPath()
          )
        )
      )
      (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (head): Identity()
    )
    (dino_head): DINOHead(
      (mlp): Sequential(
        (0): Linear(in_features=1024, out_features=2048, bias=True)
        (1): GELU(approximate='none')
        (2): Linear(in_features=2048, out_features=2048, bias=True)
        (3): GELU(approximate='none')
        (4): Linear(in_features=2048, out_features=256, bias=True)
      )
      (last_layer): Linear(in_features=256, out_features=65536, bias=False)
    )
  )
  (teacher): ModuleDict(
    (backbone): DinoVisionTransformer(
      (patch_embed): PatchEmbed(
        (proj): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))
        (norm): Identity()
      )
      (blocks): ModuleList(
        (0): BlockChunk(
          (0-5): 6 x NestedTensorBlock(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): Identity()
          )
        )
        (1): BlockChunk(
          (0-5): 6 x Identity()
          (6-11): 6 x NestedTensorBlock(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): Identity()
          )
        )
        (2): BlockChunk(
          (0-11): 12 x Identity()
          (12-17): 6 x NestedTensorBlock(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): Identity()
          )
        )
        (3): BlockChunk(
          (0-17): 18 x Identity()
          (18-23): 6 x NestedTensorBlock(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): Identity()
          )
        )
      )
      (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (head): Identity()
    )
    (dino_head): DINOHead(
      (mlp): Sequential(
        (0): Linear(in_features=1024, out_features=2048, bias=True)
        (1): GELU(approximate='none')
        (2): Linear(in_features=2048, out_features=2048, bias=True)
        (3): GELU(approximate='none')
        (4): Linear(in_features=2048, out_features=256, bias=True)
      )
      (last_layer): Linear(in_features=256, out_features=65536, bias=False)
    )
  )
)
I20240906 13:41:17 17136 dinov2 param_groups.py:54] chunked fsdp
I20240906 13:41:17 17136 dinov2 param_groups.py:87] cls_token: lr_multiplier: 0.0717897987691853, wd_multiplier: 1.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] pos_embed: lr_multiplier: 0.0717897987691853, wd_multiplier: 1.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] mask_token: lr_multiplier: 0.0717897987691853, wd_multiplier: 1.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] patch_embed.proj.weight: lr_multiplier: 0.01435795975383706, wd_multiplier: 1.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] patch_embed.proj.bias: lr_multiplier: 0.01435795975383706, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.0.0.norm1.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.0.0.norm1.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.0.0.attn.qkv.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 1.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.0.0.attn.qkv.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.0.0.attn.proj.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 1.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.0.0.attn.proj.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.0.0.ls1.gamma: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.0.0.norm2.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.0.0.norm2.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.0.0.mlp.fc1.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 1.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.0.0.mlp.fc1.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.0.0.mlp.fc2.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 1.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.0.0.mlp.fc2.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.0.0.ls2.gamma: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.0.1.norm1.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.0.1.norm1.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.0.1.attn.qkv.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 1.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.0.1.attn.qkv.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.0.1.attn.proj.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 1.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.0.1.attn.proj.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.0.1.ls1.gamma: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.0.1.norm2.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.0.1.norm2.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.0.1.mlp.fc1.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 1.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.0.1.mlp.fc1.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.0.1.mlp.fc2.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 1.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.0.1.mlp.fc2.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.0.1.ls2.gamma: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.0.2.norm1.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.0.2.norm1.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.0.2.attn.qkv.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 1.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.0.2.attn.qkv.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.0.2.attn.proj.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 1.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.0.2.attn.proj.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.0.2.ls1.gamma: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.0.2.norm2.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.0.2.norm2.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.0.2.mlp.fc1.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 1.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.0.2.mlp.fc1.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.0.2.mlp.fc2.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 1.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.0.2.mlp.fc2.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.0.2.ls2.gamma: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.0.3.norm1.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.0.3.norm1.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.0.3.attn.qkv.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 1.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.0.3.attn.qkv.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.0.3.attn.proj.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 1.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.0.3.attn.proj.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.0.3.ls1.gamma: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.0.3.norm2.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.0.3.norm2.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.0.3.mlp.fc1.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 1.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.0.3.mlp.fc1.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.0.3.mlp.fc2.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 1.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.0.3.mlp.fc2.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.0.3.ls2.gamma: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.0.4.norm1.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.0.4.norm1.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.0.4.attn.qkv.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 1.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.0.4.attn.qkv.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.0.4.attn.proj.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 1.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.0.4.attn.proj.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.0.4.ls1.gamma: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.0.4.norm2.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.0.4.norm2.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.0.4.mlp.fc1.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 1.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.0.4.mlp.fc1.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.0.4.mlp.fc2.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 1.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.0.4.mlp.fc2.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.0.4.ls2.gamma: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.0.5.norm1.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.0.5.norm1.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.0.5.attn.qkv.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 1.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.0.5.attn.qkv.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.0.5.attn.proj.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 1.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.0.5.attn.proj.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.0.5.ls1.gamma: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.0.5.norm2.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.0.5.norm2.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.0.5.mlp.fc1.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 1.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.0.5.mlp.fc1.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.0.5.mlp.fc2.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 1.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.0.5.mlp.fc2.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.0.5.ls2.gamma: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.1.6.norm1.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.1.6.norm1.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.1.6.attn.qkv.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 1.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.1.6.attn.qkv.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.1.6.attn.proj.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 1.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.1.6.attn.proj.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.1.6.ls1.gamma: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.1.6.norm2.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.1.6.norm2.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.1.6.mlp.fc1.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 1.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.1.6.mlp.fc1.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.1.6.mlp.fc2.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 1.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.1.6.mlp.fc2.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.1.6.ls2.gamma: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.1.7.norm1.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.1.7.norm1.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.1.7.attn.qkv.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 1.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.1.7.attn.qkv.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.1.7.attn.proj.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 1.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.1.7.attn.proj.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.1.7.ls1.gamma: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.1.7.norm2.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.1.7.norm2.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.1.7.mlp.fc1.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 1.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.1.7.mlp.fc1.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.1.7.mlp.fc2.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 1.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.1.7.mlp.fc2.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.1.7.ls2.gamma: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.1.8.norm1.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.1.8.norm1.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.1.8.attn.qkv.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 1.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.1.8.attn.qkv.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.1.8.attn.proj.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 1.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.1.8.attn.proj.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.1.8.ls1.gamma: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.1.8.norm2.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.1.8.norm2.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.1.8.mlp.fc1.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 1.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.1.8.mlp.fc1.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.1.8.mlp.fc2.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 1.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.1.8.mlp.fc2.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.1.8.ls2.gamma: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.1.9.norm1.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.1.9.norm1.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.1.9.attn.qkv.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 1.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.1.9.attn.qkv.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.1.9.attn.proj.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 1.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.1.9.attn.proj.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.1.9.ls1.gamma: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.1.9.norm2.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.1.9.norm2.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.1.9.mlp.fc1.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 1.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.1.9.mlp.fc1.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.1.9.mlp.fc2.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 1.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.1.9.mlp.fc2.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.1.9.ls2.gamma: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.1.10.norm1.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.1.10.norm1.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.1.10.attn.qkv.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 1.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.1.10.attn.qkv.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.1.10.attn.proj.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 1.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.1.10.attn.proj.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.1.10.ls1.gamma: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.1.10.norm2.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.1.10.norm2.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.1.10.mlp.fc1.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 1.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.1.10.mlp.fc1.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.1.10.mlp.fc2.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 1.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.1.10.mlp.fc2.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.1.10.ls2.gamma: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.1.11.norm1.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.1.11.norm1.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.1.11.attn.qkv.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 1.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.1.11.attn.qkv.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.1.11.attn.proj.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 1.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.1.11.attn.proj.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.1.11.ls1.gamma: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.1.11.norm2.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.1.11.norm2.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.1.11.mlp.fc1.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 1.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.1.11.mlp.fc1.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.1.11.mlp.fc2.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 1.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.1.11.mlp.fc2.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.1.11.ls2.gamma: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.2.12.norm1.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.2.12.norm1.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.2.12.attn.qkv.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 1.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.2.12.attn.qkv.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.2.12.attn.proj.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 1.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.2.12.attn.proj.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.2.12.ls1.gamma: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.2.12.norm2.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.2.12.norm2.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.2.12.mlp.fc1.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 1.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.2.12.mlp.fc1.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.2.12.mlp.fc2.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 1.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.2.12.mlp.fc2.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.2.12.ls2.gamma: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.2.13.norm1.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.2.13.norm1.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.2.13.attn.qkv.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 1.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.2.13.attn.qkv.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.2.13.attn.proj.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 1.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.2.13.attn.proj.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.2.13.ls1.gamma: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.2.13.norm2.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.2.13.norm2.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.2.13.mlp.fc1.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 1.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.2.13.mlp.fc1.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.2.13.mlp.fc2.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 1.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.2.13.mlp.fc2.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.2.13.ls2.gamma: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.2.14.norm1.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.2.14.norm1.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.2.14.attn.qkv.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 1.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.2.14.attn.qkv.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.2.14.attn.proj.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 1.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.2.14.attn.proj.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.2.14.ls1.gamma: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.2.14.norm2.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.2.14.norm2.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.2.14.mlp.fc1.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 1.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.2.14.mlp.fc1.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.2.14.mlp.fc2.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 1.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.2.14.mlp.fc2.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.2.14.ls2.gamma: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.2.15.norm1.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.2.15.norm1.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.2.15.attn.qkv.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 1.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.2.15.attn.qkv.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.2.15.attn.proj.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 1.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.2.15.attn.proj.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.2.15.ls1.gamma: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.2.15.norm2.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.2.15.norm2.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.2.15.mlp.fc1.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 1.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.2.15.mlp.fc1.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.2.15.mlp.fc2.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 1.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.2.15.mlp.fc2.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.2.15.ls2.gamma: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.2.16.norm1.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.2.16.norm1.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.2.16.attn.qkv.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 1.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.2.16.attn.qkv.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.2.16.attn.proj.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 1.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.2.16.attn.proj.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.2.16.ls1.gamma: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.2.16.norm2.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.2.16.norm2.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.2.16.mlp.fc1.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 1.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.2.16.mlp.fc1.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.2.16.mlp.fc2.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 1.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.2.16.mlp.fc2.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.2.16.ls2.gamma: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.2.17.norm1.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.2.17.norm1.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.2.17.attn.qkv.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 1.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.2.17.attn.qkv.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.2.17.attn.proj.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 1.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.2.17.attn.proj.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.2.17.ls1.gamma: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.2.17.norm2.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.2.17.norm2.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.2.17.mlp.fc1.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 1.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.2.17.mlp.fc1.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.2.17.mlp.fc2.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 1.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.2.17.mlp.fc2.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.2.17.ls2.gamma: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.3.18.norm1.weight: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.3.18.norm1.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.3.18.attn.qkv.weight: lr_multiplier: 0.531441, wd_multiplier: 1.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.3.18.attn.qkv.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.3.18.attn.proj.weight: lr_multiplier: 0.531441, wd_multiplier: 1.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.3.18.attn.proj.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.3.18.ls1.gamma: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.3.18.norm2.weight: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.3.18.norm2.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.3.18.mlp.fc1.weight: lr_multiplier: 0.531441, wd_multiplier: 1.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.3.18.mlp.fc1.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.3.18.mlp.fc2.weight: lr_multiplier: 0.531441, wd_multiplier: 1.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.3.18.mlp.fc2.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.3.18.ls2.gamma: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.3.19.norm1.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.3.19.norm1.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.3.19.attn.qkv.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 1.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.3.19.attn.qkv.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.3.19.attn.proj.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 1.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.3.19.attn.proj.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.3.19.ls1.gamma: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.3.19.norm2.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.3.19.norm2.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.3.19.mlp.fc1.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 1.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.3.19.mlp.fc1.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.3.19.mlp.fc2.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 1.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.3.19.mlp.fc2.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.3.19.ls2.gamma: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.3.20.norm1.weight: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.3.20.norm1.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.3.20.attn.qkv.weight: lr_multiplier: 0.6561, wd_multiplier: 1.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.3.20.attn.qkv.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.3.20.attn.proj.weight: lr_multiplier: 0.6561, wd_multiplier: 1.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.3.20.attn.proj.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.3.20.ls1.gamma: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.3.20.norm2.weight: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.3.20.norm2.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.3.20.mlp.fc1.weight: lr_multiplier: 0.6561, wd_multiplier: 1.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.3.20.mlp.fc1.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.3.20.mlp.fc2.weight: lr_multiplier: 0.6561, wd_multiplier: 1.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.3.20.mlp.fc2.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.3.20.ls2.gamma: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.3.21.norm1.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.3.21.norm1.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.3.21.attn.qkv.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 1.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.3.21.attn.qkv.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.3.21.attn.proj.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 1.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.3.21.attn.proj.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.3.21.ls1.gamma: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.3.21.norm2.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.3.21.norm2.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.3.21.mlp.fc1.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 1.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.3.21.mlp.fc1.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.3.21.mlp.fc2.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 1.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.3.21.mlp.fc2.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.3.21.ls2.gamma: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.3.22.norm1.weight: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.3.22.norm1.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.3.22.attn.qkv.weight: lr_multiplier: 0.81, wd_multiplier: 1.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.3.22.attn.qkv.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.3.22.attn.proj.weight: lr_multiplier: 0.81, wd_multiplier: 1.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.3.22.attn.proj.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.3.22.ls1.gamma: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.3.22.norm2.weight: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.3.22.norm2.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.3.22.mlp.fc1.weight: lr_multiplier: 0.81, wd_multiplier: 1.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.3.22.mlp.fc1.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.3.22.mlp.fc2.weight: lr_multiplier: 0.81, wd_multiplier: 1.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.3.22.mlp.fc2.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.3.22.ls2.gamma: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.3.23.norm1.weight: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.3.23.norm1.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.3.23.attn.qkv.weight: lr_multiplier: 0.9, wd_multiplier: 1.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.3.23.attn.qkv.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.3.23.attn.proj.weight: lr_multiplier: 0.9, wd_multiplier: 1.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.3.23.attn.proj.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.3.23.ls1.gamma: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.3.23.norm2.weight: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.3.23.norm2.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.3.23.mlp.fc1.weight: lr_multiplier: 0.9, wd_multiplier: 1.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.3.23.mlp.fc1.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.3.23.mlp.fc2.weight: lr_multiplier: 0.9, wd_multiplier: 1.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.3.23.mlp.fc2.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] blocks.3.23.ls2.gamma: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] norm.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] norm.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 ssl_meta_arch.py:378] fusing param groups
I20240906 13:41:17 17136 dinov2 param_groups.py:64] else code branch
I20240906 13:41:17 17136 dinov2 param_groups.py:87] mlp.0.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] mlp.0.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] mlp.2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] mlp.2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] mlp.4.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] mlp.4.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] last_layer.weight_g: lr_multiplier: 1.0, wd_multiplier: 1.0
I20240906 13:41:17 17136 dinov2 param_groups.py:87] last_layer.weight_v: lr_multiplier: 1.0, wd_multiplier: 1.0
I20240906 13:41:17 17136 dinov2 ssl_meta_arch.py:378] fusing param groups
I20240906 13:41:17 17136 dinov2 train.py:98] Schedulers ready.
I20240906 13:41:17 17136 dinov2 augmentations.py:34] ###################################
I20240906 13:41:17 17136 dinov2 augmentations.py:35] Using data augmentation parameters:
I20240906 13:41:17 17136 dinov2 augmentations.py:36] global_crops_scale: [0.32, 1.0]
I20240906 13:41:17 17136 dinov2 augmentations.py:37] local_crops_scale: [0.05, 0.32]
I20240906 13:41:17 17136 dinov2 augmentations.py:38] local_crops_number: 8
I20240906 13:41:17 17136 dinov2 augmentations.py:39] global_crops_size: 224
I20240906 13:41:17 17136 dinov2 augmentations.py:40] local_crops_size: 96
I20240906 13:41:17 17136 dinov2 augmentations.py:41] ###################################
I20240906 13:41:17 17136 dinov2 loaders.py:89] using dataset: "ImageNet:split=TRAIN:root=C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\datasets\ILSVRC\Data\CLS-LOC:extra=C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\datasets\ILSVRC\Data\CLS-LOC"
I20240906 13:41:17 17136 dinov2 loaders.py:94] # of dataset samples: 1,281,167
I20240906 13:41:17 17136 dinov2 loaders.py:117] sampler: infinite
I20240906 13:41:17 17136 dinov2 loaders.py:211] using PyTorch data loader
I20240906 13:41:17 17136 dinov2 loaders.py:226] infinite data loader
I20240906 13:41:17 17136 dinov2 train.py:216] Starting training from iteration 0
W20240906 13:41:22 17136 xformers __init__.py:50] A matching Triton is not available, some optimizations will not be enabled.
Error caught was: No module named 'triton'
I20240906 13:41:47 20036 dinov2 config.py:59] git:
  sha: d43546fae6805ceb22618af8cf78469b61d913c7, status: has uncommitted changes, branch: main

I20240906 13:41:47 20036 dinov2 config.py:60] config_file: configs/train/vitl16_short.yaml
eval: 
eval_only: False
no_resume: False
opts: ['train.dataset_path=ImageNet:split=TRAIN:root=C:\\Users\\ISI_UTS\\Siladittya\\iclr2023\\inexpts\\datasets\\ILSVRC\\Data\\CLS-LOC:extra=C:\\Users\\ISI_UTS\\Siladittya\\iclr2023\\inexpts\\datasets\\ILSVRC\\Data\\CLS-LOC', 'train.output_dir=C:\\Users\\ISI_UTS\\Siladittya\\iclr2023\\inexpts\\dinov2_nosetup\\dinov2\\dinov2_nosetup\\dinov2']
output_dir: C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\dinov2_nosetup\dinov2\dinov2_nosetup\dinov2
I20240906 13:41:47 20036 dinov2 config.py:26] sqrt scaling learning rate; base: 0.004, new: 0.001
I20240906 13:41:47 20036 dinov2 config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
dino:
  loss_weight: 1.0
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
  koleo_loss_weight: 0.1
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  separate_head: false
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
train:
  batch_size_per_gpu: 64
  dataset_path: ImageNet:split=TRAIN:root=C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\datasets\ILSVRC\Data\CLS-LOC:extra=C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\datasets\ILSVRC\Data\CLS-LOC
  output_dir: C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\dinov2_nosetup\dinov2\dinov2_nosetup\dinov2
  saveckp_freq: 20
  seed: 0
  num_workers: 0
  OFFICIAL_EPOCH_LENGTH: 1250
  cache_dataset: true
  centering: centering
student:
  arch: vit_large
  patch_size: 16
  drop_path_rate: 0.3
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: mlp
  block_chunks: 4
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
  num_register_tokens: 0
  interpolate_antialias: false
  interpolate_offset: 0.1
teacher:
  momentum_teacher: 0.992
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 30
optim:
  epochs: 100
  weight_decay: 0.04
  weight_decay_end: 0.4
  base_lr: 0.004
  lr: 0.001
  warmup_epochs: 10
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 0.9
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 8
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 224
  local_crops_size: 96
evaluation:
  eval_period_iterations: 12500

I20240906 13:41:47 20036 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20240906 13:41:50 20036 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20240906 13:41:52 20036 dinov2 ssl_meta_arch.py:43] OPTIONS -- architecture : embed_dim: 1024
I20240906 13:41:52 20036 dinov2 ssl_meta_arch.py:58] OPTIONS -- DINO
I20240906 13:41:52 20036 dinov2 ssl_meta_arch.py:60] OPTIONS -- DINO -- loss_weight: 1.0
I20240906 13:41:52 20036 dinov2 ssl_meta_arch.py:61] OPTIONS -- DINO -- head_n_prototypes: 65536
I20240906 13:41:52 20036 dinov2 ssl_meta_arch.py:62] OPTIONS -- DINO -- head_bottleneck_dim: 256
I20240906 13:41:52 20036 dinov2 ssl_meta_arch.py:63] OPTIONS -- DINO -- head_hidden_dim: 2048
I20240906 13:41:52 20036 dinov2 ssl_meta_arch.py:75] OPTIONS -- DINO -- applying KOLEO regularization
I20240906 13:41:53 20036 dinov2 ssl_meta_arch.py:85] OPTIONS -- IBOT
I20240906 13:41:53 20036 dinov2 ssl_meta_arch.py:86] OPTIONS -- IBOT -- loss_weight: 1.0
I20240906 13:41:53 20036 dinov2 ssl_meta_arch.py:87] OPTIONS -- IBOT masking -- ibot_mask_ratio_tuple: [0.1, 0.5]
I20240906 13:41:53 20036 dinov2 ssl_meta_arch.py:88] OPTIONS -- IBOT masking -- ibot_mask_sample_probability: 0.5
I20240906 13:41:53 20036 dinov2 ssl_meta_arch.py:111] OPTIONS -- IBOT -- head shared with DINO
I20240906 13:41:53 20036 dinov2 ssl_meta_arch.py:121] Student and Teacher are built: they are both vit_large network.
I20240906 13:41:53 20036 dinov2 train.py:302] Model:
SSLMetaArch(
  (dino_loss): DINOLoss()
  (koleo_loss): KoLeoLoss(
    (pdist): PairwiseDistance()
  )
  (ibot_patch_loss): iBOTPatchLoss()
  (student): ModuleDict(
    (backbone): DinoVisionTransformer(
      (patch_embed): PatchEmbed(
        (proj): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))
        (norm): Identity()
      )
      (blocks): ModuleList(
        (0): BlockChunk(
          (0-5): 6 x NestedTensorBlock(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): DropPath()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): DropPath()
          )
        )
        (1): BlockChunk(
          (0-5): 6 x Identity()
          (6-11): 6 x NestedTensorBlock(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): DropPath()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): DropPath()
          )
        )
        (2): BlockChunk(
          (0-11): 12 x Identity()
          (12-17): 6 x NestedTensorBlock(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): DropPath()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): DropPath()
          )
        )
        (3): BlockChunk(
          (0-17): 18 x Identity()
          (18-23): 6 x NestedTensorBlock(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): DropPath()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): DropPath()
          )
        )
      )
      (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (head): Identity()
    )
    (dino_head): DINOHead(
      (mlp): Sequential(
        (0): Linear(in_features=1024, out_features=2048, bias=True)
        (1): GELU(approximate='none')
        (2): Linear(in_features=2048, out_features=2048, bias=True)
        (3): GELU(approximate='none')
        (4): Linear(in_features=2048, out_features=256, bias=True)
      )
      (last_layer): Linear(in_features=256, out_features=65536, bias=False)
    )
  )
  (teacher): ModuleDict(
    (backbone): DinoVisionTransformer(
      (patch_embed): PatchEmbed(
        (proj): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))
        (norm): Identity()
      )
      (blocks): ModuleList(
        (0): BlockChunk(
          (0-5): 6 x NestedTensorBlock(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): Identity()
          )
        )
        (1): BlockChunk(
          (0-5): 6 x Identity()
          (6-11): 6 x NestedTensorBlock(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): Identity()
          )
        )
        (2): BlockChunk(
          (0-11): 12 x Identity()
          (12-17): 6 x NestedTensorBlock(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): Identity()
          )
        )
        (3): BlockChunk(
          (0-17): 18 x Identity()
          (18-23): 6 x NestedTensorBlock(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): Identity()
          )
        )
      )
      (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (head): Identity()
    )
    (dino_head): DINOHead(
      (mlp): Sequential(
        (0): Linear(in_features=1024, out_features=2048, bias=True)
        (1): GELU(approximate='none')
        (2): Linear(in_features=2048, out_features=2048, bias=True)
        (3): GELU(approximate='none')
        (4): Linear(in_features=2048, out_features=256, bias=True)
      )
      (last_layer): Linear(in_features=256, out_features=65536, bias=False)
    )
  )
)
I20240906 13:41:53 20036 dinov2 param_groups.py:54] chunked fsdp
I20240906 13:41:53 20036 dinov2 param_groups.py:87] cls_token: lr_multiplier: 0.0717897987691853, wd_multiplier: 1.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] pos_embed: lr_multiplier: 0.0717897987691853, wd_multiplier: 1.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] mask_token: lr_multiplier: 0.0717897987691853, wd_multiplier: 1.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] patch_embed.proj.weight: lr_multiplier: 0.01435795975383706, wd_multiplier: 1.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] patch_embed.proj.bias: lr_multiplier: 0.01435795975383706, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.0.0.norm1.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.0.0.norm1.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.0.0.attn.qkv.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 1.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.0.0.attn.qkv.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.0.0.attn.proj.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 1.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.0.0.attn.proj.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.0.0.ls1.gamma: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.0.0.norm2.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.0.0.norm2.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.0.0.mlp.fc1.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 1.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.0.0.mlp.fc1.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.0.0.mlp.fc2.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 1.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.0.0.mlp.fc2.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.0.0.ls2.gamma: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.0.1.norm1.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.0.1.norm1.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.0.1.attn.qkv.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 1.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.0.1.attn.qkv.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.0.1.attn.proj.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 1.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.0.1.attn.proj.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.0.1.ls1.gamma: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.0.1.norm2.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.0.1.norm2.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.0.1.mlp.fc1.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 1.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.0.1.mlp.fc1.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.0.1.mlp.fc2.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 1.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.0.1.mlp.fc2.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.0.1.ls2.gamma: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.0.2.norm1.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.0.2.norm1.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.0.2.attn.qkv.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 1.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.0.2.attn.qkv.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.0.2.attn.proj.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 1.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.0.2.attn.proj.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.0.2.ls1.gamma: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.0.2.norm2.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.0.2.norm2.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.0.2.mlp.fc1.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 1.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.0.2.mlp.fc1.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.0.2.mlp.fc2.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 1.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.0.2.mlp.fc2.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.0.2.ls2.gamma: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.0.3.norm1.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.0.3.norm1.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.0.3.attn.qkv.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 1.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.0.3.attn.qkv.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.0.3.attn.proj.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 1.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.0.3.attn.proj.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.0.3.ls1.gamma: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.0.3.norm2.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.0.3.norm2.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.0.3.mlp.fc1.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 1.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.0.3.mlp.fc1.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.0.3.mlp.fc2.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 1.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.0.3.mlp.fc2.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.0.3.ls2.gamma: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.0.4.norm1.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.0.4.norm1.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.0.4.attn.qkv.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 1.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.0.4.attn.qkv.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.0.4.attn.proj.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 1.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.0.4.attn.proj.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.0.4.ls1.gamma: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.0.4.norm2.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.0.4.norm2.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.0.4.mlp.fc1.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 1.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.0.4.mlp.fc1.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.0.4.mlp.fc2.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 1.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.0.4.mlp.fc2.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.0.4.ls2.gamma: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.0.5.norm1.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.0.5.norm1.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.0.5.attn.qkv.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 1.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.0.5.attn.qkv.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.0.5.attn.proj.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 1.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.0.5.attn.proj.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.0.5.ls1.gamma: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.0.5.norm2.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.0.5.norm2.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.0.5.mlp.fc1.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 1.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.0.5.mlp.fc1.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.0.5.mlp.fc2.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 1.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.0.5.mlp.fc2.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.0.5.ls2.gamma: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.1.6.norm1.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.1.6.norm1.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.1.6.attn.qkv.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 1.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.1.6.attn.qkv.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.1.6.attn.proj.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 1.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.1.6.attn.proj.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.1.6.ls1.gamma: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.1.6.norm2.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.1.6.norm2.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.1.6.mlp.fc1.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 1.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.1.6.mlp.fc1.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.1.6.mlp.fc2.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 1.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.1.6.mlp.fc2.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.1.6.ls2.gamma: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.1.7.norm1.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.1.7.norm1.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.1.7.attn.qkv.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 1.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.1.7.attn.qkv.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.1.7.attn.proj.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 1.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.1.7.attn.proj.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.1.7.ls1.gamma: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.1.7.norm2.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.1.7.norm2.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.1.7.mlp.fc1.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 1.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.1.7.mlp.fc1.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.1.7.mlp.fc2.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 1.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.1.7.mlp.fc2.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.1.7.ls2.gamma: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.1.8.norm1.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.1.8.norm1.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.1.8.attn.qkv.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 1.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.1.8.attn.qkv.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.1.8.attn.proj.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 1.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.1.8.attn.proj.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.1.8.ls1.gamma: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.1.8.norm2.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.1.8.norm2.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.1.8.mlp.fc1.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 1.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.1.8.mlp.fc1.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.1.8.mlp.fc2.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 1.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.1.8.mlp.fc2.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.1.8.ls2.gamma: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.1.9.norm1.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.1.9.norm1.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.1.9.attn.qkv.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 1.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.1.9.attn.qkv.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.1.9.attn.proj.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 1.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.1.9.attn.proj.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.1.9.ls1.gamma: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.1.9.norm2.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.1.9.norm2.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.1.9.mlp.fc1.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 1.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.1.9.mlp.fc1.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.1.9.mlp.fc2.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 1.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.1.9.mlp.fc2.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.1.9.ls2.gamma: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.1.10.norm1.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.1.10.norm1.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.1.10.attn.qkv.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 1.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.1.10.attn.qkv.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.1.10.attn.proj.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 1.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.1.10.attn.proj.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.1.10.ls1.gamma: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.1.10.norm2.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.1.10.norm2.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.1.10.mlp.fc1.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 1.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.1.10.mlp.fc1.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.1.10.mlp.fc2.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 1.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.1.10.mlp.fc2.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.1.10.ls2.gamma: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.1.11.norm1.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.1.11.norm1.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.1.11.attn.qkv.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 1.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.1.11.attn.qkv.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.1.11.attn.proj.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 1.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.1.11.attn.proj.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.1.11.ls1.gamma: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.1.11.norm2.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.1.11.norm2.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.1.11.mlp.fc1.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 1.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.1.11.mlp.fc1.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.1.11.mlp.fc2.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 1.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.1.11.mlp.fc2.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.1.11.ls2.gamma: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.2.12.norm1.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.2.12.norm1.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.2.12.attn.qkv.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 1.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.2.12.attn.qkv.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.2.12.attn.proj.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 1.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.2.12.attn.proj.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.2.12.ls1.gamma: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.2.12.norm2.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.2.12.norm2.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.2.12.mlp.fc1.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 1.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.2.12.mlp.fc1.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.2.12.mlp.fc2.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 1.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.2.12.mlp.fc2.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.2.12.ls2.gamma: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.2.13.norm1.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.2.13.norm1.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.2.13.attn.qkv.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 1.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.2.13.attn.qkv.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.2.13.attn.proj.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 1.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.2.13.attn.proj.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.2.13.ls1.gamma: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.2.13.norm2.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.2.13.norm2.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.2.13.mlp.fc1.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 1.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.2.13.mlp.fc1.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.2.13.mlp.fc2.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 1.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.2.13.mlp.fc2.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.2.13.ls2.gamma: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.2.14.norm1.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.2.14.norm1.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.2.14.attn.qkv.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 1.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.2.14.attn.qkv.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.2.14.attn.proj.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 1.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.2.14.attn.proj.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.2.14.ls1.gamma: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.2.14.norm2.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.2.14.norm2.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.2.14.mlp.fc1.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 1.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.2.14.mlp.fc1.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.2.14.mlp.fc2.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 1.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.2.14.mlp.fc2.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.2.14.ls2.gamma: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.2.15.norm1.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.2.15.norm1.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.2.15.attn.qkv.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 1.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.2.15.attn.qkv.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.2.15.attn.proj.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 1.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.2.15.attn.proj.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.2.15.ls1.gamma: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.2.15.norm2.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.2.15.norm2.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.2.15.mlp.fc1.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 1.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.2.15.mlp.fc1.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.2.15.mlp.fc2.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 1.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.2.15.mlp.fc2.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.2.15.ls2.gamma: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.2.16.norm1.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.2.16.norm1.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.2.16.attn.qkv.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 1.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.2.16.attn.qkv.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.2.16.attn.proj.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 1.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.2.16.attn.proj.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.2.16.ls1.gamma: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.2.16.norm2.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.2.16.norm2.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.2.16.mlp.fc1.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 1.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.2.16.mlp.fc1.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.2.16.mlp.fc2.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 1.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.2.16.mlp.fc2.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.2.16.ls2.gamma: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.2.17.norm1.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.2.17.norm1.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.2.17.attn.qkv.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 1.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.2.17.attn.qkv.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.2.17.attn.proj.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 1.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.2.17.attn.proj.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.2.17.ls1.gamma: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.2.17.norm2.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.2.17.norm2.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.2.17.mlp.fc1.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 1.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.2.17.mlp.fc1.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.2.17.mlp.fc2.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 1.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.2.17.mlp.fc2.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.2.17.ls2.gamma: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.3.18.norm1.weight: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.3.18.norm1.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.3.18.attn.qkv.weight: lr_multiplier: 0.531441, wd_multiplier: 1.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.3.18.attn.qkv.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.3.18.attn.proj.weight: lr_multiplier: 0.531441, wd_multiplier: 1.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.3.18.attn.proj.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.3.18.ls1.gamma: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.3.18.norm2.weight: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.3.18.norm2.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.3.18.mlp.fc1.weight: lr_multiplier: 0.531441, wd_multiplier: 1.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.3.18.mlp.fc1.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.3.18.mlp.fc2.weight: lr_multiplier: 0.531441, wd_multiplier: 1.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.3.18.mlp.fc2.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.3.18.ls2.gamma: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.3.19.norm1.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.3.19.norm1.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.3.19.attn.qkv.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 1.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.3.19.attn.qkv.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.3.19.attn.proj.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 1.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.3.19.attn.proj.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.3.19.ls1.gamma: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.3.19.norm2.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.3.19.norm2.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.3.19.mlp.fc1.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 1.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.3.19.mlp.fc1.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.3.19.mlp.fc2.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 1.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.3.19.mlp.fc2.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.3.19.ls2.gamma: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.3.20.norm1.weight: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.3.20.norm1.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.3.20.attn.qkv.weight: lr_multiplier: 0.6561, wd_multiplier: 1.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.3.20.attn.qkv.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.3.20.attn.proj.weight: lr_multiplier: 0.6561, wd_multiplier: 1.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.3.20.attn.proj.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.3.20.ls1.gamma: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.3.20.norm2.weight: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.3.20.norm2.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.3.20.mlp.fc1.weight: lr_multiplier: 0.6561, wd_multiplier: 1.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.3.20.mlp.fc1.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.3.20.mlp.fc2.weight: lr_multiplier: 0.6561, wd_multiplier: 1.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.3.20.mlp.fc2.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.3.20.ls2.gamma: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.3.21.norm1.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.3.21.norm1.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.3.21.attn.qkv.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 1.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.3.21.attn.qkv.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.3.21.attn.proj.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 1.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.3.21.attn.proj.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.3.21.ls1.gamma: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.3.21.norm2.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.3.21.norm2.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.3.21.mlp.fc1.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 1.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.3.21.mlp.fc1.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.3.21.mlp.fc2.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 1.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.3.21.mlp.fc2.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.3.21.ls2.gamma: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.3.22.norm1.weight: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.3.22.norm1.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.3.22.attn.qkv.weight: lr_multiplier: 0.81, wd_multiplier: 1.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.3.22.attn.qkv.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.3.22.attn.proj.weight: lr_multiplier: 0.81, wd_multiplier: 1.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.3.22.attn.proj.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.3.22.ls1.gamma: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.3.22.norm2.weight: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.3.22.norm2.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.3.22.mlp.fc1.weight: lr_multiplier: 0.81, wd_multiplier: 1.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.3.22.mlp.fc1.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.3.22.mlp.fc2.weight: lr_multiplier: 0.81, wd_multiplier: 1.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.3.22.mlp.fc2.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.3.22.ls2.gamma: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.3.23.norm1.weight: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.3.23.norm1.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.3.23.attn.qkv.weight: lr_multiplier: 0.9, wd_multiplier: 1.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.3.23.attn.qkv.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.3.23.attn.proj.weight: lr_multiplier: 0.9, wd_multiplier: 1.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.3.23.attn.proj.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.3.23.ls1.gamma: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.3.23.norm2.weight: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.3.23.norm2.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.3.23.mlp.fc1.weight: lr_multiplier: 0.9, wd_multiplier: 1.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.3.23.mlp.fc1.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.3.23.mlp.fc2.weight: lr_multiplier: 0.9, wd_multiplier: 1.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.3.23.mlp.fc2.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] blocks.3.23.ls2.gamma: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] norm.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] norm.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 ssl_meta_arch.py:378] fusing param groups
I20240906 13:41:53 20036 dinov2 param_groups.py:64] else code branch
I20240906 13:41:53 20036 dinov2 param_groups.py:87] mlp.0.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] mlp.0.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] mlp.2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] mlp.2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] mlp.4.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] mlp.4.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] last_layer.weight_g: lr_multiplier: 1.0, wd_multiplier: 1.0
I20240906 13:41:53 20036 dinov2 param_groups.py:87] last_layer.weight_v: lr_multiplier: 1.0, wd_multiplier: 1.0
I20240906 13:41:53 20036 dinov2 ssl_meta_arch.py:378] fusing param groups
I20240906 13:41:53 20036 dinov2 train.py:98] Schedulers ready.
I20240906 13:41:53 20036 dinov2 augmentations.py:34] ###################################
I20240906 13:41:53 20036 dinov2 augmentations.py:35] Using data augmentation parameters:
I20240906 13:41:53 20036 dinov2 augmentations.py:36] global_crops_scale: [0.32, 1.0]
I20240906 13:41:53 20036 dinov2 augmentations.py:37] local_crops_scale: [0.05, 0.32]
I20240906 13:41:53 20036 dinov2 augmentations.py:38] local_crops_number: 8
I20240906 13:41:53 20036 dinov2 augmentations.py:39] global_crops_size: 224
I20240906 13:41:53 20036 dinov2 augmentations.py:40] local_crops_size: 96
I20240906 13:41:53 20036 dinov2 augmentations.py:41] ###################################
I20240906 13:41:53 20036 dinov2 loaders.py:89] using dataset: "ImageNet:split=TRAIN:root=C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\datasets\ILSVRC\Data\CLS-LOC:extra=C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\datasets\ILSVRC\Data\CLS-LOC"
I20240906 13:41:53 20036 dinov2 loaders.py:94] # of dataset samples: 1,281,167
I20240906 13:41:53 20036 dinov2 loaders.py:117] sampler: infinite
I20240906 13:41:53 20036 dinov2 loaders.py:211] using PyTorch data loader
I20240906 13:41:53 20036 dinov2 loaders.py:226] infinite data loader
I20240906 13:41:53 20036 dinov2 train.py:216] Starting training from iteration 0
W20240906 13:41:58 20036 xformers __init__.py:50] A matching Triton is not available, some optimizations will not be enabled.
Error caught was: No module named 'triton'
I20240906 13:42:41 6528 dinov2 config.py:59] git:
  sha: d43546fae6805ceb22618af8cf78469b61d913c7, status: has uncommitted changes, branch: main

I20240906 13:42:41 6528 dinov2 config.py:60] config_file: configs/train/vitl16_short.yaml
eval: 
eval_only: False
no_resume: False
opts: ['train.dataset_path=ImageNet:split=TRAIN:root=C:\\Users\\ISI_UTS\\Siladittya\\iclr2023\\inexpts\\datasets\\ILSVRC\\Data\\CLS-LOC:extra=C:\\Users\\ISI_UTS\\Siladittya\\iclr2023\\inexpts\\datasets\\ILSVRC\\Data\\CLS-LOC', 'train.output_dir=C:\\Users\\ISI_UTS\\Siladittya\\iclr2023\\inexpts\\dinov2_nosetup\\dinov2\\dinov2_nosetup\\dinov2']
output_dir: C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\dinov2_nosetup\dinov2\dinov2_nosetup\dinov2
I20240906 13:42:41 6528 dinov2 config.py:26] sqrt scaling learning rate; base: 0.004, new: 0.001
I20240906 13:42:41 6528 dinov2 config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
dino:
  loss_weight: 1.0
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
  koleo_loss_weight: 0.1
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  separate_head: false
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
train:
  batch_size_per_gpu: 64
  dataset_path: ImageNet:split=TRAIN:root=C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\datasets\ILSVRC\Data\CLS-LOC:extra=C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\datasets\ILSVRC\Data\CLS-LOC
  output_dir: C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\dinov2_nosetup\dinov2\dinov2_nosetup\dinov2
  saveckp_freq: 20
  seed: 0
  num_workers: 0
  OFFICIAL_EPOCH_LENGTH: 1250
  cache_dataset: true
  centering: centering
student:
  arch: vit_large
  patch_size: 16
  drop_path_rate: 0.3
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: mlp
  block_chunks: 4
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
  num_register_tokens: 0
  interpolate_antialias: false
  interpolate_offset: 0.1
teacher:
  momentum_teacher: 0.992
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 30
optim:
  epochs: 100
  weight_decay: 0.04
  weight_decay_end: 0.4
  base_lr: 0.004
  lr: 0.001
  warmup_epochs: 10
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 0.9
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 8
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 224
  local_crops_size: 96
evaluation:
  eval_period_iterations: 12500

I20240906 13:42:41 6528 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20240906 13:42:43 6528 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20240906 13:42:46 6528 dinov2 ssl_meta_arch.py:43] OPTIONS -- architecture : embed_dim: 1024
I20240906 13:42:46 6528 dinov2 ssl_meta_arch.py:58] OPTIONS -- DINO
I20240906 13:42:46 6528 dinov2 ssl_meta_arch.py:60] OPTIONS -- DINO -- loss_weight: 1.0
I20240906 13:42:46 6528 dinov2 ssl_meta_arch.py:61] OPTIONS -- DINO -- head_n_prototypes: 65536
I20240906 13:42:46 6528 dinov2 ssl_meta_arch.py:62] OPTIONS -- DINO -- head_bottleneck_dim: 256
I20240906 13:42:46 6528 dinov2 ssl_meta_arch.py:63] OPTIONS -- DINO -- head_hidden_dim: 2048
I20240906 13:42:46 6528 dinov2 ssl_meta_arch.py:75] OPTIONS -- DINO -- applying KOLEO regularization
I20240906 13:42:46 6528 dinov2 ssl_meta_arch.py:85] OPTIONS -- IBOT
I20240906 13:42:46 6528 dinov2 ssl_meta_arch.py:86] OPTIONS -- IBOT -- loss_weight: 1.0
I20240906 13:42:46 6528 dinov2 ssl_meta_arch.py:87] OPTIONS -- IBOT masking -- ibot_mask_ratio_tuple: [0.1, 0.5]
I20240906 13:42:46 6528 dinov2 ssl_meta_arch.py:88] OPTIONS -- IBOT masking -- ibot_mask_sample_probability: 0.5
I20240906 13:42:46 6528 dinov2 ssl_meta_arch.py:111] OPTIONS -- IBOT -- head shared with DINO
I20240906 13:42:46 6528 dinov2 ssl_meta_arch.py:121] Student and Teacher are built: they are both vit_large network.
I20240906 13:42:47 6528 dinov2 train.py:302] Model:
SSLMetaArch(
  (dino_loss): DINOLoss()
  (koleo_loss): KoLeoLoss(
    (pdist): PairwiseDistance()
  )
  (ibot_patch_loss): iBOTPatchLoss()
  (student): ModuleDict(
    (backbone): DinoVisionTransformer(
      (patch_embed): PatchEmbed(
        (proj): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))
        (norm): Identity()
      )
      (blocks): ModuleList(
        (0): BlockChunk(
          (0-5): 6 x NestedTensorBlock(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): DropPath()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): DropPath()
          )
        )
        (1): BlockChunk(
          (0-5): 6 x Identity()
          (6-11): 6 x NestedTensorBlock(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): DropPath()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): DropPath()
          )
        )
        (2): BlockChunk(
          (0-11): 12 x Identity()
          (12-17): 6 x NestedTensorBlock(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): DropPath()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): DropPath()
          )
        )
        (3): BlockChunk(
          (0-17): 18 x Identity()
          (18-23): 6 x NestedTensorBlock(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): DropPath()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): DropPath()
          )
        )
      )
      (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (head): Identity()
    )
    (dino_head): DINOHead(
      (mlp): Sequential(
        (0): Linear(in_features=1024, out_features=2048, bias=True)
        (1): GELU(approximate='none')
        (2): Linear(in_features=2048, out_features=2048, bias=True)
        (3): GELU(approximate='none')
        (4): Linear(in_features=2048, out_features=256, bias=True)
      )
      (last_layer): Linear(in_features=256, out_features=65536, bias=False)
    )
  )
  (teacher): ModuleDict(
    (backbone): DinoVisionTransformer(
      (patch_embed): PatchEmbed(
        (proj): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))
        (norm): Identity()
      )
      (blocks): ModuleList(
        (0): BlockChunk(
          (0-5): 6 x NestedTensorBlock(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): Identity()
          )
        )
        (1): BlockChunk(
          (0-5): 6 x Identity()
          (6-11): 6 x NestedTensorBlock(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): Identity()
          )
        )
        (2): BlockChunk(
          (0-11): 12 x Identity()
          (12-17): 6 x NestedTensorBlock(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): Identity()
          )
        )
        (3): BlockChunk(
          (0-17): 18 x Identity()
          (18-23): 6 x NestedTensorBlock(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): Identity()
          )
        )
      )
      (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (head): Identity()
    )
    (dino_head): DINOHead(
      (mlp): Sequential(
        (0): Linear(in_features=1024, out_features=2048, bias=True)
        (1): GELU(approximate='none')
        (2): Linear(in_features=2048, out_features=2048, bias=True)
        (3): GELU(approximate='none')
        (4): Linear(in_features=2048, out_features=256, bias=True)
      )
      (last_layer): Linear(in_features=256, out_features=65536, bias=False)
    )
  )
)
I20240906 13:42:47 6528 dinov2 param_groups.py:54] chunked fsdp
I20240906 13:42:47 6528 dinov2 param_groups.py:87] cls_token: lr_multiplier: 0.0717897987691853, wd_multiplier: 1.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] pos_embed: lr_multiplier: 0.0717897987691853, wd_multiplier: 1.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] mask_token: lr_multiplier: 0.0717897987691853, wd_multiplier: 1.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] patch_embed.proj.weight: lr_multiplier: 0.01435795975383706, wd_multiplier: 1.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] patch_embed.proj.bias: lr_multiplier: 0.01435795975383706, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.0.0.norm1.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.0.0.norm1.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.0.0.attn.qkv.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 1.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.0.0.attn.qkv.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.0.0.attn.proj.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 1.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.0.0.attn.proj.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.0.0.ls1.gamma: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.0.0.norm2.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.0.0.norm2.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.0.0.mlp.fc1.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 1.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.0.0.mlp.fc1.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.0.0.mlp.fc2.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 1.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.0.0.mlp.fc2.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.0.0.ls2.gamma: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.0.1.norm1.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.0.1.norm1.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.0.1.attn.qkv.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 1.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.0.1.attn.qkv.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.0.1.attn.proj.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 1.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.0.1.attn.proj.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.0.1.ls1.gamma: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.0.1.norm2.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.0.1.norm2.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.0.1.mlp.fc1.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 1.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.0.1.mlp.fc1.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.0.1.mlp.fc2.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 1.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.0.1.mlp.fc2.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.0.1.ls2.gamma: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.0.2.norm1.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.0.2.norm1.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.0.2.attn.qkv.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 1.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.0.2.attn.qkv.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.0.2.attn.proj.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 1.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.0.2.attn.proj.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.0.2.ls1.gamma: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.0.2.norm2.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.0.2.norm2.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.0.2.mlp.fc1.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 1.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.0.2.mlp.fc1.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.0.2.mlp.fc2.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 1.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.0.2.mlp.fc2.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.0.2.ls2.gamma: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.0.3.norm1.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.0.3.norm1.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.0.3.attn.qkv.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 1.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.0.3.attn.qkv.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.0.3.attn.proj.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 1.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.0.3.attn.proj.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.0.3.ls1.gamma: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.0.3.norm2.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.0.3.norm2.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.0.3.mlp.fc1.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 1.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.0.3.mlp.fc1.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.0.3.mlp.fc2.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 1.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.0.3.mlp.fc2.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.0.3.ls2.gamma: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.0.4.norm1.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.0.4.norm1.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.0.4.attn.qkv.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 1.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.0.4.attn.qkv.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.0.4.attn.proj.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 1.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.0.4.attn.proj.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.0.4.ls1.gamma: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.0.4.norm2.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.0.4.norm2.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.0.4.mlp.fc1.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 1.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.0.4.mlp.fc1.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.0.4.mlp.fc2.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 1.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.0.4.mlp.fc2.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.0.4.ls2.gamma: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.0.5.norm1.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.0.5.norm1.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.0.5.attn.qkv.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 1.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.0.5.attn.qkv.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.0.5.attn.proj.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 1.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.0.5.attn.proj.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.0.5.ls1.gamma: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.0.5.norm2.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.0.5.norm2.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.0.5.mlp.fc1.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 1.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.0.5.mlp.fc1.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.0.5.mlp.fc2.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 1.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.0.5.mlp.fc2.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.0.5.ls2.gamma: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.1.6.norm1.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.1.6.norm1.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.1.6.attn.qkv.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 1.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.1.6.attn.qkv.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.1.6.attn.proj.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 1.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.1.6.attn.proj.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.1.6.ls1.gamma: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.1.6.norm2.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.1.6.norm2.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.1.6.mlp.fc1.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 1.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.1.6.mlp.fc1.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.1.6.mlp.fc2.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 1.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.1.6.mlp.fc2.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.1.6.ls2.gamma: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.1.7.norm1.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.1.7.norm1.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.1.7.attn.qkv.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 1.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.1.7.attn.qkv.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.1.7.attn.proj.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 1.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.1.7.attn.proj.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.1.7.ls1.gamma: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.1.7.norm2.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.1.7.norm2.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.1.7.mlp.fc1.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 1.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.1.7.mlp.fc1.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.1.7.mlp.fc2.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 1.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.1.7.mlp.fc2.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.1.7.ls2.gamma: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.1.8.norm1.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.1.8.norm1.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.1.8.attn.qkv.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 1.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.1.8.attn.qkv.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.1.8.attn.proj.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 1.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.1.8.attn.proj.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.1.8.ls1.gamma: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.1.8.norm2.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.1.8.norm2.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.1.8.mlp.fc1.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 1.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.1.8.mlp.fc1.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.1.8.mlp.fc2.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 1.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.1.8.mlp.fc2.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.1.8.ls2.gamma: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.1.9.norm1.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.1.9.norm1.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.1.9.attn.qkv.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 1.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.1.9.attn.qkv.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.1.9.attn.proj.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 1.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.1.9.attn.proj.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.1.9.ls1.gamma: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.1.9.norm2.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.1.9.norm2.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.1.9.mlp.fc1.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 1.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.1.9.mlp.fc1.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.1.9.mlp.fc2.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 1.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.1.9.mlp.fc2.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.1.9.ls2.gamma: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.1.10.norm1.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.1.10.norm1.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.1.10.attn.qkv.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 1.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.1.10.attn.qkv.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.1.10.attn.proj.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 1.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.1.10.attn.proj.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.1.10.ls1.gamma: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.1.10.norm2.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.1.10.norm2.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.1.10.mlp.fc1.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 1.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.1.10.mlp.fc1.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.1.10.mlp.fc2.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 1.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.1.10.mlp.fc2.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.1.10.ls2.gamma: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.1.11.norm1.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.1.11.norm1.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.1.11.attn.qkv.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 1.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.1.11.attn.qkv.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.1.11.attn.proj.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 1.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.1.11.attn.proj.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.1.11.ls1.gamma: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.1.11.norm2.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.1.11.norm2.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.1.11.mlp.fc1.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 1.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.1.11.mlp.fc1.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.1.11.mlp.fc2.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 1.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.1.11.mlp.fc2.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.1.11.ls2.gamma: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.2.12.norm1.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.2.12.norm1.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.2.12.attn.qkv.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 1.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.2.12.attn.qkv.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.2.12.attn.proj.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 1.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.2.12.attn.proj.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.2.12.ls1.gamma: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.2.12.norm2.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.2.12.norm2.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.2.12.mlp.fc1.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 1.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.2.12.mlp.fc1.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.2.12.mlp.fc2.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 1.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.2.12.mlp.fc2.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.2.12.ls2.gamma: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.2.13.norm1.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.2.13.norm1.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.2.13.attn.qkv.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 1.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.2.13.attn.qkv.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.2.13.attn.proj.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 1.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.2.13.attn.proj.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.2.13.ls1.gamma: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.2.13.norm2.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.2.13.norm2.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.2.13.mlp.fc1.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 1.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.2.13.mlp.fc1.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.2.13.mlp.fc2.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 1.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.2.13.mlp.fc2.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.2.13.ls2.gamma: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.2.14.norm1.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.2.14.norm1.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.2.14.attn.qkv.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 1.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.2.14.attn.qkv.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.2.14.attn.proj.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 1.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.2.14.attn.proj.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.2.14.ls1.gamma: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.2.14.norm2.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.2.14.norm2.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.2.14.mlp.fc1.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 1.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.2.14.mlp.fc1.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.2.14.mlp.fc2.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 1.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.2.14.mlp.fc2.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.2.14.ls2.gamma: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.2.15.norm1.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.2.15.norm1.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.2.15.attn.qkv.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 1.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.2.15.attn.qkv.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.2.15.attn.proj.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 1.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.2.15.attn.proj.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.2.15.ls1.gamma: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.2.15.norm2.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.2.15.norm2.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.2.15.mlp.fc1.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 1.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.2.15.mlp.fc1.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.2.15.mlp.fc2.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 1.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.2.15.mlp.fc2.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.2.15.ls2.gamma: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.2.16.norm1.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.2.16.norm1.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.2.16.attn.qkv.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 1.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.2.16.attn.qkv.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.2.16.attn.proj.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 1.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.2.16.attn.proj.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.2.16.ls1.gamma: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.2.16.norm2.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.2.16.norm2.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.2.16.mlp.fc1.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 1.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.2.16.mlp.fc1.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.2.16.mlp.fc2.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 1.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.2.16.mlp.fc2.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.2.16.ls2.gamma: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.2.17.norm1.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.2.17.norm1.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.2.17.attn.qkv.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 1.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.2.17.attn.qkv.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.2.17.attn.proj.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 1.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.2.17.attn.proj.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.2.17.ls1.gamma: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.2.17.norm2.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.2.17.norm2.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.2.17.mlp.fc1.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 1.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.2.17.mlp.fc1.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.2.17.mlp.fc2.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 1.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.2.17.mlp.fc2.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.2.17.ls2.gamma: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.3.18.norm1.weight: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.3.18.norm1.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.3.18.attn.qkv.weight: lr_multiplier: 0.531441, wd_multiplier: 1.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.3.18.attn.qkv.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.3.18.attn.proj.weight: lr_multiplier: 0.531441, wd_multiplier: 1.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.3.18.attn.proj.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.3.18.ls1.gamma: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.3.18.norm2.weight: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.3.18.norm2.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.3.18.mlp.fc1.weight: lr_multiplier: 0.531441, wd_multiplier: 1.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.3.18.mlp.fc1.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.3.18.mlp.fc2.weight: lr_multiplier: 0.531441, wd_multiplier: 1.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.3.18.mlp.fc2.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.3.18.ls2.gamma: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.3.19.norm1.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.3.19.norm1.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.3.19.attn.qkv.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 1.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.3.19.attn.qkv.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.3.19.attn.proj.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 1.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.3.19.attn.proj.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.3.19.ls1.gamma: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.3.19.norm2.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.3.19.norm2.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.3.19.mlp.fc1.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 1.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.3.19.mlp.fc1.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.3.19.mlp.fc2.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 1.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.3.19.mlp.fc2.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.3.19.ls2.gamma: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.3.20.norm1.weight: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.3.20.norm1.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.3.20.attn.qkv.weight: lr_multiplier: 0.6561, wd_multiplier: 1.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.3.20.attn.qkv.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.3.20.attn.proj.weight: lr_multiplier: 0.6561, wd_multiplier: 1.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.3.20.attn.proj.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.3.20.ls1.gamma: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.3.20.norm2.weight: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.3.20.norm2.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.3.20.mlp.fc1.weight: lr_multiplier: 0.6561, wd_multiplier: 1.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.3.20.mlp.fc1.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.3.20.mlp.fc2.weight: lr_multiplier: 0.6561, wd_multiplier: 1.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.3.20.mlp.fc2.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.3.20.ls2.gamma: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.3.21.norm1.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.3.21.norm1.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.3.21.attn.qkv.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 1.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.3.21.attn.qkv.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.3.21.attn.proj.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 1.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.3.21.attn.proj.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.3.21.ls1.gamma: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.3.21.norm2.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.3.21.norm2.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.3.21.mlp.fc1.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 1.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.3.21.mlp.fc1.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.3.21.mlp.fc2.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 1.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.3.21.mlp.fc2.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.3.21.ls2.gamma: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.3.22.norm1.weight: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.3.22.norm1.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.3.22.attn.qkv.weight: lr_multiplier: 0.81, wd_multiplier: 1.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.3.22.attn.qkv.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.3.22.attn.proj.weight: lr_multiplier: 0.81, wd_multiplier: 1.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.3.22.attn.proj.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.3.22.ls1.gamma: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.3.22.norm2.weight: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.3.22.norm2.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.3.22.mlp.fc1.weight: lr_multiplier: 0.81, wd_multiplier: 1.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.3.22.mlp.fc1.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.3.22.mlp.fc2.weight: lr_multiplier: 0.81, wd_multiplier: 1.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.3.22.mlp.fc2.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.3.22.ls2.gamma: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.3.23.norm1.weight: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.3.23.norm1.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.3.23.attn.qkv.weight: lr_multiplier: 0.9, wd_multiplier: 1.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.3.23.attn.qkv.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.3.23.attn.proj.weight: lr_multiplier: 0.9, wd_multiplier: 1.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.3.23.attn.proj.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.3.23.ls1.gamma: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.3.23.norm2.weight: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.3.23.norm2.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.3.23.mlp.fc1.weight: lr_multiplier: 0.9, wd_multiplier: 1.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.3.23.mlp.fc1.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.3.23.mlp.fc2.weight: lr_multiplier: 0.9, wd_multiplier: 1.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.3.23.mlp.fc2.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] blocks.3.23.ls2.gamma: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] norm.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] norm.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 ssl_meta_arch.py:378] fusing param groups
I20240906 13:42:47 6528 dinov2 param_groups.py:64] else code branch
I20240906 13:42:47 6528 dinov2 param_groups.py:87] mlp.0.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] mlp.0.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] mlp.2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] mlp.2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] mlp.4.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] mlp.4.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] last_layer.weight_g: lr_multiplier: 1.0, wd_multiplier: 1.0
I20240906 13:42:47 6528 dinov2 param_groups.py:87] last_layer.weight_v: lr_multiplier: 1.0, wd_multiplier: 1.0
I20240906 13:42:47 6528 dinov2 ssl_meta_arch.py:378] fusing param groups
I20240906 13:42:47 6528 dinov2 train.py:98] Schedulers ready.
I20240906 13:42:47 6528 dinov2 augmentations.py:34] ###################################
I20240906 13:42:47 6528 dinov2 augmentations.py:35] Using data augmentation parameters:
I20240906 13:42:47 6528 dinov2 augmentations.py:36] global_crops_scale: [0.32, 1.0]
I20240906 13:42:47 6528 dinov2 augmentations.py:37] local_crops_scale: [0.05, 0.32]
I20240906 13:42:47 6528 dinov2 augmentations.py:38] local_crops_number: 8
I20240906 13:42:47 6528 dinov2 augmentations.py:39] global_crops_size: 224
I20240906 13:42:47 6528 dinov2 augmentations.py:40] local_crops_size: 96
I20240906 13:42:47 6528 dinov2 augmentations.py:41] ###################################
I20240906 13:42:47 6528 dinov2 loaders.py:89] using dataset: "ImageNet:split=TRAIN:root=C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\datasets\ILSVRC\Data\CLS-LOC:extra=C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\datasets\ILSVRC\Data\CLS-LOC"
I20240906 13:42:47 6528 dinov2 loaders.py:94] # of dataset samples: 1,281,167
I20240906 13:42:47 6528 dinov2 loaders.py:117] sampler: infinite
I20240906 13:42:47 6528 dinov2 loaders.py:211] using PyTorch data loader
I20240906 13:42:47 6528 dinov2 loaders.py:226] infinite data loader
I20240906 13:42:47 6528 dinov2 train.py:216] Starting training from iteration 0
W20240906 13:42:51 6528 xformers __init__.py:50] A matching Triton is not available, some optimizations will not be enabled.
Error caught was: No module named 'triton'
I20240906 13:43:19 14192 dinov2 config.py:59] git:
  sha: d43546fae6805ceb22618af8cf78469b61d913c7, status: has uncommitted changes, branch: main

I20240906 13:43:19 14192 dinov2 config.py:60] config_file: configs/train/vitl16_short.yaml
eval: 
eval_only: False
no_resume: False
opts: ['train.dataset_path=ImageNet:split=TRAIN:root=C:\\Users\\ISI_UTS\\Siladittya\\iclr2023\\inexpts\\datasets\\ILSVRC\\Data\\CLS-LOC:extra=C:\\Users\\ISI_UTS\\Siladittya\\iclr2023\\inexpts\\datasets\\ILSVRC\\Data\\CLS-LOC', 'train.output_dir=C:\\Users\\ISI_UTS\\Siladittya\\iclr2023\\inexpts\\dinov2_nosetup\\dinov2\\dinov2_nosetup\\dinov2']
output_dir: C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\dinov2_nosetup\dinov2\dinov2_nosetup\dinov2
I20240906 13:43:19 14192 dinov2 config.py:26] sqrt scaling learning rate; base: 0.004, new: 0.001
I20240906 13:43:19 14192 dinov2 config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
dino:
  loss_weight: 1.0
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
  koleo_loss_weight: 0.1
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  separate_head: false
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
train:
  batch_size_per_gpu: 64
  dataset_path: ImageNet:split=TRAIN:root=C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\datasets\ILSVRC\Data\CLS-LOC:extra=C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\datasets\ILSVRC\Data\CLS-LOC
  output_dir: C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\dinov2_nosetup\dinov2\dinov2_nosetup\dinov2
  saveckp_freq: 20
  seed: 0
  num_workers: 0
  OFFICIAL_EPOCH_LENGTH: 1250
  cache_dataset: true
  centering: centering
student:
  arch: vit_large
  patch_size: 16
  drop_path_rate: 0.3
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: mlp
  block_chunks: 4
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
  num_register_tokens: 0
  interpolate_antialias: false
  interpolate_offset: 0.1
teacher:
  momentum_teacher: 0.992
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 30
optim:
  epochs: 100
  weight_decay: 0.04
  weight_decay_end: 0.4
  base_lr: 0.004
  lr: 0.001
  warmup_epochs: 10
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 0.9
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 8
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 224
  local_crops_size: 96
evaluation:
  eval_period_iterations: 12500

I20240906 13:43:19 14192 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20240906 13:43:21 14192 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20240906 13:43:24 14192 dinov2 ssl_meta_arch.py:43] OPTIONS -- architecture : embed_dim: 1024
I20240906 13:43:24 14192 dinov2 ssl_meta_arch.py:58] OPTIONS -- DINO
I20240906 13:43:24 14192 dinov2 ssl_meta_arch.py:60] OPTIONS -- DINO -- loss_weight: 1.0
I20240906 13:43:24 14192 dinov2 ssl_meta_arch.py:61] OPTIONS -- DINO -- head_n_prototypes: 65536
I20240906 13:43:24 14192 dinov2 ssl_meta_arch.py:62] OPTIONS -- DINO -- head_bottleneck_dim: 256
I20240906 13:43:24 14192 dinov2 ssl_meta_arch.py:63] OPTIONS -- DINO -- head_hidden_dim: 2048
I20240906 13:43:24 14192 dinov2 ssl_meta_arch.py:75] OPTIONS -- DINO -- applying KOLEO regularization
I20240906 13:43:24 14192 dinov2 ssl_meta_arch.py:85] OPTIONS -- IBOT
I20240906 13:43:24 14192 dinov2 ssl_meta_arch.py:86] OPTIONS -- IBOT -- loss_weight: 1.0
I20240906 13:43:24 14192 dinov2 ssl_meta_arch.py:87] OPTIONS -- IBOT masking -- ibot_mask_ratio_tuple: [0.1, 0.5]
I20240906 13:43:24 14192 dinov2 ssl_meta_arch.py:88] OPTIONS -- IBOT masking -- ibot_mask_sample_probability: 0.5
I20240906 13:43:24 14192 dinov2 ssl_meta_arch.py:111] OPTIONS -- IBOT -- head shared with DINO
I20240906 13:43:24 14192 dinov2 ssl_meta_arch.py:121] Student and Teacher are built: they are both vit_large network.
I20240906 13:43:25 14192 dinov2 train.py:302] Model:
SSLMetaArch(
  (dino_loss): DINOLoss()
  (koleo_loss): KoLeoLoss(
    (pdist): PairwiseDistance()
  )
  (ibot_patch_loss): iBOTPatchLoss()
  (student): ModuleDict(
    (backbone): DinoVisionTransformer(
      (patch_embed): PatchEmbed(
        (proj): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))
        (norm): Identity()
      )
      (blocks): ModuleList(
        (0): BlockChunk(
          (0-5): 6 x NestedTensorBlock(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): DropPath()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): DropPath()
          )
        )
        (1): BlockChunk(
          (0-5): 6 x Identity()
          (6-11): 6 x NestedTensorBlock(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): DropPath()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): DropPath()
          )
        )
        (2): BlockChunk(
          (0-11): 12 x Identity()
          (12-17): 6 x NestedTensorBlock(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): DropPath()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): DropPath()
          )
        )
        (3): BlockChunk(
          (0-17): 18 x Identity()
          (18-23): 6 x NestedTensorBlock(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): DropPath()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): DropPath()
          )
        )
      )
      (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (head): Identity()
    )
    (dino_head): DINOHead(
      (mlp): Sequential(
        (0): Linear(in_features=1024, out_features=2048, bias=True)
        (1): GELU(approximate='none')
        (2): Linear(in_features=2048, out_features=2048, bias=True)
        (3): GELU(approximate='none')
        (4): Linear(in_features=2048, out_features=256, bias=True)
      )
      (last_layer): Linear(in_features=256, out_features=65536, bias=False)
    )
  )
  (teacher): ModuleDict(
    (backbone): DinoVisionTransformer(
      (patch_embed): PatchEmbed(
        (proj): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))
        (norm): Identity()
      )
      (blocks): ModuleList(
        (0): BlockChunk(
          (0-5): 6 x NestedTensorBlock(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): Identity()
          )
        )
        (1): BlockChunk(
          (0-5): 6 x Identity()
          (6-11): 6 x NestedTensorBlock(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): Identity()
          )
        )
        (2): BlockChunk(
          (0-11): 12 x Identity()
          (12-17): 6 x NestedTensorBlock(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): Identity()
          )
        )
        (3): BlockChunk(
          (0-17): 18 x Identity()
          (18-23): 6 x NestedTensorBlock(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): Identity()
          )
        )
      )
      (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (head): Identity()
    )
    (dino_head): DINOHead(
      (mlp): Sequential(
        (0): Linear(in_features=1024, out_features=2048, bias=True)
        (1): GELU(approximate='none')
        (2): Linear(in_features=2048, out_features=2048, bias=True)
        (3): GELU(approximate='none')
        (4): Linear(in_features=2048, out_features=256, bias=True)
      )
      (last_layer): Linear(in_features=256, out_features=65536, bias=False)
    )
  )
)
I20240906 13:43:25 14192 dinov2 param_groups.py:54] chunked fsdp
I20240906 13:43:25 14192 dinov2 param_groups.py:87] cls_token: lr_multiplier: 0.0717897987691853, wd_multiplier: 1.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] pos_embed: lr_multiplier: 0.0717897987691853, wd_multiplier: 1.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] mask_token: lr_multiplier: 0.0717897987691853, wd_multiplier: 1.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] patch_embed.proj.weight: lr_multiplier: 0.01435795975383706, wd_multiplier: 1.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] patch_embed.proj.bias: lr_multiplier: 0.01435795975383706, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.0.0.norm1.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.0.0.norm1.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.0.0.attn.qkv.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 1.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.0.0.attn.qkv.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.0.0.attn.proj.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 1.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.0.0.attn.proj.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.0.0.ls1.gamma: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.0.0.norm2.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.0.0.norm2.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.0.0.mlp.fc1.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 1.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.0.0.mlp.fc1.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.0.0.mlp.fc2.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 1.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.0.0.mlp.fc2.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.0.0.ls2.gamma: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.0.1.norm1.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.0.1.norm1.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.0.1.attn.qkv.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 1.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.0.1.attn.qkv.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.0.1.attn.proj.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 1.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.0.1.attn.proj.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.0.1.ls1.gamma: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.0.1.norm2.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.0.1.norm2.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.0.1.mlp.fc1.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 1.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.0.1.mlp.fc1.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.0.1.mlp.fc2.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 1.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.0.1.mlp.fc2.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.0.1.ls2.gamma: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.0.2.norm1.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.0.2.norm1.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.0.2.attn.qkv.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 1.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.0.2.attn.qkv.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.0.2.attn.proj.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 1.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.0.2.attn.proj.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.0.2.ls1.gamma: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.0.2.norm2.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.0.2.norm2.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.0.2.mlp.fc1.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 1.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.0.2.mlp.fc1.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.0.2.mlp.fc2.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 1.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.0.2.mlp.fc2.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.0.2.ls2.gamma: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.0.3.norm1.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.0.3.norm1.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.0.3.attn.qkv.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 1.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.0.3.attn.qkv.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.0.3.attn.proj.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 1.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.0.3.attn.proj.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.0.3.ls1.gamma: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.0.3.norm2.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.0.3.norm2.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.0.3.mlp.fc1.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 1.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.0.3.mlp.fc1.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.0.3.mlp.fc2.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 1.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.0.3.mlp.fc2.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.0.3.ls2.gamma: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.0.4.norm1.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.0.4.norm1.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.0.4.attn.qkv.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 1.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.0.4.attn.qkv.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.0.4.attn.proj.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 1.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.0.4.attn.proj.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.0.4.ls1.gamma: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.0.4.norm2.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.0.4.norm2.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.0.4.mlp.fc1.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 1.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.0.4.mlp.fc1.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.0.4.mlp.fc2.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 1.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.0.4.mlp.fc2.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.0.4.ls2.gamma: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.0.5.norm1.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.0.5.norm1.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.0.5.attn.qkv.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 1.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.0.5.attn.qkv.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.0.5.attn.proj.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 1.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.0.5.attn.proj.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.0.5.ls1.gamma: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.0.5.norm2.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.0.5.norm2.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.0.5.mlp.fc1.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 1.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.0.5.mlp.fc1.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.0.5.mlp.fc2.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 1.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.0.5.mlp.fc2.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.0.5.ls2.gamma: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.1.6.norm1.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.1.6.norm1.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.1.6.attn.qkv.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 1.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.1.6.attn.qkv.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.1.6.attn.proj.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 1.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.1.6.attn.proj.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.1.6.ls1.gamma: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.1.6.norm2.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.1.6.norm2.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.1.6.mlp.fc1.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 1.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.1.6.mlp.fc1.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.1.6.mlp.fc2.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 1.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.1.6.mlp.fc2.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.1.6.ls2.gamma: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.1.7.norm1.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.1.7.norm1.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.1.7.attn.qkv.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 1.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.1.7.attn.qkv.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.1.7.attn.proj.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 1.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.1.7.attn.proj.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.1.7.ls1.gamma: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.1.7.norm2.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.1.7.norm2.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.1.7.mlp.fc1.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 1.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.1.7.mlp.fc1.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.1.7.mlp.fc2.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 1.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.1.7.mlp.fc2.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.1.7.ls2.gamma: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.1.8.norm1.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.1.8.norm1.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.1.8.attn.qkv.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 1.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.1.8.attn.qkv.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.1.8.attn.proj.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 1.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.1.8.attn.proj.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.1.8.ls1.gamma: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.1.8.norm2.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.1.8.norm2.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.1.8.mlp.fc1.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 1.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.1.8.mlp.fc1.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.1.8.mlp.fc2.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 1.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.1.8.mlp.fc2.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.1.8.ls2.gamma: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.1.9.norm1.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.1.9.norm1.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.1.9.attn.qkv.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 1.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.1.9.attn.qkv.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.1.9.attn.proj.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 1.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.1.9.attn.proj.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.1.9.ls1.gamma: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.1.9.norm2.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.1.9.norm2.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.1.9.mlp.fc1.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 1.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.1.9.mlp.fc1.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.1.9.mlp.fc2.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 1.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.1.9.mlp.fc2.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.1.9.ls2.gamma: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.1.10.norm1.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.1.10.norm1.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.1.10.attn.qkv.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 1.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.1.10.attn.qkv.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.1.10.attn.proj.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 1.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.1.10.attn.proj.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.1.10.ls1.gamma: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.1.10.norm2.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.1.10.norm2.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.1.10.mlp.fc1.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 1.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.1.10.mlp.fc1.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.1.10.mlp.fc2.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 1.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.1.10.mlp.fc2.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.1.10.ls2.gamma: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.1.11.norm1.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.1.11.norm1.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.1.11.attn.qkv.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 1.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.1.11.attn.qkv.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.1.11.attn.proj.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 1.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.1.11.attn.proj.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.1.11.ls1.gamma: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.1.11.norm2.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.1.11.norm2.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.1.11.mlp.fc1.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 1.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.1.11.mlp.fc1.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.1.11.mlp.fc2.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 1.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.1.11.mlp.fc2.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.1.11.ls2.gamma: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.2.12.norm1.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.2.12.norm1.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.2.12.attn.qkv.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 1.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.2.12.attn.qkv.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.2.12.attn.proj.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 1.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.2.12.attn.proj.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.2.12.ls1.gamma: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.2.12.norm2.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.2.12.norm2.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.2.12.mlp.fc1.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 1.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.2.12.mlp.fc1.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.2.12.mlp.fc2.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 1.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.2.12.mlp.fc2.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.2.12.ls2.gamma: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.2.13.norm1.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.2.13.norm1.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.2.13.attn.qkv.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 1.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.2.13.attn.qkv.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.2.13.attn.proj.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 1.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.2.13.attn.proj.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.2.13.ls1.gamma: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.2.13.norm2.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.2.13.norm2.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.2.13.mlp.fc1.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 1.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.2.13.mlp.fc1.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.2.13.mlp.fc2.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 1.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.2.13.mlp.fc2.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.2.13.ls2.gamma: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.2.14.norm1.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.2.14.norm1.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.2.14.attn.qkv.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 1.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.2.14.attn.qkv.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.2.14.attn.proj.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 1.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.2.14.attn.proj.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.2.14.ls1.gamma: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.2.14.norm2.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.2.14.norm2.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.2.14.mlp.fc1.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 1.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.2.14.mlp.fc1.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.2.14.mlp.fc2.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 1.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.2.14.mlp.fc2.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.2.14.ls2.gamma: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.2.15.norm1.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.2.15.norm1.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.2.15.attn.qkv.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 1.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.2.15.attn.qkv.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.2.15.attn.proj.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 1.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.2.15.attn.proj.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.2.15.ls1.gamma: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.2.15.norm2.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.2.15.norm2.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.2.15.mlp.fc1.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 1.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.2.15.mlp.fc1.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.2.15.mlp.fc2.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 1.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.2.15.mlp.fc2.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.2.15.ls2.gamma: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.2.16.norm1.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.2.16.norm1.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.2.16.attn.qkv.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 1.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.2.16.attn.qkv.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.2.16.attn.proj.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 1.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.2.16.attn.proj.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.2.16.ls1.gamma: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.2.16.norm2.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.2.16.norm2.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.2.16.mlp.fc1.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 1.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.2.16.mlp.fc1.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.2.16.mlp.fc2.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 1.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.2.16.mlp.fc2.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.2.16.ls2.gamma: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.2.17.norm1.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.2.17.norm1.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.2.17.attn.qkv.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 1.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.2.17.attn.qkv.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.2.17.attn.proj.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 1.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.2.17.attn.proj.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.2.17.ls1.gamma: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.2.17.norm2.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.2.17.norm2.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.2.17.mlp.fc1.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 1.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.2.17.mlp.fc1.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.2.17.mlp.fc2.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 1.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.2.17.mlp.fc2.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.2.17.ls2.gamma: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.3.18.norm1.weight: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.3.18.norm1.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.3.18.attn.qkv.weight: lr_multiplier: 0.531441, wd_multiplier: 1.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.3.18.attn.qkv.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.3.18.attn.proj.weight: lr_multiplier: 0.531441, wd_multiplier: 1.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.3.18.attn.proj.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.3.18.ls1.gamma: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.3.18.norm2.weight: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.3.18.norm2.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.3.18.mlp.fc1.weight: lr_multiplier: 0.531441, wd_multiplier: 1.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.3.18.mlp.fc1.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.3.18.mlp.fc2.weight: lr_multiplier: 0.531441, wd_multiplier: 1.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.3.18.mlp.fc2.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.3.18.ls2.gamma: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.3.19.norm1.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.3.19.norm1.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.3.19.attn.qkv.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 1.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.3.19.attn.qkv.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.3.19.attn.proj.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 1.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.3.19.attn.proj.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.3.19.ls1.gamma: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.3.19.norm2.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.3.19.norm2.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.3.19.mlp.fc1.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 1.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.3.19.mlp.fc1.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.3.19.mlp.fc2.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 1.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.3.19.mlp.fc2.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.3.19.ls2.gamma: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.3.20.norm1.weight: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.3.20.norm1.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.3.20.attn.qkv.weight: lr_multiplier: 0.6561, wd_multiplier: 1.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.3.20.attn.qkv.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.3.20.attn.proj.weight: lr_multiplier: 0.6561, wd_multiplier: 1.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.3.20.attn.proj.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.3.20.ls1.gamma: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.3.20.norm2.weight: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.3.20.norm2.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.3.20.mlp.fc1.weight: lr_multiplier: 0.6561, wd_multiplier: 1.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.3.20.mlp.fc1.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.3.20.mlp.fc2.weight: lr_multiplier: 0.6561, wd_multiplier: 1.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.3.20.mlp.fc2.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.3.20.ls2.gamma: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.3.21.norm1.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.3.21.norm1.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.3.21.attn.qkv.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 1.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.3.21.attn.qkv.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.3.21.attn.proj.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 1.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.3.21.attn.proj.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.3.21.ls1.gamma: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.3.21.norm2.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.3.21.norm2.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.3.21.mlp.fc1.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 1.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.3.21.mlp.fc1.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.3.21.mlp.fc2.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 1.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.3.21.mlp.fc2.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.3.21.ls2.gamma: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.3.22.norm1.weight: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.3.22.norm1.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.3.22.attn.qkv.weight: lr_multiplier: 0.81, wd_multiplier: 1.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.3.22.attn.qkv.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.3.22.attn.proj.weight: lr_multiplier: 0.81, wd_multiplier: 1.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.3.22.attn.proj.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.3.22.ls1.gamma: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.3.22.norm2.weight: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.3.22.norm2.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.3.22.mlp.fc1.weight: lr_multiplier: 0.81, wd_multiplier: 1.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.3.22.mlp.fc1.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.3.22.mlp.fc2.weight: lr_multiplier: 0.81, wd_multiplier: 1.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.3.22.mlp.fc2.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.3.22.ls2.gamma: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.3.23.norm1.weight: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.3.23.norm1.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.3.23.attn.qkv.weight: lr_multiplier: 0.9, wd_multiplier: 1.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.3.23.attn.qkv.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.3.23.attn.proj.weight: lr_multiplier: 0.9, wd_multiplier: 1.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.3.23.attn.proj.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.3.23.ls1.gamma: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.3.23.norm2.weight: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.3.23.norm2.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.3.23.mlp.fc1.weight: lr_multiplier: 0.9, wd_multiplier: 1.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.3.23.mlp.fc1.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.3.23.mlp.fc2.weight: lr_multiplier: 0.9, wd_multiplier: 1.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.3.23.mlp.fc2.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] blocks.3.23.ls2.gamma: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] norm.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] norm.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 ssl_meta_arch.py:378] fusing param groups
I20240906 13:43:25 14192 dinov2 param_groups.py:64] else code branch
I20240906 13:43:25 14192 dinov2 param_groups.py:87] mlp.0.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] mlp.0.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] mlp.2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] mlp.2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] mlp.4.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] mlp.4.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] last_layer.weight_g: lr_multiplier: 1.0, wd_multiplier: 1.0
I20240906 13:43:25 14192 dinov2 param_groups.py:87] last_layer.weight_v: lr_multiplier: 1.0, wd_multiplier: 1.0
I20240906 13:43:25 14192 dinov2 ssl_meta_arch.py:378] fusing param groups
I20240906 13:43:25 14192 dinov2 train.py:98] Schedulers ready.
I20240906 13:43:25 14192 dinov2 augmentations.py:34] ###################################
I20240906 13:43:25 14192 dinov2 augmentations.py:35] Using data augmentation parameters:
I20240906 13:43:25 14192 dinov2 augmentations.py:36] global_crops_scale: [0.32, 1.0]
I20240906 13:43:25 14192 dinov2 augmentations.py:37] local_crops_scale: [0.05, 0.32]
I20240906 13:43:25 14192 dinov2 augmentations.py:38] local_crops_number: 8
I20240906 13:43:25 14192 dinov2 augmentations.py:39] global_crops_size: 224
I20240906 13:43:25 14192 dinov2 augmentations.py:40] local_crops_size: 96
I20240906 13:43:25 14192 dinov2 augmentations.py:41] ###################################
I20240906 13:43:25 14192 dinov2 loaders.py:89] using dataset: "ImageNet:split=TRAIN:root=C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\datasets\ILSVRC\Data\CLS-LOC:extra=C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\datasets\ILSVRC\Data\CLS-LOC"
I20240906 13:43:25 14192 dinov2 loaders.py:94] # of dataset samples: 1,281,167
I20240906 13:43:25 14192 dinov2 loaders.py:117] sampler: infinite
I20240906 13:43:25 14192 dinov2 loaders.py:211] using PyTorch data loader
I20240906 13:43:25 14192 dinov2 loaders.py:226] infinite data loader
I20240906 13:43:25 14192 dinov2 train.py:216] Starting training from iteration 0
W20240906 13:43:29 14192 xformers __init__.py:50] A matching Triton is not available, some optimizations will not be enabled.
Error caught was: No module named 'triton'
I20240906 13:55:16 4100 dinov2 config.py:59] git:
  sha: d43546fae6805ceb22618af8cf78469b61d913c7, status: has uncommitted changes, branch: main

I20240906 13:55:16 4100 dinov2 config.py:60] config_file: configs/train/vitl16_short.yaml
eval: 
eval_only: False
no_resume: False
opts: ['train.dataset_path=ImageNet:split=TRAIN:root=C:\\Users\\ISI_UTS\\Siladittya\\iclr2023\\inexpts\\datasets\\ILSVRC\\Data\\CLS-LOC:extra=C:\\Users\\ISI_UTS\\Siladittya\\iclr2023\\inexpts\\datasets\\ILSVRC\\Data\\CLS-LOC', 'train.output_dir=C:\\Users\\ISI_UTS\\Siladittya\\iclr2023\\inexpts\\dinov2_nosetup\\dinov2\\dinov2_nosetup\\dinov2']
output_dir: C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\dinov2_nosetup\dinov2\dinov2_nosetup\dinov2
I20240906 13:55:16 4100 dinov2 config.py:26] sqrt scaling learning rate; base: 0.004, new: 0.001
I20240906 13:55:16 4100 dinov2 config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
dino:
  loss_weight: 1.0
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
  koleo_loss_weight: 0.1
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  separate_head: false
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
train:
  batch_size_per_gpu: 64
  dataset_path: ImageNet:split=TRAIN:root=C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\datasets\ILSVRC\Data\CLS-LOC:extra=C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\datasets\ILSVRC\Data\CLS-LOC
  output_dir: C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\dinov2_nosetup\dinov2\dinov2_nosetup\dinov2
  saveckp_freq: 20
  seed: 0
  num_workers: 0
  OFFICIAL_EPOCH_LENGTH: 1250
  cache_dataset: true
  centering: centering
student:
  arch: vit_large
  patch_size: 16
  drop_path_rate: 0.3
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: mlp
  block_chunks: 4
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
  num_register_tokens: 0
  interpolate_antialias: false
  interpolate_offset: 0.1
teacher:
  momentum_teacher: 0.992
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 30
optim:
  epochs: 100
  weight_decay: 0.04
  weight_decay_end: 0.4
  base_lr: 0.004
  lr: 0.001
  warmup_epochs: 10
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 0.9
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 8
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 224
  local_crops_size: 96
evaluation:
  eval_period_iterations: 12500

I20240906 13:55:16 4100 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20240906 13:55:18 4100 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20240906 13:55:21 4100 dinov2 ssl_meta_arch.py:45] OPTIONS -- architecture : embed_dim: 1024
I20240906 13:55:21 4100 dinov2 ssl_meta_arch.py:60] OPTIONS -- DINO
I20240906 13:55:21 4100 dinov2 ssl_meta_arch.py:62] OPTIONS -- DINO -- loss_weight: 1.0
I20240906 13:55:21 4100 dinov2 ssl_meta_arch.py:63] OPTIONS -- DINO -- head_n_prototypes: 65536
I20240906 13:55:21 4100 dinov2 ssl_meta_arch.py:64] OPTIONS -- DINO -- head_bottleneck_dim: 256
I20240906 13:55:21 4100 dinov2 ssl_meta_arch.py:65] OPTIONS -- DINO -- head_hidden_dim: 2048
I20240906 13:55:21 4100 dinov2 ssl_meta_arch.py:77] OPTIONS -- DINO -- applying KOLEO regularization
I20240906 13:55:21 4100 dinov2 ssl_meta_arch.py:87] OPTIONS -- IBOT
I20240906 13:55:21 4100 dinov2 ssl_meta_arch.py:88] OPTIONS -- IBOT -- loss_weight: 1.0
I20240906 13:55:21 4100 dinov2 ssl_meta_arch.py:89] OPTIONS -- IBOT masking -- ibot_mask_ratio_tuple: [0.1, 0.5]
I20240906 13:55:21 4100 dinov2 ssl_meta_arch.py:90] OPTIONS -- IBOT masking -- ibot_mask_sample_probability: 0.5
I20240906 13:55:21 4100 dinov2 ssl_meta_arch.py:113] OPTIONS -- IBOT -- head shared with DINO
I20240906 13:55:21 4100 dinov2 ssl_meta_arch.py:123] Student and Teacher are built: they are both vit_large network.
I20240906 13:55:22 4100 dinov2 train.py:302] Model:
SSLMetaArch(
  (dino_loss): DINOLoss()
  (koleo_loss): KoLeoLoss(
    (pdist): PairwiseDistance()
  )
  (ibot_patch_loss): iBOTPatchLoss()
  (student): ModuleDict(
    (backbone): DinoVisionTransformer(
      (patch_embed): PatchEmbed(
        (proj): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))
        (norm): Identity()
      )
      (blocks): ModuleList(
        (0): BlockChunk(
          (0-5): 6 x NestedTensorBlock(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): DropPath()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): DropPath()
          )
        )
        (1): BlockChunk(
          (0-5): 6 x Identity()
          (6-11): 6 x NestedTensorBlock(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): DropPath()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): DropPath()
          )
        )
        (2): BlockChunk(
          (0-11): 12 x Identity()
          (12-17): 6 x NestedTensorBlock(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): DropPath()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): DropPath()
          )
        )
        (3): BlockChunk(
          (0-17): 18 x Identity()
          (18-23): 6 x NestedTensorBlock(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): DropPath()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): DropPath()
          )
        )
      )
      (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (head): Identity()
    )
    (dino_head): DINOHead(
      (mlp): Sequential(
        (0): Linear(in_features=1024, out_features=2048, bias=True)
        (1): GELU(approximate='none')
        (2): Linear(in_features=2048, out_features=2048, bias=True)
        (3): GELU(approximate='none')
        (4): Linear(in_features=2048, out_features=256, bias=True)
      )
      (last_layer): Linear(in_features=256, out_features=65536, bias=False)
    )
  )
  (teacher): ModuleDict(
    (backbone): DinoVisionTransformer(
      (patch_embed): PatchEmbed(
        (proj): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))
        (norm): Identity()
      )
      (blocks): ModuleList(
        (0): BlockChunk(
          (0-5): 6 x NestedTensorBlock(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): Identity()
          )
        )
        (1): BlockChunk(
          (0-5): 6 x Identity()
          (6-11): 6 x NestedTensorBlock(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): Identity()
          )
        )
        (2): BlockChunk(
          (0-11): 12 x Identity()
          (12-17): 6 x NestedTensorBlock(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): Identity()
          )
        )
        (3): BlockChunk(
          (0-17): 18 x Identity()
          (18-23): 6 x NestedTensorBlock(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): Identity()
          )
        )
      )
      (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (head): Identity()
    )
    (dino_head): DINOHead(
      (mlp): Sequential(
        (0): Linear(in_features=1024, out_features=2048, bias=True)
        (1): GELU(approximate='none')
        (2): Linear(in_features=2048, out_features=2048, bias=True)
        (3): GELU(approximate='none')
        (4): Linear(in_features=2048, out_features=256, bias=True)
      )
      (last_layer): Linear(in_features=256, out_features=65536, bias=False)
    )
  )
)
I20240906 13:55:22 4100 dinov2 param_groups.py:54] chunked fsdp
I20240906 13:55:22 4100 dinov2 param_groups.py:87] cls_token: lr_multiplier: 0.0717897987691853, wd_multiplier: 1.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] pos_embed: lr_multiplier: 0.0717897987691853, wd_multiplier: 1.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] mask_token: lr_multiplier: 0.0717897987691853, wd_multiplier: 1.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] patch_embed.proj.weight: lr_multiplier: 0.01435795975383706, wd_multiplier: 1.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] patch_embed.proj.bias: lr_multiplier: 0.01435795975383706, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.0.0.norm1.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.0.0.norm1.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.0.0.attn.qkv.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 1.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.0.0.attn.qkv.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.0.0.attn.proj.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 1.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.0.0.attn.proj.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.0.0.ls1.gamma: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.0.0.norm2.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.0.0.norm2.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.0.0.mlp.fc1.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 1.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.0.0.mlp.fc1.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.0.0.mlp.fc2.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 1.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.0.0.mlp.fc2.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.0.0.ls2.gamma: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.0.1.norm1.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.0.1.norm1.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.0.1.attn.qkv.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 1.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.0.1.attn.qkv.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.0.1.attn.proj.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 1.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.0.1.attn.proj.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.0.1.ls1.gamma: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.0.1.norm2.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.0.1.norm2.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.0.1.mlp.fc1.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 1.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.0.1.mlp.fc1.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.0.1.mlp.fc2.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 1.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.0.1.mlp.fc2.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.0.1.ls2.gamma: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.0.2.norm1.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.0.2.norm1.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.0.2.attn.qkv.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 1.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.0.2.attn.qkv.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.0.2.attn.proj.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 1.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.0.2.attn.proj.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.0.2.ls1.gamma: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.0.2.norm2.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.0.2.norm2.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.0.2.mlp.fc1.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 1.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.0.2.mlp.fc1.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.0.2.mlp.fc2.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 1.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.0.2.mlp.fc2.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.0.2.ls2.gamma: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.0.3.norm1.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.0.3.norm1.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.0.3.attn.qkv.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 1.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.0.3.attn.qkv.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.0.3.attn.proj.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 1.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.0.3.attn.proj.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.0.3.ls1.gamma: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.0.3.norm2.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.0.3.norm2.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.0.3.mlp.fc1.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 1.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.0.3.mlp.fc1.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.0.3.mlp.fc2.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 1.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.0.3.mlp.fc2.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.0.3.ls2.gamma: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.0.4.norm1.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.0.4.norm1.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.0.4.attn.qkv.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 1.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.0.4.attn.qkv.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.0.4.attn.proj.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 1.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.0.4.attn.proj.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.0.4.ls1.gamma: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.0.4.norm2.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.0.4.norm2.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.0.4.mlp.fc1.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 1.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.0.4.mlp.fc1.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.0.4.mlp.fc2.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 1.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.0.4.mlp.fc2.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.0.4.ls2.gamma: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.0.5.norm1.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.0.5.norm1.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.0.5.attn.qkv.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 1.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.0.5.attn.qkv.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.0.5.attn.proj.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 1.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.0.5.attn.proj.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.0.5.ls1.gamma: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.0.5.norm2.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.0.5.norm2.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.0.5.mlp.fc1.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 1.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.0.5.mlp.fc1.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.0.5.mlp.fc2.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 1.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.0.5.mlp.fc2.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.0.5.ls2.gamma: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.1.6.norm1.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.1.6.norm1.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.1.6.attn.qkv.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 1.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.1.6.attn.qkv.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.1.6.attn.proj.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 1.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.1.6.attn.proj.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.1.6.ls1.gamma: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.1.6.norm2.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.1.6.norm2.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.1.6.mlp.fc1.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 1.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.1.6.mlp.fc1.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.1.6.mlp.fc2.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 1.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.1.6.mlp.fc2.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.1.6.ls2.gamma: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.1.7.norm1.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.1.7.norm1.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.1.7.attn.qkv.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 1.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.1.7.attn.qkv.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.1.7.attn.proj.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 1.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.1.7.attn.proj.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.1.7.ls1.gamma: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.1.7.norm2.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.1.7.norm2.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.1.7.mlp.fc1.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 1.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.1.7.mlp.fc1.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.1.7.mlp.fc2.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 1.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.1.7.mlp.fc2.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.1.7.ls2.gamma: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.1.8.norm1.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.1.8.norm1.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.1.8.attn.qkv.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 1.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.1.8.attn.qkv.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.1.8.attn.proj.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 1.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.1.8.attn.proj.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.1.8.ls1.gamma: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.1.8.norm2.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.1.8.norm2.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.1.8.mlp.fc1.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 1.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.1.8.mlp.fc1.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.1.8.mlp.fc2.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 1.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.1.8.mlp.fc2.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.1.8.ls2.gamma: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.1.9.norm1.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.1.9.norm1.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.1.9.attn.qkv.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 1.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.1.9.attn.qkv.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.1.9.attn.proj.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 1.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.1.9.attn.proj.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.1.9.ls1.gamma: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.1.9.norm2.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.1.9.norm2.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.1.9.mlp.fc1.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 1.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.1.9.mlp.fc1.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.1.9.mlp.fc2.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 1.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.1.9.mlp.fc2.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.1.9.ls2.gamma: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.1.10.norm1.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.1.10.norm1.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.1.10.attn.qkv.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 1.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.1.10.attn.qkv.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.1.10.attn.proj.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 1.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.1.10.attn.proj.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.1.10.ls1.gamma: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.1.10.norm2.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.1.10.norm2.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.1.10.mlp.fc1.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 1.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.1.10.mlp.fc1.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.1.10.mlp.fc2.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 1.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.1.10.mlp.fc2.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.1.10.ls2.gamma: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.1.11.norm1.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.1.11.norm1.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.1.11.attn.qkv.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 1.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.1.11.attn.qkv.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.1.11.attn.proj.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 1.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.1.11.attn.proj.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.1.11.ls1.gamma: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.1.11.norm2.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.1.11.norm2.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.1.11.mlp.fc1.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 1.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.1.11.mlp.fc1.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.1.11.mlp.fc2.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 1.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.1.11.mlp.fc2.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.1.11.ls2.gamma: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.2.12.norm1.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.2.12.norm1.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.2.12.attn.qkv.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 1.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.2.12.attn.qkv.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.2.12.attn.proj.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 1.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.2.12.attn.proj.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.2.12.ls1.gamma: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.2.12.norm2.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.2.12.norm2.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.2.12.mlp.fc1.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 1.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.2.12.mlp.fc1.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.2.12.mlp.fc2.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 1.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.2.12.mlp.fc2.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.2.12.ls2.gamma: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.2.13.norm1.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.2.13.norm1.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.2.13.attn.qkv.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 1.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.2.13.attn.qkv.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.2.13.attn.proj.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 1.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.2.13.attn.proj.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.2.13.ls1.gamma: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.2.13.norm2.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.2.13.norm2.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.2.13.mlp.fc1.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 1.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.2.13.mlp.fc1.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.2.13.mlp.fc2.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 1.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.2.13.mlp.fc2.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.2.13.ls2.gamma: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.2.14.norm1.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.2.14.norm1.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.2.14.attn.qkv.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 1.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.2.14.attn.qkv.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.2.14.attn.proj.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 1.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.2.14.attn.proj.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.2.14.ls1.gamma: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.2.14.norm2.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.2.14.norm2.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.2.14.mlp.fc1.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 1.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.2.14.mlp.fc1.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.2.14.mlp.fc2.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 1.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.2.14.mlp.fc2.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.2.14.ls2.gamma: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.2.15.norm1.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.2.15.norm1.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.2.15.attn.qkv.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 1.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.2.15.attn.qkv.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.2.15.attn.proj.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 1.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.2.15.attn.proj.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.2.15.ls1.gamma: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.2.15.norm2.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.2.15.norm2.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.2.15.mlp.fc1.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 1.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.2.15.mlp.fc1.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.2.15.mlp.fc2.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 1.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.2.15.mlp.fc2.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.2.15.ls2.gamma: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.2.16.norm1.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.2.16.norm1.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.2.16.attn.qkv.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 1.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.2.16.attn.qkv.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.2.16.attn.proj.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 1.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.2.16.attn.proj.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.2.16.ls1.gamma: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.2.16.norm2.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.2.16.norm2.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.2.16.mlp.fc1.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 1.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.2.16.mlp.fc1.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.2.16.mlp.fc2.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 1.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.2.16.mlp.fc2.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.2.16.ls2.gamma: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.2.17.norm1.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.2.17.norm1.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.2.17.attn.qkv.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 1.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.2.17.attn.qkv.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.2.17.attn.proj.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 1.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.2.17.attn.proj.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.2.17.ls1.gamma: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.2.17.norm2.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.2.17.norm2.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.2.17.mlp.fc1.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 1.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.2.17.mlp.fc1.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.2.17.mlp.fc2.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 1.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.2.17.mlp.fc2.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.2.17.ls2.gamma: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.3.18.norm1.weight: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.3.18.norm1.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.3.18.attn.qkv.weight: lr_multiplier: 0.531441, wd_multiplier: 1.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.3.18.attn.qkv.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.3.18.attn.proj.weight: lr_multiplier: 0.531441, wd_multiplier: 1.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.3.18.attn.proj.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.3.18.ls1.gamma: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.3.18.norm2.weight: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.3.18.norm2.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.3.18.mlp.fc1.weight: lr_multiplier: 0.531441, wd_multiplier: 1.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.3.18.mlp.fc1.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.3.18.mlp.fc2.weight: lr_multiplier: 0.531441, wd_multiplier: 1.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.3.18.mlp.fc2.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.3.18.ls2.gamma: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.3.19.norm1.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.3.19.norm1.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.3.19.attn.qkv.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 1.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.3.19.attn.qkv.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.3.19.attn.proj.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 1.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.3.19.attn.proj.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.3.19.ls1.gamma: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.3.19.norm2.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.3.19.norm2.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.3.19.mlp.fc1.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 1.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.3.19.mlp.fc1.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.3.19.mlp.fc2.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 1.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.3.19.mlp.fc2.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.3.19.ls2.gamma: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.3.20.norm1.weight: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.3.20.norm1.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.3.20.attn.qkv.weight: lr_multiplier: 0.6561, wd_multiplier: 1.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.3.20.attn.qkv.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.3.20.attn.proj.weight: lr_multiplier: 0.6561, wd_multiplier: 1.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.3.20.attn.proj.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.3.20.ls1.gamma: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.3.20.norm2.weight: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.3.20.norm2.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.3.20.mlp.fc1.weight: lr_multiplier: 0.6561, wd_multiplier: 1.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.3.20.mlp.fc1.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.3.20.mlp.fc2.weight: lr_multiplier: 0.6561, wd_multiplier: 1.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.3.20.mlp.fc2.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.3.20.ls2.gamma: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.3.21.norm1.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.3.21.norm1.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.3.21.attn.qkv.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 1.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.3.21.attn.qkv.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.3.21.attn.proj.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 1.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.3.21.attn.proj.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.3.21.ls1.gamma: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.3.21.norm2.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.3.21.norm2.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.3.21.mlp.fc1.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 1.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.3.21.mlp.fc1.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.3.21.mlp.fc2.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 1.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.3.21.mlp.fc2.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.3.21.ls2.gamma: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.3.22.norm1.weight: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.3.22.norm1.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.3.22.attn.qkv.weight: lr_multiplier: 0.81, wd_multiplier: 1.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.3.22.attn.qkv.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.3.22.attn.proj.weight: lr_multiplier: 0.81, wd_multiplier: 1.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.3.22.attn.proj.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.3.22.ls1.gamma: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.3.22.norm2.weight: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.3.22.norm2.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.3.22.mlp.fc1.weight: lr_multiplier: 0.81, wd_multiplier: 1.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.3.22.mlp.fc1.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.3.22.mlp.fc2.weight: lr_multiplier: 0.81, wd_multiplier: 1.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.3.22.mlp.fc2.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.3.22.ls2.gamma: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.3.23.norm1.weight: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.3.23.norm1.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.3.23.attn.qkv.weight: lr_multiplier: 0.9, wd_multiplier: 1.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.3.23.attn.qkv.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.3.23.attn.proj.weight: lr_multiplier: 0.9, wd_multiplier: 1.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.3.23.attn.proj.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.3.23.ls1.gamma: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.3.23.norm2.weight: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.3.23.norm2.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.3.23.mlp.fc1.weight: lr_multiplier: 0.9, wd_multiplier: 1.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.3.23.mlp.fc1.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.3.23.mlp.fc2.weight: lr_multiplier: 0.9, wd_multiplier: 1.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.3.23.mlp.fc2.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] blocks.3.23.ls2.gamma: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] norm.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] norm.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 ssl_meta_arch.py:380] fusing param groups
I20240906 13:55:22 4100 dinov2 param_groups.py:64] else code branch
I20240906 13:55:22 4100 dinov2 param_groups.py:87] mlp.0.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] mlp.0.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] mlp.2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] mlp.2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] mlp.4.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] mlp.4.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] last_layer.weight_g: lr_multiplier: 1.0, wd_multiplier: 1.0
I20240906 13:55:22 4100 dinov2 param_groups.py:87] last_layer.weight_v: lr_multiplier: 1.0, wd_multiplier: 1.0
I20240906 13:55:22 4100 dinov2 ssl_meta_arch.py:380] fusing param groups
I20240906 13:55:22 4100 dinov2 train.py:98] Schedulers ready.
I20240906 13:55:22 4100 dinov2 augmentations.py:34] ###################################
I20240906 13:55:22 4100 dinov2 augmentations.py:35] Using data augmentation parameters:
I20240906 13:55:22 4100 dinov2 augmentations.py:36] global_crops_scale: [0.32, 1.0]
I20240906 13:55:22 4100 dinov2 augmentations.py:37] local_crops_scale: [0.05, 0.32]
I20240906 13:55:22 4100 dinov2 augmentations.py:38] local_crops_number: 8
I20240906 13:55:22 4100 dinov2 augmentations.py:39] global_crops_size: 224
I20240906 13:55:22 4100 dinov2 augmentations.py:40] local_crops_size: 96
I20240906 13:55:22 4100 dinov2 augmentations.py:41] ###################################
I20240906 13:55:22 4100 dinov2 loaders.py:89] using dataset: "ImageNet:split=TRAIN:root=C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\datasets\ILSVRC\Data\CLS-LOC:extra=C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\datasets\ILSVRC\Data\CLS-LOC"
I20240906 13:55:22 4100 dinov2 loaders.py:94] # of dataset samples: 1,281,167
I20240906 13:55:22 4100 dinov2 loaders.py:117] sampler: infinite
I20240906 13:55:22 4100 dinov2 loaders.py:211] using PyTorch data loader
I20240906 13:55:22 4100 dinov2 loaders.py:226] infinite data loader
I20240906 13:55:22 4100 dinov2 train.py:216] Starting training from iteration 0
I20240906 14:17:13 1720 dinov2 config.py:59] git:
  sha: d43546fae6805ceb22618af8cf78469b61d913c7, status: has uncommitted changes, branch: main

I20240906 14:17:13 1720 dinov2 config.py:60] config_file: configs/train/vitl16_short.yaml
eval: 
eval_only: False
no_resume: False
opts: ['train.dataset_path=ImageNet:split=TRAIN:root=C:\\Users\\ISI_UTS\\Siladittya\\iclr2023\\inexpts\\datasets\\ILSVRC\\Data\\CLS-LOC:extra=C:\\Users\\ISI_UTS\\Siladittya\\iclr2023\\inexpts\\datasets\\ILSVRC\\Data\\CLS-LOC', 'train.output_dir=C:\\Users\\ISI_UTS\\Siladittya\\iclr2023\\inexpts\\dinov2_nosetup\\dinov2\\dinov2_nosetup\\dinov2']
output_dir: C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\dinov2_nosetup\dinov2\dinov2_nosetup\dinov2
I20240906 14:17:13 1720 dinov2 config.py:26] sqrt scaling learning rate; base: 0.004, new: 0.001
I20240906 14:17:13 1720 dinov2 config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
dino:
  loss_weight: 1.0
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
  koleo_loss_weight: 0.1
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  separate_head: false
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
train:
  batch_size_per_gpu: 64
  dataset_path: ImageNet:split=TRAIN:root=C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\datasets\ILSVRC\Data\CLS-LOC:extra=C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\datasets\ILSVRC\Data\CLS-LOC
  output_dir: C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\dinov2_nosetup\dinov2\dinov2_nosetup\dinov2
  saveckp_freq: 20
  seed: 0
  num_workers: 0
  OFFICIAL_EPOCH_LENGTH: 1250
  cache_dataset: true
  centering: centering
student:
  arch: vit_large
  patch_size: 16
  drop_path_rate: 0.3
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: mlp
  block_chunks: 4
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
  num_register_tokens: 0
  interpolate_antialias: false
  interpolate_offset: 0.1
teacher:
  momentum_teacher: 0.992
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 30
optim:
  epochs: 100
  weight_decay: 0.04
  weight_decay_end: 0.4
  base_lr: 0.004
  lr: 0.001
  warmup_epochs: 10
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 0.9
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 8
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 224
  local_crops_size: 96
evaluation:
  eval_period_iterations: 12500

I20240906 14:17:13 1720 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20240906 14:17:15 1720 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20240906 14:17:18 1720 dinov2 ssl_meta_arch.py:45] OPTIONS -- architecture : embed_dim: 1024
I20240906 14:17:18 1720 dinov2 ssl_meta_arch.py:60] OPTIONS -- DINO
I20240906 14:17:18 1720 dinov2 ssl_meta_arch.py:62] OPTIONS -- DINO -- loss_weight: 1.0
I20240906 14:17:18 1720 dinov2 ssl_meta_arch.py:63] OPTIONS -- DINO -- head_n_prototypes: 65536
I20240906 14:17:18 1720 dinov2 ssl_meta_arch.py:64] OPTIONS -- DINO -- head_bottleneck_dim: 256
I20240906 14:17:18 1720 dinov2 ssl_meta_arch.py:65] OPTIONS -- DINO -- head_hidden_dim: 2048
I20240906 14:17:18 1720 dinov2 ssl_meta_arch.py:77] OPTIONS -- DINO -- applying KOLEO regularization
I20240906 14:17:18 1720 dinov2 ssl_meta_arch.py:87] OPTIONS -- IBOT
I20240906 14:17:18 1720 dinov2 ssl_meta_arch.py:88] OPTIONS -- IBOT -- loss_weight: 1.0
I20240906 14:17:18 1720 dinov2 ssl_meta_arch.py:89] OPTIONS -- IBOT masking -- ibot_mask_ratio_tuple: [0.1, 0.5]
I20240906 14:17:18 1720 dinov2 ssl_meta_arch.py:90] OPTIONS -- IBOT masking -- ibot_mask_sample_probability: 0.5
I20240906 14:17:18 1720 dinov2 ssl_meta_arch.py:113] OPTIONS -- IBOT -- head shared with DINO
I20240906 14:17:18 1720 dinov2 ssl_meta_arch.py:123] Student and Teacher are built: they are both vit_large network.
I20240906 14:17:19 1720 dinov2 train.py:302] Model:
SSLMetaArch(
  (dino_loss): DINOLoss()
  (koleo_loss): KoLeoLoss(
    (pdist): PairwiseDistance()
  )
  (ibot_patch_loss): iBOTPatchLoss()
  (student): ModuleDict(
    (backbone): DinoVisionTransformer(
      (patch_embed): PatchEmbed(
        (proj): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))
        (norm): Identity()
      )
      (blocks): ModuleList(
        (0): BlockChunk(
          (0-5): 6 x NestedTensorBlock(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): DropPath()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): DropPath()
          )
        )
        (1): BlockChunk(
          (0-5): 6 x Identity()
          (6-11): 6 x NestedTensorBlock(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): DropPath()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): DropPath()
          )
        )
        (2): BlockChunk(
          (0-11): 12 x Identity()
          (12-17): 6 x NestedTensorBlock(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): DropPath()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): DropPath()
          )
        )
        (3): BlockChunk(
          (0-17): 18 x Identity()
          (18-23): 6 x NestedTensorBlock(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): DropPath()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): DropPath()
          )
        )
      )
      (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (head): Identity()
    )
    (dino_head): DINOHead(
      (mlp): Sequential(
        (0): Linear(in_features=1024, out_features=2048, bias=True)
        (1): GELU(approximate='none')
        (2): Linear(in_features=2048, out_features=2048, bias=True)
        (3): GELU(approximate='none')
        (4): Linear(in_features=2048, out_features=256, bias=True)
      )
      (last_layer): Linear(in_features=256, out_features=65536, bias=False)
    )
  )
  (teacher): ModuleDict(
    (backbone): DinoVisionTransformer(
      (patch_embed): PatchEmbed(
        (proj): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))
        (norm): Identity()
      )
      (blocks): ModuleList(
        (0): BlockChunk(
          (0-5): 6 x NestedTensorBlock(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): Identity()
          )
        )
        (1): BlockChunk(
          (0-5): 6 x Identity()
          (6-11): 6 x NestedTensorBlock(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): Identity()
          )
        )
        (2): BlockChunk(
          (0-11): 12 x Identity()
          (12-17): 6 x NestedTensorBlock(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): Identity()
          )
        )
        (3): BlockChunk(
          (0-17): 18 x Identity()
          (18-23): 6 x NestedTensorBlock(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): Identity()
          )
        )
      )
      (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (head): Identity()
    )
    (dino_head): DINOHead(
      (mlp): Sequential(
        (0): Linear(in_features=1024, out_features=2048, bias=True)
        (1): GELU(approximate='none')
        (2): Linear(in_features=2048, out_features=2048, bias=True)
        (3): GELU(approximate='none')
        (4): Linear(in_features=2048, out_features=256, bias=True)
      )
      (last_layer): Linear(in_features=256, out_features=65536, bias=False)
    )
  )
)
I20240906 14:17:19 1720 dinov2 param_groups.py:54] chunked fsdp
I20240906 14:17:19 1720 dinov2 param_groups.py:87] cls_token: lr_multiplier: 0.0717897987691853, wd_multiplier: 1.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] pos_embed: lr_multiplier: 0.0717897987691853, wd_multiplier: 1.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] mask_token: lr_multiplier: 0.0717897987691853, wd_multiplier: 1.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] patch_embed.proj.weight: lr_multiplier: 0.01435795975383706, wd_multiplier: 1.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] patch_embed.proj.bias: lr_multiplier: 0.01435795975383706, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.0.0.norm1.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.0.0.norm1.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.0.0.attn.qkv.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 1.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.0.0.attn.qkv.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.0.0.attn.proj.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 1.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.0.0.attn.proj.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.0.0.ls1.gamma: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.0.0.norm2.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.0.0.norm2.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.0.0.mlp.fc1.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 1.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.0.0.mlp.fc1.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.0.0.mlp.fc2.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 1.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.0.0.mlp.fc2.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.0.0.ls2.gamma: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.0.1.norm1.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.0.1.norm1.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.0.1.attn.qkv.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 1.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.0.1.attn.qkv.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.0.1.attn.proj.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 1.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.0.1.attn.proj.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.0.1.ls1.gamma: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.0.1.norm2.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.0.1.norm2.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.0.1.mlp.fc1.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 1.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.0.1.mlp.fc1.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.0.1.mlp.fc2.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 1.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.0.1.mlp.fc2.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.0.1.ls2.gamma: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.0.2.norm1.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.0.2.norm1.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.0.2.attn.qkv.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 1.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.0.2.attn.qkv.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.0.2.attn.proj.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 1.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.0.2.attn.proj.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.0.2.ls1.gamma: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.0.2.norm2.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.0.2.norm2.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.0.2.mlp.fc1.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 1.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.0.2.mlp.fc1.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.0.2.mlp.fc2.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 1.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.0.2.mlp.fc2.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.0.2.ls2.gamma: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.0.3.norm1.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.0.3.norm1.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.0.3.attn.qkv.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 1.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.0.3.attn.qkv.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.0.3.attn.proj.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 1.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.0.3.attn.proj.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.0.3.ls1.gamma: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.0.3.norm2.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.0.3.norm2.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.0.3.mlp.fc1.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 1.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.0.3.mlp.fc1.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.0.3.mlp.fc2.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 1.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.0.3.mlp.fc2.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.0.3.ls2.gamma: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.0.4.norm1.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.0.4.norm1.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.0.4.attn.qkv.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 1.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.0.4.attn.qkv.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.0.4.attn.proj.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 1.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.0.4.attn.proj.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.0.4.ls1.gamma: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.0.4.norm2.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.0.4.norm2.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.0.4.mlp.fc1.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 1.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.0.4.mlp.fc1.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.0.4.mlp.fc2.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 1.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.0.4.mlp.fc2.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.0.4.ls2.gamma: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.0.5.norm1.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.0.5.norm1.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.0.5.attn.qkv.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 1.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.0.5.attn.qkv.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.0.5.attn.proj.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 1.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.0.5.attn.proj.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.0.5.ls1.gamma: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.0.5.norm2.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.0.5.norm2.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.0.5.mlp.fc1.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 1.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.0.5.mlp.fc1.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.0.5.mlp.fc2.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 1.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.0.5.mlp.fc2.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.0.5.ls2.gamma: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.1.6.norm1.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.1.6.norm1.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.1.6.attn.qkv.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 1.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.1.6.attn.qkv.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.1.6.attn.proj.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 1.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.1.6.attn.proj.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.1.6.ls1.gamma: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.1.6.norm2.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.1.6.norm2.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.1.6.mlp.fc1.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 1.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.1.6.mlp.fc1.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.1.6.mlp.fc2.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 1.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.1.6.mlp.fc2.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.1.6.ls2.gamma: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.1.7.norm1.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.1.7.norm1.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.1.7.attn.qkv.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 1.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.1.7.attn.qkv.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.1.7.attn.proj.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 1.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.1.7.attn.proj.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.1.7.ls1.gamma: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.1.7.norm2.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.1.7.norm2.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.1.7.mlp.fc1.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 1.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.1.7.mlp.fc1.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.1.7.mlp.fc2.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 1.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.1.7.mlp.fc2.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.1.7.ls2.gamma: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.1.8.norm1.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.1.8.norm1.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.1.8.attn.qkv.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 1.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.1.8.attn.qkv.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.1.8.attn.proj.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 1.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.1.8.attn.proj.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.1.8.ls1.gamma: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.1.8.norm2.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.1.8.norm2.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.1.8.mlp.fc1.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 1.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.1.8.mlp.fc1.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.1.8.mlp.fc2.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 1.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.1.8.mlp.fc2.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.1.8.ls2.gamma: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.1.9.norm1.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.1.9.norm1.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.1.9.attn.qkv.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 1.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.1.9.attn.qkv.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.1.9.attn.proj.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 1.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.1.9.attn.proj.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.1.9.ls1.gamma: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.1.9.norm2.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.1.9.norm2.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.1.9.mlp.fc1.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 1.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.1.9.mlp.fc1.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.1.9.mlp.fc2.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 1.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.1.9.mlp.fc2.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.1.9.ls2.gamma: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.1.10.norm1.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.1.10.norm1.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.1.10.attn.qkv.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 1.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.1.10.attn.qkv.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.1.10.attn.proj.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 1.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.1.10.attn.proj.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.1.10.ls1.gamma: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.1.10.norm2.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.1.10.norm2.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.1.10.mlp.fc1.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 1.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.1.10.mlp.fc1.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.1.10.mlp.fc2.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 1.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.1.10.mlp.fc2.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.1.10.ls2.gamma: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.1.11.norm1.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.1.11.norm1.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.1.11.attn.qkv.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 1.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.1.11.attn.qkv.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.1.11.attn.proj.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 1.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.1.11.attn.proj.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.1.11.ls1.gamma: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.1.11.norm2.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.1.11.norm2.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.1.11.mlp.fc1.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 1.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.1.11.mlp.fc1.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.1.11.mlp.fc2.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 1.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.1.11.mlp.fc2.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.1.11.ls2.gamma: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.2.12.norm1.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.2.12.norm1.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.2.12.attn.qkv.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 1.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.2.12.attn.qkv.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.2.12.attn.proj.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 1.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.2.12.attn.proj.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.2.12.ls1.gamma: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.2.12.norm2.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.2.12.norm2.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.2.12.mlp.fc1.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 1.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.2.12.mlp.fc1.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.2.12.mlp.fc2.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 1.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.2.12.mlp.fc2.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.2.12.ls2.gamma: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.2.13.norm1.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.2.13.norm1.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.2.13.attn.qkv.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 1.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.2.13.attn.qkv.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.2.13.attn.proj.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 1.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.2.13.attn.proj.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.2.13.ls1.gamma: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.2.13.norm2.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.2.13.norm2.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.2.13.mlp.fc1.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 1.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.2.13.mlp.fc1.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.2.13.mlp.fc2.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 1.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.2.13.mlp.fc2.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.2.13.ls2.gamma: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.2.14.norm1.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.2.14.norm1.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.2.14.attn.qkv.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 1.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.2.14.attn.qkv.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.2.14.attn.proj.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 1.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.2.14.attn.proj.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.2.14.ls1.gamma: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.2.14.norm2.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.2.14.norm2.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.2.14.mlp.fc1.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 1.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.2.14.mlp.fc1.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.2.14.mlp.fc2.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 1.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.2.14.mlp.fc2.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.2.14.ls2.gamma: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.2.15.norm1.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.2.15.norm1.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.2.15.attn.qkv.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 1.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.2.15.attn.qkv.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.2.15.attn.proj.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 1.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.2.15.attn.proj.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.2.15.ls1.gamma: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.2.15.norm2.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.2.15.norm2.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.2.15.mlp.fc1.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 1.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.2.15.mlp.fc1.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.2.15.mlp.fc2.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 1.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.2.15.mlp.fc2.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.2.15.ls2.gamma: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.2.16.norm1.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.2.16.norm1.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.2.16.attn.qkv.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 1.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.2.16.attn.qkv.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.2.16.attn.proj.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 1.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.2.16.attn.proj.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.2.16.ls1.gamma: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.2.16.norm2.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.2.16.norm2.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.2.16.mlp.fc1.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 1.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.2.16.mlp.fc1.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.2.16.mlp.fc2.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 1.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.2.16.mlp.fc2.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.2.16.ls2.gamma: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.2.17.norm1.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.2.17.norm1.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.2.17.attn.qkv.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 1.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.2.17.attn.qkv.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.2.17.attn.proj.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 1.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.2.17.attn.proj.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.2.17.ls1.gamma: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.2.17.norm2.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.2.17.norm2.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.2.17.mlp.fc1.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 1.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.2.17.mlp.fc1.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.2.17.mlp.fc2.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 1.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.2.17.mlp.fc2.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.2.17.ls2.gamma: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.3.18.norm1.weight: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.3.18.norm1.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.3.18.attn.qkv.weight: lr_multiplier: 0.531441, wd_multiplier: 1.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.3.18.attn.qkv.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.3.18.attn.proj.weight: lr_multiplier: 0.531441, wd_multiplier: 1.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.3.18.attn.proj.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.3.18.ls1.gamma: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.3.18.norm2.weight: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.3.18.norm2.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.3.18.mlp.fc1.weight: lr_multiplier: 0.531441, wd_multiplier: 1.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.3.18.mlp.fc1.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.3.18.mlp.fc2.weight: lr_multiplier: 0.531441, wd_multiplier: 1.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.3.18.mlp.fc2.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.3.18.ls2.gamma: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.3.19.norm1.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.3.19.norm1.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.3.19.attn.qkv.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 1.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.3.19.attn.qkv.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.3.19.attn.proj.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 1.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.3.19.attn.proj.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.3.19.ls1.gamma: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.3.19.norm2.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.3.19.norm2.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.3.19.mlp.fc1.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 1.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.3.19.mlp.fc1.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.3.19.mlp.fc2.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 1.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.3.19.mlp.fc2.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.3.19.ls2.gamma: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.3.20.norm1.weight: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.3.20.norm1.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.3.20.attn.qkv.weight: lr_multiplier: 0.6561, wd_multiplier: 1.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.3.20.attn.qkv.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.3.20.attn.proj.weight: lr_multiplier: 0.6561, wd_multiplier: 1.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.3.20.attn.proj.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.3.20.ls1.gamma: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.3.20.norm2.weight: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.3.20.norm2.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.3.20.mlp.fc1.weight: lr_multiplier: 0.6561, wd_multiplier: 1.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.3.20.mlp.fc1.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.3.20.mlp.fc2.weight: lr_multiplier: 0.6561, wd_multiplier: 1.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.3.20.mlp.fc2.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.3.20.ls2.gamma: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.3.21.norm1.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.3.21.norm1.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.3.21.attn.qkv.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 1.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.3.21.attn.qkv.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.3.21.attn.proj.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 1.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.3.21.attn.proj.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.3.21.ls1.gamma: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.3.21.norm2.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.3.21.norm2.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.3.21.mlp.fc1.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 1.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.3.21.mlp.fc1.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.3.21.mlp.fc2.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 1.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.3.21.mlp.fc2.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.3.21.ls2.gamma: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.3.22.norm1.weight: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.3.22.norm1.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.3.22.attn.qkv.weight: lr_multiplier: 0.81, wd_multiplier: 1.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.3.22.attn.qkv.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.3.22.attn.proj.weight: lr_multiplier: 0.81, wd_multiplier: 1.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.3.22.attn.proj.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.3.22.ls1.gamma: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.3.22.norm2.weight: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.3.22.norm2.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.3.22.mlp.fc1.weight: lr_multiplier: 0.81, wd_multiplier: 1.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.3.22.mlp.fc1.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.3.22.mlp.fc2.weight: lr_multiplier: 0.81, wd_multiplier: 1.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.3.22.mlp.fc2.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.3.22.ls2.gamma: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.3.23.norm1.weight: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.3.23.norm1.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.3.23.attn.qkv.weight: lr_multiplier: 0.9, wd_multiplier: 1.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.3.23.attn.qkv.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.3.23.attn.proj.weight: lr_multiplier: 0.9, wd_multiplier: 1.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.3.23.attn.proj.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.3.23.ls1.gamma: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.3.23.norm2.weight: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.3.23.norm2.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.3.23.mlp.fc1.weight: lr_multiplier: 0.9, wd_multiplier: 1.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.3.23.mlp.fc1.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.3.23.mlp.fc2.weight: lr_multiplier: 0.9, wd_multiplier: 1.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.3.23.mlp.fc2.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] blocks.3.23.ls2.gamma: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] norm.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] norm.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 ssl_meta_arch.py:380] fusing param groups
I20240906 14:17:19 1720 dinov2 param_groups.py:64] else code branch
I20240906 14:17:19 1720 dinov2 param_groups.py:87] mlp.0.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] mlp.0.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] mlp.2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] mlp.2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] mlp.4.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] mlp.4.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] last_layer.weight_g: lr_multiplier: 1.0, wd_multiplier: 1.0
I20240906 14:17:19 1720 dinov2 param_groups.py:87] last_layer.weight_v: lr_multiplier: 1.0, wd_multiplier: 1.0
I20240906 14:17:19 1720 dinov2 ssl_meta_arch.py:380] fusing param groups
I20240906 14:17:19 1720 dinov2 train.py:98] Schedulers ready.
I20240906 14:17:19 1720 dinov2 augmentations.py:34] ###################################
I20240906 14:17:19 1720 dinov2 augmentations.py:35] Using data augmentation parameters:
I20240906 14:17:19 1720 dinov2 augmentations.py:36] global_crops_scale: [0.32, 1.0]
I20240906 14:17:19 1720 dinov2 augmentations.py:37] local_crops_scale: [0.05, 0.32]
I20240906 14:17:19 1720 dinov2 augmentations.py:38] local_crops_number: 8
I20240906 14:17:19 1720 dinov2 augmentations.py:39] global_crops_size: 224
I20240906 14:17:19 1720 dinov2 augmentations.py:40] local_crops_size: 96
I20240906 14:17:19 1720 dinov2 augmentations.py:41] ###################################
I20240906 14:17:19 1720 dinov2 loaders.py:89] using dataset: "ImageNet:split=TRAIN:root=C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\datasets\ILSVRC\Data\CLS-LOC:extra=C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\datasets\ILSVRC\Data\CLS-LOC"
I20240906 14:17:19 1720 dinov2 loaders.py:94] # of dataset samples: 1,281,167
I20240906 14:17:19 1720 dinov2 loaders.py:117] sampler: infinite
I20240906 14:17:19 1720 dinov2 loaders.py:211] using PyTorch data loader
I20240906 14:17:19 1720 dinov2 loaders.py:226] infinite data loader
I20240906 14:17:19 1720 dinov2 train.py:216] Starting training from iteration 0
I20240907 15:59:24 11724 dinov2 config.py:59] git:
  sha: d43546fae6805ceb22618af8cf78469b61d913c7, status: has uncommitted changes, branch: main

I20240907 15:59:24 11724 dinov2 config.py:60] config_file: configs/train/vitl16_short.yaml
eval: 
eval_only: False
no_resume: False
opts: ['train.dataset_path=ImageNet:split=TRAIN:root=C:\\Users\\ISI_UTS\\Siladittya\\iclr2023\\inexpts\\datasets\\ILSVRC\\Data\\CLS-LOC:extra=C:\\Users\\ISI_UTS\\Siladittya\\iclr2023\\inexpts\\datasets\\ILSVRC\\Data\\CLS-LOC', 'train.output_dir=C:\\Users\\ISI_UTS\\Siladittya\\iclr2023\\inexpts\\dinov2_nosetup\\dinov2\\dinov2_nosetup\\dinov2']
output_dir: C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\dinov2_nosetup\dinov2\dinov2_nosetup\dinov2
I20240907 15:59:24 11724 dinov2 config.py:26] sqrt scaling learning rate; base: 0.004, new: 0.001
I20240907 15:59:24 11724 dinov2 config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
dino:
  loss_weight: 1.0
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
  koleo_loss_weight: 0.1
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  separate_head: false
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
train:
  batch_size_per_gpu: 64
  dataset_path: ImageNet:split=TRAIN:root=C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\datasets\ILSVRC\Data\CLS-LOC:extra=C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\datasets\ILSVRC\Data\CLS-LOC
  output_dir: C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\dinov2_nosetup\dinov2\dinov2_nosetup\dinov2
  saveckp_freq: 20
  seed: 0
  num_workers: 0
  OFFICIAL_EPOCH_LENGTH: 1250
  cache_dataset: true
  centering: centering
student:
  arch: vit_large
  patch_size: 16
  drop_path_rate: 0.3
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: mlp
  block_chunks: 4
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
  num_register_tokens: 0
  interpolate_antialias: false
  interpolate_offset: 0.1
teacher:
  momentum_teacher: 0.992
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 30
optim:
  epochs: 100
  weight_decay: 0.04
  weight_decay_end: 0.4
  base_lr: 0.004
  lr: 0.001
  warmup_epochs: 10
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 0.9
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 8
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 224
  local_crops_size: 96
evaluation:
  eval_period_iterations: 12500

I20240907 15:59:24 11724 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20240907 15:59:26 11724 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20240907 15:59:29 11724 dinov2 ssl_meta_arch.py:45] OPTIONS -- architecture : embed_dim: 1024
I20240907 15:59:29 11724 dinov2 ssl_meta_arch.py:60] OPTIONS -- DINO
I20240907 15:59:29 11724 dinov2 ssl_meta_arch.py:62] OPTIONS -- DINO -- loss_weight: 1.0
I20240907 15:59:29 11724 dinov2 ssl_meta_arch.py:63] OPTIONS -- DINO -- head_n_prototypes: 65536
I20240907 15:59:29 11724 dinov2 ssl_meta_arch.py:64] OPTIONS -- DINO -- head_bottleneck_dim: 256
I20240907 15:59:29 11724 dinov2 ssl_meta_arch.py:65] OPTIONS -- DINO -- head_hidden_dim: 2048
I20240907 15:59:29 11724 dinov2 ssl_meta_arch.py:77] OPTIONS -- DINO -- applying KOLEO regularization
I20240907 15:59:29 11724 dinov2 ssl_meta_arch.py:87] OPTIONS -- IBOT
I20240907 15:59:29 11724 dinov2 ssl_meta_arch.py:88] OPTIONS -- IBOT -- loss_weight: 1.0
I20240907 15:59:29 11724 dinov2 ssl_meta_arch.py:89] OPTIONS -- IBOT masking -- ibot_mask_ratio_tuple: [0.1, 0.5]
I20240907 15:59:29 11724 dinov2 ssl_meta_arch.py:90] OPTIONS -- IBOT masking -- ibot_mask_sample_probability: 0.5
I20240907 15:59:29 11724 dinov2 ssl_meta_arch.py:113] OPTIONS -- IBOT -- head shared with DINO
I20240907 15:59:29 11724 dinov2 ssl_meta_arch.py:123] Student and Teacher are built: they are both vit_large network.
I20240907 15:59:30 11724 dinov2 ssl_meta_arch.py:393] DISTRIBUTED FSDP -- preparing model for distributed training
I20240907 16:04:43 13476 dinov2 config.py:59] git:
  sha: d43546fae6805ceb22618af8cf78469b61d913c7, status: has uncommitted changes, branch: main

I20240907 16:04:43 13476 dinov2 config.py:60] config_file: configs/train/vitl16_short.yaml
eval: 
eval_only: False
no_resume: False
opts: ['train.dataset_path=ImageNet:split=TRAIN:root=C:\\Users\\ISI_UTS\\Siladittya\\iclr2023\\inexpts\\datasets\\ILSVRC\\Data\\CLS-LOC:extra=C:\\Users\\ISI_UTS\\Siladittya\\iclr2023\\inexpts\\datasets\\ILSVRC\\Data\\CLS-LOC', 'train.output_dir=C:\\Users\\ISI_UTS\\Siladittya\\iclr2023\\inexpts\\dinov2_nosetup\\dinov2\\dinov2_nosetup\\dinov2']
output_dir: C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\dinov2_nosetup\dinov2\dinov2_nosetup\dinov2
I20240907 16:04:43 13476 dinov2 config.py:26] sqrt scaling learning rate; base: 0.004, new: 0.001
I20240907 16:04:43 13476 dinov2 config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
dino:
  loss_weight: 1.0
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
  koleo_loss_weight: 0.1
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  separate_head: false
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
train:
  batch_size_per_gpu: 64
  dataset_path: ImageNet:split=TRAIN:root=C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\datasets\ILSVRC\Data\CLS-LOC:extra=C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\datasets\ILSVRC\Data\CLS-LOC
  output_dir: C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\dinov2_nosetup\dinov2\dinov2_nosetup\dinov2
  saveckp_freq: 20
  seed: 0
  num_workers: 0
  OFFICIAL_EPOCH_LENGTH: 1250
  cache_dataset: true
  centering: centering
student:
  arch: vit_large
  patch_size: 16
  drop_path_rate: 0.3
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: mlp
  block_chunks: 4
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
  num_register_tokens: 0
  interpolate_antialias: false
  interpolate_offset: 0.1
teacher:
  momentum_teacher: 0.992
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 30
optim:
  epochs: 100
  weight_decay: 0.04
  weight_decay_end: 0.4
  base_lr: 0.004
  lr: 0.001
  warmup_epochs: 10
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 0.9
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 8
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 224
  local_crops_size: 96
evaluation:
  eval_period_iterations: 12500

I20240907 16:04:43 13476 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20240907 16:04:46 13476 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20240907 16:04:48 13476 dinov2 ssl_meta_arch.py:45] OPTIONS -- architecture : embed_dim: 1024
I20240907 16:04:48 13476 dinov2 ssl_meta_arch.py:60] OPTIONS -- DINO
I20240907 16:04:48 13476 dinov2 ssl_meta_arch.py:62] OPTIONS -- DINO -- loss_weight: 1.0
I20240907 16:04:48 13476 dinov2 ssl_meta_arch.py:63] OPTIONS -- DINO -- head_n_prototypes: 65536
I20240907 16:04:48 13476 dinov2 ssl_meta_arch.py:64] OPTIONS -- DINO -- head_bottleneck_dim: 256
I20240907 16:04:48 13476 dinov2 ssl_meta_arch.py:65] OPTIONS -- DINO -- head_hidden_dim: 2048
I20240907 16:04:48 13476 dinov2 ssl_meta_arch.py:77] OPTIONS -- DINO -- applying KOLEO regularization
I20240907 16:04:48 13476 dinov2 ssl_meta_arch.py:87] OPTIONS -- IBOT
I20240907 16:04:48 13476 dinov2 ssl_meta_arch.py:88] OPTIONS -- IBOT -- loss_weight: 1.0
I20240907 16:04:48 13476 dinov2 ssl_meta_arch.py:89] OPTIONS -- IBOT masking -- ibot_mask_ratio_tuple: [0.1, 0.5]
I20240907 16:04:48 13476 dinov2 ssl_meta_arch.py:90] OPTIONS -- IBOT masking -- ibot_mask_sample_probability: 0.5
I20240907 16:04:48 13476 dinov2 ssl_meta_arch.py:113] OPTIONS -- IBOT -- head shared with DINO
I20240907 16:04:48 13476 dinov2 ssl_meta_arch.py:123] Student and Teacher are built: they are both vit_large network.
I20240907 16:04:49 13476 dinov2 ssl_meta_arch.py:393] DISTRIBUTED FSDP -- preparing model for distributed training
I20240907 16:21:55 2256 dinov2 config.py:59] git:
  sha: d43546fae6805ceb22618af8cf78469b61d913c7, status: has uncommitted changes, branch: main

I20240907 16:21:55 2256 dinov2 config.py:60] config_file: configs/train/vitl16_short.yaml
eval: 
eval_only: False
no_resume: False
opts: ['train.dataset_path=ImageNet:split=TRAIN:root=C:\\Users\\ISI_UTS\\Siladittya\\iclr2023\\inexpts\\datasets\\ILSVRC\\Data\\CLS-LOC:extra=C:\\Users\\ISI_UTS\\Siladittya\\iclr2023\\inexpts\\datasets\\ILSVRC\\Data\\CLS-LOC', '--num-workers', '4', 'train.output_dir=C:\\Users\\ISI_UTS\\Siladittya\\iclr2023\\inexpts\\dinov2_nosetup\\dinov2\\dinov2_nosetup\\dinov2']
output_dir: C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\dinov2_nosetup\dinov2\dinov2_nosetup\dinov2
I20240907 16:21:55 2256 dinov2 config.py:26] sqrt scaling learning rate; base: 0.004, new: 0.001
I20240907 16:21:55 2256 dinov2 config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
dino:
  loss_weight: 1.0
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
  koleo_loss_weight: 0.1
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  separate_head: false
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
train:
  batch_size_per_gpu: 64
  dataset_path: ImageNet:split=TRAIN:root=C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\datasets\ILSVRC\Data\CLS-LOC:extra=C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\datasets\ILSVRC\Data\CLS-LOC
  output_dir: C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\dinov2_nosetup\dinov2\dinov2_nosetup\dinov2
  saveckp_freq: 20
  seed: 0
  num_workers: 0
  OFFICIAL_EPOCH_LENGTH: 1250
  cache_dataset: true
  centering: centering
student:
  arch: vit_large
  patch_size: 16
  drop_path_rate: 0.3
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: mlp
  block_chunks: 4
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
  num_register_tokens: 0
  interpolate_antialias: false
  interpolate_offset: 0.1
teacher:
  momentum_teacher: 0.992
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 30
optim:
  epochs: 100
  weight_decay: 0.04
  weight_decay_end: 0.4
  base_lr: 0.004
  lr: 0.001
  warmup_epochs: 10
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 0.9
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 8
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 224
  local_crops_size: 96
evaluation:
  eval_period_iterations: 12500
--num-workers: null
'4': null

I20240907 16:21:55 2256 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20240907 16:21:57 2256 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20240907 16:21:59 2256 dinov2 ssl_meta_arch.py:45] OPTIONS -- architecture : embed_dim: 1024
I20240907 16:21:59 2256 dinov2 ssl_meta_arch.py:60] OPTIONS -- DINO
I20240907 16:21:59 2256 dinov2 ssl_meta_arch.py:62] OPTIONS -- DINO -- loss_weight: 1.0
I20240907 16:21:59 2256 dinov2 ssl_meta_arch.py:63] OPTIONS -- DINO -- head_n_prototypes: 65536
I20240907 16:21:59 2256 dinov2 ssl_meta_arch.py:64] OPTIONS -- DINO -- head_bottleneck_dim: 256
I20240907 16:21:59 2256 dinov2 ssl_meta_arch.py:65] OPTIONS -- DINO -- head_hidden_dim: 2048
I20240907 16:21:59 2256 dinov2 ssl_meta_arch.py:77] OPTIONS -- DINO -- applying KOLEO regularization
I20240907 16:22:00 2256 dinov2 ssl_meta_arch.py:87] OPTIONS -- IBOT
I20240907 16:22:00 2256 dinov2 ssl_meta_arch.py:88] OPTIONS -- IBOT -- loss_weight: 1.0
I20240907 16:22:00 2256 dinov2 ssl_meta_arch.py:89] OPTIONS -- IBOT masking -- ibot_mask_ratio_tuple: [0.1, 0.5]
I20240907 16:22:00 2256 dinov2 ssl_meta_arch.py:90] OPTIONS -- IBOT masking -- ibot_mask_sample_probability: 0.5
I20240907 16:22:00 2256 dinov2 ssl_meta_arch.py:113] OPTIONS -- IBOT -- head shared with DINO
I20240907 16:22:00 2256 dinov2 ssl_meta_arch.py:123] Student and Teacher are built: they are both vit_large network.
I20240907 16:22:01 2256 dinov2 ssl_meta_arch.py:393] DISTRIBUTED FSDP -- preparing model for distributed training
I20240907 16:25:01 18804 dinov2 config.py:59] git:
  sha: d43546fae6805ceb22618af8cf78469b61d913c7, status: has uncommitted changes, branch: main

I20240907 16:25:01 18804 dinov2 config.py:60] config_file: configs/train/vitl16_short.yaml
eval: 
eval_only: False
no_resume: False
opts: ['train.dataset_path=ImageNet:split=TRAIN:root=C:\\Users\\ISI_UTS\\Siladittya\\iclr2023\\inexpts\\datasets\\ILSVRC\\Data\\CLS-LOC:extra=C:\\Users\\ISI_UTS\\Siladittya\\iclr2023\\inexpts\\datasets\\ILSVRC\\Data\\CLS-LOC', '--num-workers', '0', 'train.output_dir=C:\\Users\\ISI_UTS\\Siladittya\\iclr2023\\inexpts\\dinov2_nosetup\\dinov2\\dinov2_nosetup\\dinov2']
output_dir: C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\dinov2_nosetup\dinov2\dinov2_nosetup\dinov2
I20240907 16:25:01 18804 dinov2 config.py:26] sqrt scaling learning rate; base: 0.004, new: 0.001
I20240907 16:25:01 18804 dinov2 config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
dino:
  loss_weight: 1.0
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
  koleo_loss_weight: 0.1
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  separate_head: false
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
train:
  batch_size_per_gpu: 64
  dataset_path: ImageNet:split=TRAIN:root=C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\datasets\ILSVRC\Data\CLS-LOC:extra=C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\datasets\ILSVRC\Data\CLS-LOC
  output_dir: C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\dinov2_nosetup\dinov2\dinov2_nosetup\dinov2
  saveckp_freq: 20
  seed: 0
  num_workers: 0
  OFFICIAL_EPOCH_LENGTH: 1250
  cache_dataset: true
  centering: centering
student:
  arch: vit_large
  patch_size: 16
  drop_path_rate: 0.3
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: mlp
  block_chunks: 4
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
  num_register_tokens: 0
  interpolate_antialias: false
  interpolate_offset: 0.1
teacher:
  momentum_teacher: 0.992
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 30
optim:
  epochs: 100
  weight_decay: 0.04
  weight_decay_end: 0.4
  base_lr: 0.004
  lr: 0.001
  warmup_epochs: 10
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 0.9
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 8
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 224
  local_crops_size: 96
evaluation:
  eval_period_iterations: 12500
--num-workers: null
'0': null

I20240907 16:25:01 18804 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20240907 16:25:03 18804 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20240907 16:25:05 18804 dinov2 ssl_meta_arch.py:45] OPTIONS -- architecture : embed_dim: 1024
I20240907 16:25:05 18804 dinov2 ssl_meta_arch.py:60] OPTIONS -- DINO
I20240907 16:25:05 18804 dinov2 ssl_meta_arch.py:62] OPTIONS -- DINO -- loss_weight: 1.0
I20240907 16:25:05 18804 dinov2 ssl_meta_arch.py:63] OPTIONS -- DINO -- head_n_prototypes: 65536
I20240907 16:25:05 18804 dinov2 ssl_meta_arch.py:64] OPTIONS -- DINO -- head_bottleneck_dim: 256
I20240907 16:25:05 18804 dinov2 ssl_meta_arch.py:65] OPTIONS -- DINO -- head_hidden_dim: 2048
I20240907 16:25:05 18804 dinov2 ssl_meta_arch.py:77] OPTIONS -- DINO -- applying KOLEO regularization
I20240907 16:25:06 18804 dinov2 ssl_meta_arch.py:87] OPTIONS -- IBOT
I20240907 16:25:06 18804 dinov2 ssl_meta_arch.py:88] OPTIONS -- IBOT -- loss_weight: 1.0
I20240907 16:25:06 18804 dinov2 ssl_meta_arch.py:89] OPTIONS -- IBOT masking -- ibot_mask_ratio_tuple: [0.1, 0.5]
I20240907 16:25:06 18804 dinov2 ssl_meta_arch.py:90] OPTIONS -- IBOT masking -- ibot_mask_sample_probability: 0.5
I20240907 16:25:06 18804 dinov2 ssl_meta_arch.py:113] OPTIONS -- IBOT -- head shared with DINO
I20240907 16:25:06 18804 dinov2 ssl_meta_arch.py:123] Student and Teacher are built: they are both vit_large network.
I20240907 16:25:07 18804 dinov2 ssl_meta_arch.py:393] DISTRIBUTED FSDP -- preparing model for distributed training
I20240907 16:27:16 10728 dinov2 config.py:59] git:
  sha: d43546fae6805ceb22618af8cf78469b61d913c7, status: has uncommitted changes, branch: main

I20240907 16:27:16 10728 dinov2 config.py:60] config_file: configs/train/vitl16_short.yaml
eval: 
eval_only: False
no_resume: False
opts: ['train.dataset_path=ImageNet:split=TRAIN:root=C:\\Users\\ISI_UTS\\Siladittya\\iclr2023\\inexpts\\datasets\\ILSVRC\\Data\\CLS-LOC:extra=C:\\Users\\ISI_UTS\\Siladittya\\iclr2023\\inexpts\\datasets\\ILSVRC\\Data\\CLS-LOC', 'train.output_dir=C:\\Users\\ISI_UTS\\Siladittya\\iclr2023\\inexpts\\dinov2_nosetup\\dinov2\\dinov2_nosetup\\dinov2']
output_dir: C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\dinov2_nosetup\dinov2\dinov2_nosetup\dinov2
I20240907 16:27:16 10728 dinov2 config.py:26] sqrt scaling learning rate; base: 0.004, new: 0.001
I20240907 16:27:17 10728 dinov2 config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
dino:
  loss_weight: 1.0
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
  koleo_loss_weight: 0.1
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  separate_head: false
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
train:
  batch_size_per_gpu: 64
  dataset_path: ImageNet:split=TRAIN:root=C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\datasets\ILSVRC\Data\CLS-LOC:extra=C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\datasets\ILSVRC\Data\CLS-LOC
  output_dir: C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\dinov2_nosetup\dinov2\dinov2_nosetup\dinov2
  saveckp_freq: 20
  seed: 0
  num_workers: 4
  OFFICIAL_EPOCH_LENGTH: 1250
  cache_dataset: true
  centering: centering
student:
  arch: vit_large
  patch_size: 16
  drop_path_rate: 0.3
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: mlp
  block_chunks: 4
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
  num_register_tokens: 0
  interpolate_antialias: false
  interpolate_offset: 0.1
teacher:
  momentum_teacher: 0.992
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 30
optim:
  epochs: 100
  weight_decay: 0.04
  weight_decay_end: 0.4
  base_lr: 0.004
  lr: 0.001
  warmup_epochs: 10
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 0.9
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 8
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 224
  local_crops_size: 96
evaluation:
  eval_period_iterations: 12500

I20240907 16:27:17 10728 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20240907 16:27:19 10728 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20240907 16:27:21 10728 dinov2 ssl_meta_arch.py:45] OPTIONS -- architecture : embed_dim: 1024
I20240907 16:27:21 10728 dinov2 ssl_meta_arch.py:60] OPTIONS -- DINO
I20240907 16:27:21 10728 dinov2 ssl_meta_arch.py:62] OPTIONS -- DINO -- loss_weight: 1.0
I20240907 16:27:21 10728 dinov2 ssl_meta_arch.py:63] OPTIONS -- DINO -- head_n_prototypes: 65536
I20240907 16:27:21 10728 dinov2 ssl_meta_arch.py:64] OPTIONS -- DINO -- head_bottleneck_dim: 256
I20240907 16:27:21 10728 dinov2 ssl_meta_arch.py:65] OPTIONS -- DINO -- head_hidden_dim: 2048
I20240907 16:27:21 10728 dinov2 ssl_meta_arch.py:77] OPTIONS -- DINO -- applying KOLEO regularization
I20240907 16:27:22 10728 dinov2 ssl_meta_arch.py:87] OPTIONS -- IBOT
I20240907 16:27:22 10728 dinov2 ssl_meta_arch.py:88] OPTIONS -- IBOT -- loss_weight: 1.0
I20240907 16:27:22 10728 dinov2 ssl_meta_arch.py:89] OPTIONS -- IBOT masking -- ibot_mask_ratio_tuple: [0.1, 0.5]
I20240907 16:27:22 10728 dinov2 ssl_meta_arch.py:90] OPTIONS -- IBOT masking -- ibot_mask_sample_probability: 0.5
I20240907 16:27:22 10728 dinov2 ssl_meta_arch.py:113] OPTIONS -- IBOT -- head shared with DINO
I20240907 16:27:22 10728 dinov2 ssl_meta_arch.py:123] Student and Teacher are built: they are both vit_large network.
I20240907 16:27:22 10728 dinov2 ssl_meta_arch.py:393] DISTRIBUTED FSDP -- preparing model for distributed training
I20240910 13:25:00 7852 dinov2 config.py:59] git:
  sha: d43546fae6805ceb22618af8cf78469b61d913c7, status: has uncommitted changes, branch: main

I20240910 13:25:00 7852 dinov2 config.py:60] config_file: configs/train/vitl16_short.yaml
eval: 
eval_only: False
no_resume: False
opts: ['train.dataset_path=ImageNet:split=TRAIN:root=C:\\Users\\ISI_UTS\\Siladittya\\iclr2023\\inexpts\\datasets\\ILSVRC\\Data\\CLS-LOC:extra=C:\\Users\\ISI_UTS\\Siladittya\\iclr2023\\inexpts\\datasets\\ILSVRC\\Data\\CLS-LOC', 'train.output_dir=C:\\Users\\ISI_UTS\\Siladittya\\iclr2023\\inexpts\\dinov2_nosetup\\dinov2\\dinov2_nosetup\\dinov2']
output_dir: C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\dinov2_nosetup\dinov2\dinov2_nosetup\dinov2
I20240910 13:25:00 7852 dinov2 config.py:26] sqrt scaling learning rate; base: 0.004, new: 0.001
I20240910 13:25:00 7852 dinov2 config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
dino:
  loss_weight: 1.0
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
  koleo_loss_weight: 0.1
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  separate_head: false
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
train:
  batch_size_per_gpu: 64
  dataset_path: ImageNet:split=TRAIN:root=C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\datasets\ILSVRC\Data\CLS-LOC:extra=C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\datasets\ILSVRC\Data\CLS-LOC
  output_dir: C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\dinov2_nosetup\dinov2\dinov2_nosetup\dinov2
  saveckp_freq: 20
  seed: 0
  num_workers: 0
  OFFICIAL_EPOCH_LENGTH: 1250
  cache_dataset: true
  centering: centering
student:
  arch: vit_large
  patch_size: 16
  drop_path_rate: 0.3
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: mlp
  block_chunks: 4
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
  num_register_tokens: 0
  interpolate_antialias: false
  interpolate_offset: 0.1
teacher:
  momentum_teacher: 0.992
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 30
optim:
  epochs: 100
  weight_decay: 0.04
  weight_decay_end: 0.4
  base_lr: 0.004
  lr: 0.001
  warmup_epochs: 10
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 0.9
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 8
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 224
  local_crops_size: 96
evaluation:
  eval_period_iterations: 12500

I20240910 13:25:00 7852 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20240910 13:25:03 7852 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20240910 13:25:05 7852 dinov2 ssl_meta_arch.py:45] OPTIONS -- architecture : embed_dim: 1024
I20240910 13:25:05 7852 dinov2 ssl_meta_arch.py:60] OPTIONS -- DINO
I20240910 13:25:05 7852 dinov2 ssl_meta_arch.py:62] OPTIONS -- DINO -- loss_weight: 1.0
I20240910 13:25:05 7852 dinov2 ssl_meta_arch.py:63] OPTIONS -- DINO -- head_n_prototypes: 65536
I20240910 13:25:05 7852 dinov2 ssl_meta_arch.py:64] OPTIONS -- DINO -- head_bottleneck_dim: 256
I20240910 13:25:05 7852 dinov2 ssl_meta_arch.py:65] OPTIONS -- DINO -- head_hidden_dim: 2048
I20240910 13:25:05 7852 dinov2 ssl_meta_arch.py:77] OPTIONS -- DINO -- applying KOLEO regularization
I20240910 13:25:06 7852 dinov2 ssl_meta_arch.py:87] OPTIONS -- IBOT
I20240910 13:25:06 7852 dinov2 ssl_meta_arch.py:88] OPTIONS -- IBOT -- loss_weight: 1.0
I20240910 13:25:06 7852 dinov2 ssl_meta_arch.py:89] OPTIONS -- IBOT masking -- ibot_mask_ratio_tuple: [0.1, 0.5]
I20240910 13:25:06 7852 dinov2 ssl_meta_arch.py:90] OPTIONS -- IBOT masking -- ibot_mask_sample_probability: 0.5
I20240910 13:25:06 7852 dinov2 ssl_meta_arch.py:113] OPTIONS -- IBOT -- head shared with DINO
I20240910 13:25:06 7852 dinov2 ssl_meta_arch.py:123] Student and Teacher are built: they are both vit_large network.
I20240910 13:25:07 7852 dinov2 ssl_meta_arch.py:393] DISTRIBUTED FSDP -- preparing model for distributed training
I20240910 13:38:03 20100 dinov2 config.py:59] git:
  sha: d43546fae6805ceb22618af8cf78469b61d913c7, status: has uncommitted changes, branch: main

I20240910 13:38:03 20100 dinov2 config.py:60] config_file: configs/train/vitl16_short.yaml
eval: 
eval_only: False
no_resume: False
opts: ['train.dataset_path=ImageNet:split=TRAIN:root=C:\\Users\\ISI_UTS\\Siladittya\\iclr2023\\inexpts\\datasets\\ILSVRC\\Data\\CLS-LOC:extra=C:\\Users\\ISI_UTS\\Siladittya\\iclr2023\\inexpts\\datasets\\ILSVRC\\Data\\CLS-LOC', 'train.output_dir=C:\\Users\\ISI_UTS\\Siladittya\\iclr2023\\inexpts\\dinov2_nosetup\\dinov2\\dinov2_nosetup\\dinov2']
output_dir: C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\dinov2_nosetup\dinov2\dinov2_nosetup\dinov2
I20240910 13:38:03 20100 dinov2 config.py:26] sqrt scaling learning rate; base: 0.004, new: 0.001
I20240910 13:38:03 20100 dinov2 config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
dino:
  loss_weight: 1.0
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
  koleo_loss_weight: 0.1
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  separate_head: false
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
train:
  batch_size_per_gpu: 64
  dataset_path: ImageNet:split=TRAIN:root=C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\datasets\ILSVRC\Data\CLS-LOC:extra=C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\datasets\ILSVRC\Data\CLS-LOC
  output_dir: C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\dinov2_nosetup\dinov2\dinov2_nosetup\dinov2
  saveckp_freq: 20
  seed: 0
  num_workers: 0
  OFFICIAL_EPOCH_LENGTH: 1250
  cache_dataset: true
  centering: centering
student:
  arch: vit_large
  patch_size: 16
  drop_path_rate: 0.3
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: mlp
  block_chunks: 4
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
  num_register_tokens: 0
  interpolate_antialias: false
  interpolate_offset: 0.1
teacher:
  momentum_teacher: 0.992
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 30
optim:
  epochs: 100
  weight_decay: 0.04
  weight_decay_end: 0.4
  base_lr: 0.004
  lr: 0.001
  warmup_epochs: 10
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 0.9
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 8
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 224
  local_crops_size: 96
evaluation:
  eval_period_iterations: 12500

I20240910 13:38:03 20100 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20240910 13:38:06 20100 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20240910 13:38:08 20100 dinov2 ssl_meta_arch.py:45] OPTIONS -- architecture : embed_dim: 1024
I20240910 13:38:08 20100 dinov2 ssl_meta_arch.py:60] OPTIONS -- DINO
I20240910 13:38:08 20100 dinov2 ssl_meta_arch.py:62] OPTIONS -- DINO -- loss_weight: 1.0
I20240910 13:38:08 20100 dinov2 ssl_meta_arch.py:63] OPTIONS -- DINO -- head_n_prototypes: 65536
I20240910 13:38:08 20100 dinov2 ssl_meta_arch.py:64] OPTIONS -- DINO -- head_bottleneck_dim: 256
I20240910 13:38:08 20100 dinov2 ssl_meta_arch.py:65] OPTIONS -- DINO -- head_hidden_dim: 2048
I20240910 13:38:08 20100 dinov2 ssl_meta_arch.py:77] OPTIONS -- DINO -- applying KOLEO regularization
I20240910 13:38:08 20100 dinov2 ssl_meta_arch.py:87] OPTIONS -- IBOT
I20240910 13:38:08 20100 dinov2 ssl_meta_arch.py:88] OPTIONS -- IBOT -- loss_weight: 1.0
I20240910 13:38:08 20100 dinov2 ssl_meta_arch.py:89] OPTIONS -- IBOT masking -- ibot_mask_ratio_tuple: [0.1, 0.5]
I20240910 13:38:08 20100 dinov2 ssl_meta_arch.py:90] OPTIONS -- IBOT masking -- ibot_mask_sample_probability: 0.5
I20240910 13:38:08 20100 dinov2 ssl_meta_arch.py:113] OPTIONS -- IBOT -- head shared with DINO
I20240910 13:38:08 20100 dinov2 ssl_meta_arch.py:123] Student and Teacher are built: they are both vit_large network.
I20240910 13:38:09 20100 dinov2 ssl_meta_arch.py:393] DISTRIBUTED FSDP -- preparing model for distributed training
W20240910 13:38:10 20100 py.warnings warnings.py:109] C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\lib\site-packages\torch\distributed\fsdp\_init_utils.py:295: UserWarning: FSDP is switching to use `NO_SHARD` instead of ShardingStrategy.SHARD_GRAD_OP since the world size is 1.
  warnings.warn(

I20240910 13:38:11 20100 dinov2 train.py:304] Model:
SSLMetaArch(
  (dino_loss): DINOLoss()
  (koleo_loss): KoLeoLoss(
    (pdist): PairwiseDistance()
  )
  (ibot_patch_loss): iBOTPatchLoss()
  (student): ModuleDict(
    (backbone): FullyShardedDataParallel(
      (_fsdp_wrapped_module): DinoVisionTransformer(
        (patch_embed): PatchEmbed(
          (proj): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))
          (norm): Identity()
        )
        (blocks): ModuleList(
          (0): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-5): 6 x NestedTensorBlock(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): DropPath()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): DropPath()
              )
            )
          )
          (1): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-5): 6 x Identity()
              (6-11): 6 x NestedTensorBlock(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): DropPath()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): DropPath()
              )
            )
          )
          (2): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-11): 12 x Identity()
              (12-17): 6 x NestedTensorBlock(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): DropPath()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): DropPath()
              )
            )
          )
          (3): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-17): 18 x Identity()
              (18-23): 6 x NestedTensorBlock(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): DropPath()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): DropPath()
              )
            )
          )
        )
        (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (head): Identity()
      )
    )
    (dino_head): FullyShardedDataParallel(
      (_fsdp_wrapped_module): DINOHead(
        (mlp): Sequential(
          (0): Linear(in_features=1024, out_features=2048, bias=True)
          (1): GELU(approximate='none')
          (2): Linear(in_features=2048, out_features=2048, bias=True)
          (3): GELU(approximate='none')
          (4): Linear(in_features=2048, out_features=256, bias=True)
        )
        (last_layer): Linear(in_features=256, out_features=65536, bias=False)
      )
    )
  )
  (teacher): ModuleDict(
    (backbone): FullyShardedDataParallel(
      (_fsdp_wrapped_module): DinoVisionTransformer(
        (patch_embed): PatchEmbed(
          (proj): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))
          (norm): Identity()
        )
        (blocks): ModuleList(
          (0): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-5): 6 x NestedTensorBlock(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): Identity()
              )
            )
          )
          (1): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-5): 6 x Identity()
              (6-11): 6 x NestedTensorBlock(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): Identity()
              )
            )
          )
          (2): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-11): 12 x Identity()
              (12-17): 6 x NestedTensorBlock(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): Identity()
              )
            )
          )
          (3): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-17): 18 x Identity()
              (18-23): 6 x NestedTensorBlock(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): Identity()
              )
            )
          )
        )
        (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (head): Identity()
      )
    )
    (dino_head): FullyShardedDataParallel(
      (_fsdp_wrapped_module): DINOHead(
        (mlp): Sequential(
          (0): Linear(in_features=1024, out_features=2048, bias=True)
          (1): GELU(approximate='none')
          (2): Linear(in_features=2048, out_features=2048, bias=True)
          (3): GELU(approximate='none')
          (4): Linear(in_features=2048, out_features=256, bias=True)
        )
        (last_layer): Linear(in_features=256, out_features=65536, bias=False)
      )
    )
  )
)
I20240910 13:38:11 20100 dinov2 param_groups.py:54] chunked fsdp
I20240910 13:38:11 20100 dinov2 param_groups.py:87] cls_token: lr_multiplier: 0.0717897987691853, wd_multiplier: 1.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] pos_embed: lr_multiplier: 0.0717897987691853, wd_multiplier: 1.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] mask_token: lr_multiplier: 0.0717897987691853, wd_multiplier: 1.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] patch_embed.proj.weight: lr_multiplier: 0.01435795975383706, wd_multiplier: 1.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] patch_embed.proj.bias: lr_multiplier: 0.01435795975383706, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.0.0.norm1.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.0.0.norm1.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.0.0.attn.qkv.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 1.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.0.0.attn.qkv.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.0.0.attn.proj.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 1.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.0.0.attn.proj.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.0.0.ls1.gamma: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.0.0.norm2.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.0.0.norm2.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.0.0.mlp.fc1.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 1.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.0.0.mlp.fc1.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.0.0.mlp.fc2.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 1.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.0.0.mlp.fc2.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.0.0.ls2.gamma: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.0.1.norm1.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.0.1.norm1.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.0.1.attn.qkv.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 1.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.0.1.attn.qkv.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.0.1.attn.proj.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 1.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.0.1.attn.proj.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.0.1.ls1.gamma: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.0.1.norm2.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.0.1.norm2.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.0.1.mlp.fc1.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 1.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.0.1.mlp.fc1.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.0.1.mlp.fc2.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 1.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.0.1.mlp.fc2.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.0.1.ls2.gamma: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.0.2.norm1.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.0.2.norm1.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.0.2.attn.qkv.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 1.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.0.2.attn.qkv.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.0.2.attn.proj.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 1.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.0.2.attn.proj.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.0.2.ls1.gamma: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.0.2.norm2.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.0.2.norm2.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.0.2.mlp.fc1.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 1.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.0.2.mlp.fc1.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.0.2.mlp.fc2.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 1.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.0.2.mlp.fc2.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.0.2.ls2.gamma: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.0.3.norm1.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.0.3.norm1.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.0.3.attn.qkv.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 1.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.0.3.attn.qkv.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.0.3.attn.proj.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 1.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.0.3.attn.proj.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.0.3.ls1.gamma: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.0.3.norm2.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.0.3.norm2.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.0.3.mlp.fc1.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 1.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.0.3.mlp.fc1.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.0.3.mlp.fc2.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 1.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.0.3.mlp.fc2.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.0.3.ls2.gamma: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.0.4.norm1.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.0.4.norm1.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.0.4.attn.qkv.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 1.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.0.4.attn.qkv.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.0.4.attn.proj.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 1.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.0.4.attn.proj.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.0.4.ls1.gamma: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.0.4.norm2.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.0.4.norm2.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.0.4.mlp.fc1.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 1.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.0.4.mlp.fc1.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.0.4.mlp.fc2.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 1.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.0.4.mlp.fc2.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.0.4.ls2.gamma: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.0.5.norm1.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.0.5.norm1.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.0.5.attn.qkv.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 1.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.0.5.attn.qkv.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.0.5.attn.proj.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 1.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.0.5.attn.proj.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.0.5.ls1.gamma: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.0.5.norm2.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.0.5.norm2.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.0.5.mlp.fc1.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 1.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.0.5.mlp.fc1.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.0.5.mlp.fc2.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 1.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.0.5.mlp.fc2.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.0.5.ls2.gamma: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.1.6.norm1.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.1.6.norm1.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.1.6.attn.qkv.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 1.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.1.6.attn.qkv.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.1.6.attn.proj.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 1.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.1.6.attn.proj.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.1.6.ls1.gamma: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.1.6.norm2.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.1.6.norm2.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.1.6.mlp.fc1.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 1.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.1.6.mlp.fc1.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.1.6.mlp.fc2.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 1.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.1.6.mlp.fc2.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.1.6.ls2.gamma: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.1.7.norm1.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.1.7.norm1.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.1.7.attn.qkv.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 1.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.1.7.attn.qkv.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.1.7.attn.proj.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 1.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.1.7.attn.proj.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.1.7.ls1.gamma: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.1.7.norm2.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.1.7.norm2.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.1.7.mlp.fc1.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 1.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.1.7.mlp.fc1.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.1.7.mlp.fc2.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 1.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.1.7.mlp.fc2.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.1.7.ls2.gamma: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.1.8.norm1.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.1.8.norm1.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.1.8.attn.qkv.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 1.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.1.8.attn.qkv.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.1.8.attn.proj.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 1.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.1.8.attn.proj.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.1.8.ls1.gamma: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.1.8.norm2.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.1.8.norm2.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.1.8.mlp.fc1.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 1.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.1.8.mlp.fc1.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.1.8.mlp.fc2.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 1.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.1.8.mlp.fc2.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.1.8.ls2.gamma: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.1.9.norm1.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.1.9.norm1.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.1.9.attn.qkv.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 1.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.1.9.attn.qkv.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.1.9.attn.proj.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 1.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.1.9.attn.proj.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.1.9.ls1.gamma: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.1.9.norm2.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.1.9.norm2.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.1.9.mlp.fc1.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 1.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.1.9.mlp.fc1.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.1.9.mlp.fc2.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 1.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.1.9.mlp.fc2.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.1.9.ls2.gamma: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.1.10.norm1.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.1.10.norm1.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.1.10.attn.qkv.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 1.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.1.10.attn.qkv.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.1.10.attn.proj.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 1.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.1.10.attn.proj.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.1.10.ls1.gamma: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.1.10.norm2.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.1.10.norm2.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.1.10.mlp.fc1.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 1.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.1.10.mlp.fc1.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.1.10.mlp.fc2.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 1.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.1.10.mlp.fc2.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.1.10.ls2.gamma: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.1.11.norm1.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.1.11.norm1.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.1.11.attn.qkv.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 1.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.1.11.attn.qkv.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.1.11.attn.proj.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 1.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.1.11.attn.proj.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.1.11.ls1.gamma: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.1.11.norm2.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.1.11.norm2.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.1.11.mlp.fc1.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 1.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.1.11.mlp.fc1.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.1.11.mlp.fc2.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 1.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.1.11.mlp.fc2.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.1.11.ls2.gamma: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.2.12.norm1.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.2.12.norm1.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.2.12.attn.qkv.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 1.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.2.12.attn.qkv.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.2.12.attn.proj.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 1.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.2.12.attn.proj.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.2.12.ls1.gamma: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.2.12.norm2.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.2.12.norm2.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.2.12.mlp.fc1.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 1.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.2.12.mlp.fc1.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.2.12.mlp.fc2.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 1.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.2.12.mlp.fc2.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.2.12.ls2.gamma: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.2.13.norm1.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.2.13.norm1.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.2.13.attn.qkv.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 1.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.2.13.attn.qkv.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.2.13.attn.proj.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 1.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.2.13.attn.proj.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.2.13.ls1.gamma: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.2.13.norm2.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.2.13.norm2.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.2.13.mlp.fc1.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 1.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.2.13.mlp.fc1.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.2.13.mlp.fc2.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 1.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.2.13.mlp.fc2.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.2.13.ls2.gamma: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.2.14.norm1.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.2.14.norm1.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.2.14.attn.qkv.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 1.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.2.14.attn.qkv.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.2.14.attn.proj.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 1.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.2.14.attn.proj.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.2.14.ls1.gamma: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.2.14.norm2.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.2.14.norm2.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.2.14.mlp.fc1.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 1.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.2.14.mlp.fc1.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.2.14.mlp.fc2.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 1.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.2.14.mlp.fc2.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.2.14.ls2.gamma: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.2.15.norm1.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.2.15.norm1.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.2.15.attn.qkv.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 1.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.2.15.attn.qkv.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.2.15.attn.proj.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 1.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.2.15.attn.proj.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.2.15.ls1.gamma: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.2.15.norm2.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.2.15.norm2.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.2.15.mlp.fc1.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 1.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.2.15.mlp.fc1.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.2.15.mlp.fc2.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 1.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.2.15.mlp.fc2.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.2.15.ls2.gamma: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.2.16.norm1.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.2.16.norm1.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.2.16.attn.qkv.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 1.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.2.16.attn.qkv.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.2.16.attn.proj.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 1.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.2.16.attn.proj.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.2.16.ls1.gamma: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.2.16.norm2.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.2.16.norm2.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.2.16.mlp.fc1.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 1.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.2.16.mlp.fc1.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.2.16.mlp.fc2.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 1.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.2.16.mlp.fc2.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.2.16.ls2.gamma: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.2.17.norm1.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.2.17.norm1.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.2.17.attn.qkv.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 1.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.2.17.attn.qkv.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.2.17.attn.proj.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 1.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.2.17.attn.proj.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.2.17.ls1.gamma: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.2.17.norm2.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.2.17.norm2.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.2.17.mlp.fc1.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 1.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.2.17.mlp.fc1.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.2.17.mlp.fc2.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 1.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.2.17.mlp.fc2.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.2.17.ls2.gamma: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.3.18.norm1.weight: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.3.18.norm1.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.3.18.attn.qkv.weight: lr_multiplier: 0.531441, wd_multiplier: 1.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.3.18.attn.qkv.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.3.18.attn.proj.weight: lr_multiplier: 0.531441, wd_multiplier: 1.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.3.18.attn.proj.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.3.18.ls1.gamma: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.3.18.norm2.weight: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.3.18.norm2.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.3.18.mlp.fc1.weight: lr_multiplier: 0.531441, wd_multiplier: 1.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.3.18.mlp.fc1.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.3.18.mlp.fc2.weight: lr_multiplier: 0.531441, wd_multiplier: 1.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.3.18.mlp.fc2.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.3.18.ls2.gamma: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.3.19.norm1.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.3.19.norm1.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.3.19.attn.qkv.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 1.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.3.19.attn.qkv.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.3.19.attn.proj.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 1.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.3.19.attn.proj.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.3.19.ls1.gamma: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.3.19.norm2.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.3.19.norm2.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.3.19.mlp.fc1.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 1.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.3.19.mlp.fc1.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.3.19.mlp.fc2.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 1.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.3.19.mlp.fc2.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.3.19.ls2.gamma: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.3.20.norm1.weight: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.3.20.norm1.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.3.20.attn.qkv.weight: lr_multiplier: 0.6561, wd_multiplier: 1.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.3.20.attn.qkv.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.3.20.attn.proj.weight: lr_multiplier: 0.6561, wd_multiplier: 1.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.3.20.attn.proj.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.3.20.ls1.gamma: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.3.20.norm2.weight: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.3.20.norm2.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.3.20.mlp.fc1.weight: lr_multiplier: 0.6561, wd_multiplier: 1.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.3.20.mlp.fc1.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.3.20.mlp.fc2.weight: lr_multiplier: 0.6561, wd_multiplier: 1.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.3.20.mlp.fc2.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.3.20.ls2.gamma: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.3.21.norm1.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.3.21.norm1.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.3.21.attn.qkv.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 1.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.3.21.attn.qkv.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.3.21.attn.proj.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 1.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.3.21.attn.proj.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.3.21.ls1.gamma: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.3.21.norm2.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.3.21.norm2.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.3.21.mlp.fc1.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 1.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.3.21.mlp.fc1.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.3.21.mlp.fc2.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 1.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.3.21.mlp.fc2.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.3.21.ls2.gamma: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.3.22.norm1.weight: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.3.22.norm1.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.3.22.attn.qkv.weight: lr_multiplier: 0.81, wd_multiplier: 1.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.3.22.attn.qkv.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.3.22.attn.proj.weight: lr_multiplier: 0.81, wd_multiplier: 1.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.3.22.attn.proj.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.3.22.ls1.gamma: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.3.22.norm2.weight: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.3.22.norm2.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.3.22.mlp.fc1.weight: lr_multiplier: 0.81, wd_multiplier: 1.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.3.22.mlp.fc1.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.3.22.mlp.fc2.weight: lr_multiplier: 0.81, wd_multiplier: 1.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.3.22.mlp.fc2.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.3.22.ls2.gamma: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.3.23.norm1.weight: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.3.23.norm1.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.3.23.attn.qkv.weight: lr_multiplier: 0.9, wd_multiplier: 1.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.3.23.attn.qkv.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.3.23.attn.proj.weight: lr_multiplier: 0.9, wd_multiplier: 1.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.3.23.attn.proj.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.3.23.ls1.gamma: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.3.23.norm2.weight: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.3.23.norm2.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.3.23.mlp.fc1.weight: lr_multiplier: 0.9, wd_multiplier: 1.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.3.23.mlp.fc1.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.3.23.mlp.fc2.weight: lr_multiplier: 0.9, wd_multiplier: 1.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.3.23.mlp.fc2.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] blocks.3.23.ls2.gamma: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] norm.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] norm.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 ssl_meta_arch.py:380] fusing param groups
I20240910 13:38:11 20100 dinov2 param_groups.py:64] else code branch
I20240910 13:38:11 20100 dinov2 param_groups.py:87] mlp.0.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] mlp.0.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] mlp.2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] mlp.2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] mlp.4.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] mlp.4.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] last_layer.weight_g: lr_multiplier: 1.0, wd_multiplier: 1.0
I20240910 13:38:11 20100 dinov2 param_groups.py:87] last_layer.weight_v: lr_multiplier: 1.0, wd_multiplier: 1.0
I20240910 13:38:11 20100 dinov2 ssl_meta_arch.py:380] fusing param groups
I20240910 13:38:11 20100 dinov2 train.py:100] Schedulers ready.
I20240910 13:38:11 20100 dinov2 augmentations.py:34] ###################################
I20240910 13:38:11 20100 dinov2 augmentations.py:35] Using data augmentation parameters:
I20240910 13:38:11 20100 dinov2 augmentations.py:36] global_crops_scale: [0.32, 1.0]
I20240910 13:38:11 20100 dinov2 augmentations.py:37] local_crops_scale: [0.05, 0.32]
I20240910 13:38:11 20100 dinov2 augmentations.py:38] local_crops_number: 8
I20240910 13:38:11 20100 dinov2 augmentations.py:39] global_crops_size: 224
I20240910 13:38:11 20100 dinov2 augmentations.py:40] local_crops_size: 96
I20240910 13:38:11 20100 dinov2 augmentations.py:41] ###################################
I20240910 13:38:11 20100 dinov2 loaders.py:89] using dataset: "ImageNet:split=TRAIN:root=C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\datasets\ILSVRC\Data\CLS-LOC:extra=C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\datasets\ILSVRC\Data\CLS-LOC"
I20240910 13:38:11 20100 dinov2 loaders.py:94] # of dataset samples: 1,281,167
I20240910 13:38:11 20100 dinov2 loaders.py:127] sampler: sharded infinite
I20240910 13:38:11 20100 dinov2 loaders.py:211] using PyTorch data loader
I20240910 13:38:11 20100 dinov2 loaders.py:226] infinite data loader
I20240910 13:38:11 20100 dinov2 train.py:218] Starting training from iteration 0
W20240910 13:38:26 20100 xformers __init__.py:50] A matching Triton is not available, some optimizations will not be enabled.
Error caught was: No module named 'triton'
I20240910 13:43:06 1292 dinov2 config.py:59] git:
  sha: d43546fae6805ceb22618af8cf78469b61d913c7, status: has uncommitted changes, branch: main

I20240910 13:43:06 1292 dinov2 config.py:60] config_file: configs/train/vitl16_short.yaml
eval: 
eval_only: False
no_resume: False
opts: ['train.dataset_path=ImageNet:split=TRAIN:root=C:\\Users\\ISI_UTS\\Siladittya\\iclr2023\\inexpts\\datasets\\ILSVRC\\Data\\CLS-LOC:extra=C:\\Users\\ISI_UTS\\Siladittya\\iclr2023\\inexpts\\datasets\\ILSVRC\\Data\\CLS-LOC', 'train.output_dir=C:\\Users\\ISI_UTS\\Siladittya\\iclr2023\\inexpts\\dinov2_nosetup\\dinov2\\dinov2_nosetup\\dinov2']
output_dir: C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\dinov2_nosetup\dinov2\dinov2_nosetup\dinov2
I20240910 13:43:06 1292 dinov2 config.py:26] sqrt scaling learning rate; base: 0.004, new: 0.001
I20240910 13:43:06 1292 dinov2 config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
dino:
  loss_weight: 1.0
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
  koleo_loss_weight: 0.1
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  separate_head: false
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
train:
  batch_size_per_gpu: 64
  dataset_path: ImageNet:split=TRAIN:root=C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\datasets\ILSVRC\Data\CLS-LOC:extra=C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\datasets\ILSVRC\Data\CLS-LOC
  output_dir: C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\dinov2_nosetup\dinov2\dinov2_nosetup\dinov2
  saveckp_freq: 20
  seed: 0
  num_workers: 0
  OFFICIAL_EPOCH_LENGTH: 1250
  cache_dataset: true
  centering: centering
student:
  arch: vit_large
  patch_size: 16
  drop_path_rate: 0.3
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: mlp
  block_chunks: 4
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
  num_register_tokens: 0
  interpolate_antialias: false
  interpolate_offset: 0.1
teacher:
  momentum_teacher: 0.992
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 30
optim:
  epochs: 100
  weight_decay: 0.04
  weight_decay_end: 0.4
  base_lr: 0.004
  lr: 0.001
  warmup_epochs: 10
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 0.9
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 8
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 224
  local_crops_size: 96
evaluation:
  eval_period_iterations: 12500

I20240910 13:43:06 1292 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20240910 13:43:09 1292 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20240910 13:43:11 1292 dinov2 ssl_meta_arch.py:45] OPTIONS -- architecture : embed_dim: 1024
I20240910 13:43:11 1292 dinov2 ssl_meta_arch.py:60] OPTIONS -- DINO
I20240910 13:43:11 1292 dinov2 ssl_meta_arch.py:62] OPTIONS -- DINO -- loss_weight: 1.0
I20240910 13:43:11 1292 dinov2 ssl_meta_arch.py:63] OPTIONS -- DINO -- head_n_prototypes: 65536
I20240910 13:43:11 1292 dinov2 ssl_meta_arch.py:64] OPTIONS -- DINO -- head_bottleneck_dim: 256
I20240910 13:43:11 1292 dinov2 ssl_meta_arch.py:65] OPTIONS -- DINO -- head_hidden_dim: 2048
I20240910 13:43:11 1292 dinov2 ssl_meta_arch.py:77] OPTIONS -- DINO -- applying KOLEO regularization
I20240910 13:43:12 1292 dinov2 ssl_meta_arch.py:87] OPTIONS -- IBOT
I20240910 13:43:12 1292 dinov2 ssl_meta_arch.py:88] OPTIONS -- IBOT -- loss_weight: 1.0
I20240910 13:43:12 1292 dinov2 ssl_meta_arch.py:89] OPTIONS -- IBOT masking -- ibot_mask_ratio_tuple: [0.1, 0.5]
I20240910 13:43:12 1292 dinov2 ssl_meta_arch.py:90] OPTIONS -- IBOT masking -- ibot_mask_sample_probability: 0.5
I20240910 13:43:12 1292 dinov2 ssl_meta_arch.py:113] OPTIONS -- IBOT -- head shared with DINO
I20240910 13:43:12 1292 dinov2 ssl_meta_arch.py:123] Student and Teacher are built: they are both vit_large network.
I20240910 13:43:12 1292 dinov2 ssl_meta_arch.py:393] DISTRIBUTED FSDP -- preparing model for distributed training
W20240910 13:43:13 1292 py.warnings warnings.py:109] C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\lib\site-packages\torch\distributed\fsdp\_init_utils.py:295: UserWarning: FSDP is switching to use `NO_SHARD` instead of ShardingStrategy.SHARD_GRAD_OP since the world size is 1.
  warnings.warn(

I20240910 13:43:14 1292 dinov2 train.py:304] Model:
SSLMetaArch(
  (dino_loss): DINOLoss()
  (koleo_loss): KoLeoLoss(
    (pdist): PairwiseDistance()
  )
  (ibot_patch_loss): iBOTPatchLoss()
  (student): ModuleDict(
    (backbone): FullyShardedDataParallel(
      (_fsdp_wrapped_module): DinoVisionTransformer(
        (patch_embed): PatchEmbed(
          (proj): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))
          (norm): Identity()
        )
        (blocks): ModuleList(
          (0): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-5): 6 x NestedTensorBlock(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): DropPath()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): DropPath()
              )
            )
          )
          (1): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-5): 6 x Identity()
              (6-11): 6 x NestedTensorBlock(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): DropPath()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): DropPath()
              )
            )
          )
          (2): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-11): 12 x Identity()
              (12-17): 6 x NestedTensorBlock(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): DropPath()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): DropPath()
              )
            )
          )
          (3): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-17): 18 x Identity()
              (18-23): 6 x NestedTensorBlock(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): DropPath()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): DropPath()
              )
            )
          )
        )
        (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (head): Identity()
      )
    )
    (dino_head): FullyShardedDataParallel(
      (_fsdp_wrapped_module): DINOHead(
        (mlp): Sequential(
          (0): Linear(in_features=1024, out_features=2048, bias=True)
          (1): GELU(approximate='none')
          (2): Linear(in_features=2048, out_features=2048, bias=True)
          (3): GELU(approximate='none')
          (4): Linear(in_features=2048, out_features=256, bias=True)
        )
        (last_layer): Linear(in_features=256, out_features=65536, bias=False)
      )
    )
  )
  (teacher): ModuleDict(
    (backbone): FullyShardedDataParallel(
      (_fsdp_wrapped_module): DinoVisionTransformer(
        (patch_embed): PatchEmbed(
          (proj): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))
          (norm): Identity()
        )
        (blocks): ModuleList(
          (0): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-5): 6 x NestedTensorBlock(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): Identity()
              )
            )
          )
          (1): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-5): 6 x Identity()
              (6-11): 6 x NestedTensorBlock(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): Identity()
              )
            )
          )
          (2): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-11): 12 x Identity()
              (12-17): 6 x NestedTensorBlock(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): Identity()
              )
            )
          )
          (3): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-17): 18 x Identity()
              (18-23): 6 x NestedTensorBlock(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): Identity()
              )
            )
          )
        )
        (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (head): Identity()
      )
    )
    (dino_head): FullyShardedDataParallel(
      (_fsdp_wrapped_module): DINOHead(
        (mlp): Sequential(
          (0): Linear(in_features=1024, out_features=2048, bias=True)
          (1): GELU(approximate='none')
          (2): Linear(in_features=2048, out_features=2048, bias=True)
          (3): GELU(approximate='none')
          (4): Linear(in_features=2048, out_features=256, bias=True)
        )
        (last_layer): Linear(in_features=256, out_features=65536, bias=False)
      )
    )
  )
)
I20240910 13:43:14 1292 dinov2 param_groups.py:54] chunked fsdp
I20240910 13:43:14 1292 dinov2 param_groups.py:87] cls_token: lr_multiplier: 0.0717897987691853, wd_multiplier: 1.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] pos_embed: lr_multiplier: 0.0717897987691853, wd_multiplier: 1.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] mask_token: lr_multiplier: 0.0717897987691853, wd_multiplier: 1.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] patch_embed.proj.weight: lr_multiplier: 0.01435795975383706, wd_multiplier: 1.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] patch_embed.proj.bias: lr_multiplier: 0.01435795975383706, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.0.0.norm1.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.0.0.norm1.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.0.0.attn.qkv.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 1.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.0.0.attn.qkv.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.0.0.attn.proj.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 1.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.0.0.attn.proj.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.0.0.ls1.gamma: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.0.0.norm2.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.0.0.norm2.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.0.0.mlp.fc1.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 1.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.0.0.mlp.fc1.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.0.0.mlp.fc2.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 1.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.0.0.mlp.fc2.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.0.0.ls2.gamma: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.0.1.norm1.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.0.1.norm1.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.0.1.attn.qkv.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 1.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.0.1.attn.qkv.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.0.1.attn.proj.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 1.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.0.1.attn.proj.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.0.1.ls1.gamma: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.0.1.norm2.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.0.1.norm2.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.0.1.mlp.fc1.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 1.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.0.1.mlp.fc1.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.0.1.mlp.fc2.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 1.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.0.1.mlp.fc2.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.0.1.ls2.gamma: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.0.2.norm1.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.0.2.norm1.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.0.2.attn.qkv.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 1.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.0.2.attn.qkv.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.0.2.attn.proj.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 1.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.0.2.attn.proj.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.0.2.ls1.gamma: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.0.2.norm2.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.0.2.norm2.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.0.2.mlp.fc1.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 1.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.0.2.mlp.fc1.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.0.2.mlp.fc2.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 1.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.0.2.mlp.fc2.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.0.2.ls2.gamma: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.0.3.norm1.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.0.3.norm1.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.0.3.attn.qkv.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 1.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.0.3.attn.qkv.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.0.3.attn.proj.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 1.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.0.3.attn.proj.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.0.3.ls1.gamma: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.0.3.norm2.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.0.3.norm2.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.0.3.mlp.fc1.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 1.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.0.3.mlp.fc1.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.0.3.mlp.fc2.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 1.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.0.3.mlp.fc2.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.0.3.ls2.gamma: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.0.4.norm1.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.0.4.norm1.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.0.4.attn.qkv.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 1.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.0.4.attn.qkv.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.0.4.attn.proj.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 1.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.0.4.attn.proj.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.0.4.ls1.gamma: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.0.4.norm2.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.0.4.norm2.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.0.4.mlp.fc1.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 1.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.0.4.mlp.fc1.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.0.4.mlp.fc2.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 1.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.0.4.mlp.fc2.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.0.4.ls2.gamma: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.0.5.norm1.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.0.5.norm1.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.0.5.attn.qkv.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 1.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.0.5.attn.qkv.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.0.5.attn.proj.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 1.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.0.5.attn.proj.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.0.5.ls1.gamma: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.0.5.norm2.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.0.5.norm2.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.0.5.mlp.fc1.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 1.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.0.5.mlp.fc1.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.0.5.mlp.fc2.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 1.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.0.5.mlp.fc2.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.0.5.ls2.gamma: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.1.6.norm1.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.1.6.norm1.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.1.6.attn.qkv.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 1.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.1.6.attn.qkv.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.1.6.attn.proj.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 1.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.1.6.attn.proj.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.1.6.ls1.gamma: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.1.6.norm2.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.1.6.norm2.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.1.6.mlp.fc1.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 1.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.1.6.mlp.fc1.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.1.6.mlp.fc2.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 1.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.1.6.mlp.fc2.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.1.6.ls2.gamma: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.1.7.norm1.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.1.7.norm1.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.1.7.attn.qkv.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 1.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.1.7.attn.qkv.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.1.7.attn.proj.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 1.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.1.7.attn.proj.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.1.7.ls1.gamma: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.1.7.norm2.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.1.7.norm2.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.1.7.mlp.fc1.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 1.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.1.7.mlp.fc1.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.1.7.mlp.fc2.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 1.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.1.7.mlp.fc2.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.1.7.ls2.gamma: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.1.8.norm1.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.1.8.norm1.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.1.8.attn.qkv.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 1.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.1.8.attn.qkv.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.1.8.attn.proj.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 1.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.1.8.attn.proj.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.1.8.ls1.gamma: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.1.8.norm2.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.1.8.norm2.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.1.8.mlp.fc1.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 1.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.1.8.mlp.fc1.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.1.8.mlp.fc2.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 1.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.1.8.mlp.fc2.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.1.8.ls2.gamma: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.1.9.norm1.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.1.9.norm1.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.1.9.attn.qkv.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 1.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.1.9.attn.qkv.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.1.9.attn.proj.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 1.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.1.9.attn.proj.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.1.9.ls1.gamma: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.1.9.norm2.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.1.9.norm2.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.1.9.mlp.fc1.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 1.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.1.9.mlp.fc1.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.1.9.mlp.fc2.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 1.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.1.9.mlp.fc2.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.1.9.ls2.gamma: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.1.10.norm1.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.1.10.norm1.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.1.10.attn.qkv.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 1.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.1.10.attn.qkv.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.1.10.attn.proj.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 1.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.1.10.attn.proj.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.1.10.ls1.gamma: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.1.10.norm2.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.1.10.norm2.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.1.10.mlp.fc1.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 1.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.1.10.mlp.fc1.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.1.10.mlp.fc2.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 1.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.1.10.mlp.fc2.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.1.10.ls2.gamma: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.1.11.norm1.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.1.11.norm1.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.1.11.attn.qkv.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 1.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.1.11.attn.qkv.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.1.11.attn.proj.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 1.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.1.11.attn.proj.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.1.11.ls1.gamma: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.1.11.norm2.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.1.11.norm2.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.1.11.mlp.fc1.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 1.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.1.11.mlp.fc1.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.1.11.mlp.fc2.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 1.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.1.11.mlp.fc2.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.1.11.ls2.gamma: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.2.12.norm1.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.2.12.norm1.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.2.12.attn.qkv.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 1.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.2.12.attn.qkv.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.2.12.attn.proj.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 1.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.2.12.attn.proj.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.2.12.ls1.gamma: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.2.12.norm2.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.2.12.norm2.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.2.12.mlp.fc1.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 1.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.2.12.mlp.fc1.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.2.12.mlp.fc2.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 1.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.2.12.mlp.fc2.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.2.12.ls2.gamma: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.2.13.norm1.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.2.13.norm1.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.2.13.attn.qkv.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 1.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.2.13.attn.qkv.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.2.13.attn.proj.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 1.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.2.13.attn.proj.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.2.13.ls1.gamma: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.2.13.norm2.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.2.13.norm2.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.2.13.mlp.fc1.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 1.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.2.13.mlp.fc1.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.2.13.mlp.fc2.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 1.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.2.13.mlp.fc2.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.2.13.ls2.gamma: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.2.14.norm1.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.2.14.norm1.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.2.14.attn.qkv.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 1.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.2.14.attn.qkv.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.2.14.attn.proj.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 1.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.2.14.attn.proj.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.2.14.ls1.gamma: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.2.14.norm2.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.2.14.norm2.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.2.14.mlp.fc1.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 1.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.2.14.mlp.fc1.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.2.14.mlp.fc2.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 1.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.2.14.mlp.fc2.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.2.14.ls2.gamma: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.2.15.norm1.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.2.15.norm1.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.2.15.attn.qkv.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 1.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.2.15.attn.qkv.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.2.15.attn.proj.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 1.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.2.15.attn.proj.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.2.15.ls1.gamma: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.2.15.norm2.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.2.15.norm2.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.2.15.mlp.fc1.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 1.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.2.15.mlp.fc1.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.2.15.mlp.fc2.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 1.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.2.15.mlp.fc2.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.2.15.ls2.gamma: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.2.16.norm1.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.2.16.norm1.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.2.16.attn.qkv.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 1.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.2.16.attn.qkv.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.2.16.attn.proj.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 1.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.2.16.attn.proj.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.2.16.ls1.gamma: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.2.16.norm2.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.2.16.norm2.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.2.16.mlp.fc1.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 1.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.2.16.mlp.fc1.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.2.16.mlp.fc2.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 1.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.2.16.mlp.fc2.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.2.16.ls2.gamma: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.2.17.norm1.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.2.17.norm1.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.2.17.attn.qkv.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 1.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.2.17.attn.qkv.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.2.17.attn.proj.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 1.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.2.17.attn.proj.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.2.17.ls1.gamma: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.2.17.norm2.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.2.17.norm2.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.2.17.mlp.fc1.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 1.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.2.17.mlp.fc1.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.2.17.mlp.fc2.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 1.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.2.17.mlp.fc2.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.2.17.ls2.gamma: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.3.18.norm1.weight: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.3.18.norm1.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.3.18.attn.qkv.weight: lr_multiplier: 0.531441, wd_multiplier: 1.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.3.18.attn.qkv.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.3.18.attn.proj.weight: lr_multiplier: 0.531441, wd_multiplier: 1.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.3.18.attn.proj.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.3.18.ls1.gamma: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.3.18.norm2.weight: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.3.18.norm2.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.3.18.mlp.fc1.weight: lr_multiplier: 0.531441, wd_multiplier: 1.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.3.18.mlp.fc1.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.3.18.mlp.fc2.weight: lr_multiplier: 0.531441, wd_multiplier: 1.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.3.18.mlp.fc2.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.3.18.ls2.gamma: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.3.19.norm1.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.3.19.norm1.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.3.19.attn.qkv.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 1.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.3.19.attn.qkv.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.3.19.attn.proj.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 1.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.3.19.attn.proj.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.3.19.ls1.gamma: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.3.19.norm2.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.3.19.norm2.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.3.19.mlp.fc1.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 1.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.3.19.mlp.fc1.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.3.19.mlp.fc2.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 1.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.3.19.mlp.fc2.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.3.19.ls2.gamma: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.3.20.norm1.weight: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.3.20.norm1.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.3.20.attn.qkv.weight: lr_multiplier: 0.6561, wd_multiplier: 1.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.3.20.attn.qkv.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.3.20.attn.proj.weight: lr_multiplier: 0.6561, wd_multiplier: 1.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.3.20.attn.proj.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.3.20.ls1.gamma: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.3.20.norm2.weight: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.3.20.norm2.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.3.20.mlp.fc1.weight: lr_multiplier: 0.6561, wd_multiplier: 1.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.3.20.mlp.fc1.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.3.20.mlp.fc2.weight: lr_multiplier: 0.6561, wd_multiplier: 1.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.3.20.mlp.fc2.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.3.20.ls2.gamma: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.3.21.norm1.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.3.21.norm1.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.3.21.attn.qkv.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 1.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.3.21.attn.qkv.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.3.21.attn.proj.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 1.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.3.21.attn.proj.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.3.21.ls1.gamma: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.3.21.norm2.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.3.21.norm2.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.3.21.mlp.fc1.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 1.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.3.21.mlp.fc1.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.3.21.mlp.fc2.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 1.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.3.21.mlp.fc2.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.3.21.ls2.gamma: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.3.22.norm1.weight: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.3.22.norm1.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.3.22.attn.qkv.weight: lr_multiplier: 0.81, wd_multiplier: 1.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.3.22.attn.qkv.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.3.22.attn.proj.weight: lr_multiplier: 0.81, wd_multiplier: 1.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.3.22.attn.proj.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.3.22.ls1.gamma: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.3.22.norm2.weight: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.3.22.norm2.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.3.22.mlp.fc1.weight: lr_multiplier: 0.81, wd_multiplier: 1.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.3.22.mlp.fc1.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.3.22.mlp.fc2.weight: lr_multiplier: 0.81, wd_multiplier: 1.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.3.22.mlp.fc2.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.3.22.ls2.gamma: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.3.23.norm1.weight: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.3.23.norm1.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.3.23.attn.qkv.weight: lr_multiplier: 0.9, wd_multiplier: 1.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.3.23.attn.qkv.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.3.23.attn.proj.weight: lr_multiplier: 0.9, wd_multiplier: 1.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.3.23.attn.proj.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.3.23.ls1.gamma: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.3.23.norm2.weight: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.3.23.norm2.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.3.23.mlp.fc1.weight: lr_multiplier: 0.9, wd_multiplier: 1.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.3.23.mlp.fc1.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.3.23.mlp.fc2.weight: lr_multiplier: 0.9, wd_multiplier: 1.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.3.23.mlp.fc2.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] blocks.3.23.ls2.gamma: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] norm.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] norm.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 ssl_meta_arch.py:380] fusing param groups
I20240910 13:43:14 1292 dinov2 param_groups.py:64] else code branch
I20240910 13:43:14 1292 dinov2 param_groups.py:87] mlp.0.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] mlp.0.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] mlp.2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] mlp.2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] mlp.4.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] mlp.4.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] last_layer.weight_g: lr_multiplier: 1.0, wd_multiplier: 1.0
I20240910 13:43:14 1292 dinov2 param_groups.py:87] last_layer.weight_v: lr_multiplier: 1.0, wd_multiplier: 1.0
I20240910 13:43:14 1292 dinov2 ssl_meta_arch.py:380] fusing param groups
I20240910 13:43:14 1292 dinov2 train.py:100] Schedulers ready.
I20240910 13:43:14 1292 dinov2 augmentations.py:34] ###################################
I20240910 13:43:14 1292 dinov2 augmentations.py:35] Using data augmentation parameters:
I20240910 13:43:14 1292 dinov2 augmentations.py:36] global_crops_scale: [0.32, 1.0]
I20240910 13:43:14 1292 dinov2 augmentations.py:37] local_crops_scale: [0.05, 0.32]
I20240910 13:43:14 1292 dinov2 augmentations.py:38] local_crops_number: 8
I20240910 13:43:14 1292 dinov2 augmentations.py:39] global_crops_size: 224
I20240910 13:43:14 1292 dinov2 augmentations.py:40] local_crops_size: 96
I20240910 13:43:14 1292 dinov2 augmentations.py:41] ###################################
I20240910 13:43:14 1292 dinov2 loaders.py:89] using dataset: "ImageNet:split=TRAIN:root=C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\datasets\ILSVRC\Data\CLS-LOC:extra=C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\datasets\ILSVRC\Data\CLS-LOC"
I20240910 13:43:14 1292 dinov2 loaders.py:94] # of dataset samples: 1,281,167
I20240910 13:43:14 1292 dinov2 loaders.py:127] sampler: sharded infinite
I20240910 13:43:14 1292 dinov2 loaders.py:211] using PyTorch data loader
I20240910 13:43:14 1292 dinov2 loaders.py:226] infinite data loader
I20240910 13:43:14 1292 dinov2 train.py:218] Starting training from iteration 0
W20240910 13:43:26 1292 xformers __init__.py:50] A matching Triton is not available, some optimizations will not be enabled.
Error caught was: No module named 'triton'
I20240910 13:54:43 2580 dinov2 config.py:59] git:
  sha: d43546fae6805ceb22618af8cf78469b61d913c7, status: has uncommitted changes, branch: main

I20240910 13:54:43 2580 dinov2 config.py:60] config_file: configs/train/vitl16_short.yaml
eval: 
eval_only: False
no_resume: False
opts: ['train.dataset_path=ImageNet:split=TRAIN:root=C:\\Users\\ISI_UTS\\Siladittya\\iclr2023\\inexpts\\datasets\\ILSVRC\\Data\\CLS-LOC:extra=C:\\Users\\ISI_UTS\\Siladittya\\iclr2023\\inexpts\\datasets\\ILSVRC\\Data\\CLS-LOC', 'train.output_dir=C:\\Users\\ISI_UTS\\Siladittya\\iclr2023\\inexpts\\dinov2_nosetup\\dinov2\\dinov2_nosetup\\dinov2']
output_dir: C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\dinov2_nosetup\dinov2\dinov2_nosetup\dinov2
I20240910 13:54:43 2580 dinov2 config.py:26] sqrt scaling learning rate; base: 0.004, new: 0.001
I20240910 13:54:43 2580 dinov2 config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
dino:
  loss_weight: 1.0
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
  koleo_loss_weight: 0.1
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  separate_head: false
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
train:
  batch_size_per_gpu: 64
  dataset_path: ImageNet:split=TRAIN:root=C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\datasets\ILSVRC\Data\CLS-LOC:extra=C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\datasets\ILSVRC\Data\CLS-LOC
  output_dir: C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\dinov2_nosetup\dinov2\dinov2_nosetup\dinov2
  saveckp_freq: 20
  seed: 0
  num_workers: 0
  OFFICIAL_EPOCH_LENGTH: 1250
  cache_dataset: true
  centering: centering
student:
  arch: vit_large
  patch_size: 16
  drop_path_rate: 0.3
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: mlp
  block_chunks: 4
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
  num_register_tokens: 0
  interpolate_antialias: false
  interpolate_offset: 0.1
teacher:
  momentum_teacher: 0.992
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 30
optim:
  epochs: 100
  weight_decay: 0.04
  weight_decay_end: 0.4
  base_lr: 0.004
  lr: 0.001
  warmup_epochs: 10
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 0.9
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 8
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 224
  local_crops_size: 96
evaluation:
  eval_period_iterations: 12500

I20240910 13:54:43 2580 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20240910 13:54:46 2580 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20240910 13:54:48 2580 dinov2 ssl_meta_arch.py:45] OPTIONS -- architecture : embed_dim: 1024
I20240910 13:54:48 2580 dinov2 ssl_meta_arch.py:60] OPTIONS -- DINO
I20240910 13:54:48 2580 dinov2 ssl_meta_arch.py:62] OPTIONS -- DINO -- loss_weight: 1.0
I20240910 13:54:48 2580 dinov2 ssl_meta_arch.py:63] OPTIONS -- DINO -- head_n_prototypes: 65536
I20240910 13:54:48 2580 dinov2 ssl_meta_arch.py:64] OPTIONS -- DINO -- head_bottleneck_dim: 256
I20240910 13:54:48 2580 dinov2 ssl_meta_arch.py:65] OPTIONS -- DINO -- head_hidden_dim: 2048
I20240910 13:54:48 2580 dinov2 ssl_meta_arch.py:77] OPTIONS -- DINO -- applying KOLEO regularization
I20240910 13:54:48 2580 dinov2 ssl_meta_arch.py:87] OPTIONS -- IBOT
I20240910 13:54:48 2580 dinov2 ssl_meta_arch.py:88] OPTIONS -- IBOT -- loss_weight: 1.0
I20240910 13:54:48 2580 dinov2 ssl_meta_arch.py:89] OPTIONS -- IBOT masking -- ibot_mask_ratio_tuple: [0.1, 0.5]
I20240910 13:54:48 2580 dinov2 ssl_meta_arch.py:90] OPTIONS -- IBOT masking -- ibot_mask_sample_probability: 0.5
I20240910 13:54:48 2580 dinov2 ssl_meta_arch.py:113] OPTIONS -- IBOT -- head shared with DINO
I20240910 13:54:48 2580 dinov2 ssl_meta_arch.py:123] Student and Teacher are built: they are both vit_large network.
I20240910 13:54:49 2580 dinov2 ssl_meta_arch.py:393] DISTRIBUTED FSDP -- preparing model for distributed training
W20240910 13:54:50 2580 py.warnings warnings.py:109] C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\lib\site-packages\torch\distributed\fsdp\_init_utils.py:295: UserWarning: FSDP is switching to use `NO_SHARD` instead of ShardingStrategy.SHARD_GRAD_OP since the world size is 1.
  warnings.warn(

I20240910 13:54:50 2580 dinov2 train.py:304] Model:
SSLMetaArch(
  (dino_loss): DINOLoss()
  (koleo_loss): KoLeoLoss(
    (pdist): PairwiseDistance()
  )
  (ibot_patch_loss): iBOTPatchLoss()
  (student): ModuleDict(
    (backbone): FullyShardedDataParallel(
      (_fsdp_wrapped_module): DinoVisionTransformer(
        (patch_embed): PatchEmbed(
          (proj): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))
          (norm): Identity()
        )
        (blocks): ModuleList(
          (0): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-5): 6 x NestedTensorBlock(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): DropPath()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): DropPath()
              )
            )
          )
          (1): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-5): 6 x Identity()
              (6-11): 6 x NestedTensorBlock(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): DropPath()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): DropPath()
              )
            )
          )
          (2): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-11): 12 x Identity()
              (12-17): 6 x NestedTensorBlock(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): DropPath()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): DropPath()
              )
            )
          )
          (3): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-17): 18 x Identity()
              (18-23): 6 x NestedTensorBlock(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): DropPath()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): DropPath()
              )
            )
          )
        )
        (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (head): Identity()
      )
    )
    (dino_head): FullyShardedDataParallel(
      (_fsdp_wrapped_module): DINOHead(
        (mlp): Sequential(
          (0): Linear(in_features=1024, out_features=2048, bias=True)
          (1): GELU(approximate='none')
          (2): Linear(in_features=2048, out_features=2048, bias=True)
          (3): GELU(approximate='none')
          (4): Linear(in_features=2048, out_features=256, bias=True)
        )
        (last_layer): Linear(in_features=256, out_features=65536, bias=False)
      )
    )
  )
  (teacher): ModuleDict(
    (backbone): FullyShardedDataParallel(
      (_fsdp_wrapped_module): DinoVisionTransformer(
        (patch_embed): PatchEmbed(
          (proj): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))
          (norm): Identity()
        )
        (blocks): ModuleList(
          (0): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-5): 6 x NestedTensorBlock(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): Identity()
              )
            )
          )
          (1): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-5): 6 x Identity()
              (6-11): 6 x NestedTensorBlock(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): Identity()
              )
            )
          )
          (2): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-11): 12 x Identity()
              (12-17): 6 x NestedTensorBlock(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): Identity()
              )
            )
          )
          (3): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-17): 18 x Identity()
              (18-23): 6 x NestedTensorBlock(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): Identity()
              )
            )
          )
        )
        (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (head): Identity()
      )
    )
    (dino_head): FullyShardedDataParallel(
      (_fsdp_wrapped_module): DINOHead(
        (mlp): Sequential(
          (0): Linear(in_features=1024, out_features=2048, bias=True)
          (1): GELU(approximate='none')
          (2): Linear(in_features=2048, out_features=2048, bias=True)
          (3): GELU(approximate='none')
          (4): Linear(in_features=2048, out_features=256, bias=True)
        )
        (last_layer): Linear(in_features=256, out_features=65536, bias=False)
      )
    )
  )
)
I20240910 13:54:50 2580 dinov2 param_groups.py:54] chunked fsdp
I20240910 13:54:50 2580 dinov2 param_groups.py:87] cls_token: lr_multiplier: 0.0717897987691853, wd_multiplier: 1.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] pos_embed: lr_multiplier: 0.0717897987691853, wd_multiplier: 1.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] mask_token: lr_multiplier: 0.0717897987691853, wd_multiplier: 1.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] patch_embed.proj.weight: lr_multiplier: 0.01435795975383706, wd_multiplier: 1.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] patch_embed.proj.bias: lr_multiplier: 0.01435795975383706, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.0.0.norm1.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.0.0.norm1.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.0.0.attn.qkv.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 1.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.0.0.attn.qkv.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.0.0.attn.proj.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 1.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.0.0.attn.proj.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.0.0.ls1.gamma: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.0.0.norm2.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.0.0.norm2.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.0.0.mlp.fc1.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 1.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.0.0.mlp.fc1.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.0.0.mlp.fc2.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 1.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.0.0.mlp.fc2.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.0.0.ls2.gamma: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.0.1.norm1.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.0.1.norm1.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.0.1.attn.qkv.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 1.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.0.1.attn.qkv.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.0.1.attn.proj.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 1.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.0.1.attn.proj.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.0.1.ls1.gamma: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.0.1.norm2.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.0.1.norm2.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.0.1.mlp.fc1.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 1.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.0.1.mlp.fc1.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.0.1.mlp.fc2.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 1.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.0.1.mlp.fc2.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.0.1.ls2.gamma: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.0.2.norm1.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.0.2.norm1.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.0.2.attn.qkv.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 1.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.0.2.attn.qkv.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.0.2.attn.proj.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 1.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.0.2.attn.proj.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.0.2.ls1.gamma: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.0.2.norm2.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.0.2.norm2.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.0.2.mlp.fc1.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 1.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.0.2.mlp.fc1.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.0.2.mlp.fc2.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 1.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.0.2.mlp.fc2.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.0.2.ls2.gamma: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.0.3.norm1.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.0.3.norm1.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.0.3.attn.qkv.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 1.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.0.3.attn.qkv.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.0.3.attn.proj.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 1.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.0.3.attn.proj.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.0.3.ls1.gamma: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.0.3.norm2.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.0.3.norm2.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.0.3.mlp.fc1.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 1.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.0.3.mlp.fc1.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.0.3.mlp.fc2.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 1.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.0.3.mlp.fc2.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.0.3.ls2.gamma: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.0.4.norm1.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.0.4.norm1.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.0.4.attn.qkv.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 1.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.0.4.attn.qkv.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.0.4.attn.proj.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 1.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.0.4.attn.proj.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.0.4.ls1.gamma: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.0.4.norm2.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.0.4.norm2.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.0.4.mlp.fc1.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 1.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.0.4.mlp.fc1.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.0.4.mlp.fc2.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 1.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.0.4.mlp.fc2.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.0.4.ls2.gamma: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.0.5.norm1.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.0.5.norm1.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.0.5.attn.qkv.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 1.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.0.5.attn.qkv.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.0.5.attn.proj.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 1.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.0.5.attn.proj.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.0.5.ls1.gamma: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.0.5.norm2.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.0.5.norm2.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.0.5.mlp.fc1.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 1.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.0.5.mlp.fc1.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.0.5.mlp.fc2.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 1.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.0.5.mlp.fc2.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.0.5.ls2.gamma: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.1.6.norm1.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.1.6.norm1.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.1.6.attn.qkv.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 1.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.1.6.attn.qkv.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.1.6.attn.proj.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 1.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.1.6.attn.proj.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.1.6.ls1.gamma: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.1.6.norm2.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.1.6.norm2.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.1.6.mlp.fc1.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 1.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.1.6.mlp.fc1.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.1.6.mlp.fc2.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 1.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.1.6.mlp.fc2.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.1.6.ls2.gamma: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.1.7.norm1.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.1.7.norm1.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.1.7.attn.qkv.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 1.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.1.7.attn.qkv.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.1.7.attn.proj.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 1.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.1.7.attn.proj.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.1.7.ls1.gamma: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.1.7.norm2.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.1.7.norm2.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.1.7.mlp.fc1.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 1.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.1.7.mlp.fc1.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.1.7.mlp.fc2.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 1.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.1.7.mlp.fc2.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.1.7.ls2.gamma: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.1.8.norm1.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.1.8.norm1.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.1.8.attn.qkv.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 1.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.1.8.attn.qkv.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.1.8.attn.proj.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 1.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.1.8.attn.proj.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.1.8.ls1.gamma: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.1.8.norm2.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.1.8.norm2.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.1.8.mlp.fc1.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 1.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.1.8.mlp.fc1.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.1.8.mlp.fc2.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 1.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.1.8.mlp.fc2.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.1.8.ls2.gamma: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.1.9.norm1.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.1.9.norm1.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.1.9.attn.qkv.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 1.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.1.9.attn.qkv.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.1.9.attn.proj.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 1.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.1.9.attn.proj.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.1.9.ls1.gamma: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.1.9.norm2.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.1.9.norm2.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.1.9.mlp.fc1.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 1.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.1.9.mlp.fc1.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.1.9.mlp.fc2.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 1.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.1.9.mlp.fc2.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.1.9.ls2.gamma: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.1.10.norm1.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.1.10.norm1.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.1.10.attn.qkv.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 1.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.1.10.attn.qkv.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.1.10.attn.proj.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 1.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.1.10.attn.proj.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.1.10.ls1.gamma: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.1.10.norm2.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.1.10.norm2.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.1.10.mlp.fc1.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 1.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.1.10.mlp.fc1.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.1.10.mlp.fc2.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 1.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.1.10.mlp.fc2.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.1.10.ls2.gamma: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.1.11.norm1.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.1.11.norm1.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.1.11.attn.qkv.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 1.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.1.11.attn.qkv.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.1.11.attn.proj.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 1.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.1.11.attn.proj.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.1.11.ls1.gamma: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.1.11.norm2.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.1.11.norm2.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.1.11.mlp.fc1.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 1.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.1.11.mlp.fc1.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.1.11.mlp.fc2.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 1.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.1.11.mlp.fc2.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.1.11.ls2.gamma: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.2.12.norm1.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.2.12.norm1.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.2.12.attn.qkv.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 1.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.2.12.attn.qkv.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.2.12.attn.proj.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 1.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.2.12.attn.proj.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.2.12.ls1.gamma: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.2.12.norm2.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.2.12.norm2.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.2.12.mlp.fc1.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 1.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.2.12.mlp.fc1.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.2.12.mlp.fc2.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 1.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.2.12.mlp.fc2.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.2.12.ls2.gamma: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.2.13.norm1.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.2.13.norm1.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.2.13.attn.qkv.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 1.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.2.13.attn.qkv.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.2.13.attn.proj.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 1.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.2.13.attn.proj.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.2.13.ls1.gamma: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.2.13.norm2.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.2.13.norm2.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.2.13.mlp.fc1.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 1.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.2.13.mlp.fc1.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.2.13.mlp.fc2.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 1.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.2.13.mlp.fc2.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.2.13.ls2.gamma: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.2.14.norm1.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.2.14.norm1.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.2.14.attn.qkv.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 1.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.2.14.attn.qkv.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.2.14.attn.proj.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 1.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.2.14.attn.proj.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.2.14.ls1.gamma: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.2.14.norm2.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.2.14.norm2.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.2.14.mlp.fc1.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 1.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.2.14.mlp.fc1.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.2.14.mlp.fc2.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 1.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.2.14.mlp.fc2.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.2.14.ls2.gamma: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.2.15.norm1.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.2.15.norm1.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.2.15.attn.qkv.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 1.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.2.15.attn.qkv.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.2.15.attn.proj.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 1.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.2.15.attn.proj.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.2.15.ls1.gamma: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.2.15.norm2.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.2.15.norm2.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.2.15.mlp.fc1.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 1.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.2.15.mlp.fc1.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.2.15.mlp.fc2.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 1.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.2.15.mlp.fc2.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.2.15.ls2.gamma: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.2.16.norm1.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.2.16.norm1.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.2.16.attn.qkv.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 1.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.2.16.attn.qkv.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.2.16.attn.proj.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 1.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.2.16.attn.proj.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.2.16.ls1.gamma: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.2.16.norm2.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.2.16.norm2.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.2.16.mlp.fc1.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 1.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.2.16.mlp.fc1.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.2.16.mlp.fc2.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 1.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.2.16.mlp.fc2.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.2.16.ls2.gamma: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.2.17.norm1.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.2.17.norm1.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.2.17.attn.qkv.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 1.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.2.17.attn.qkv.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.2.17.attn.proj.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 1.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.2.17.attn.proj.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.2.17.ls1.gamma: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.2.17.norm2.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.2.17.norm2.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.2.17.mlp.fc1.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 1.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.2.17.mlp.fc1.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.2.17.mlp.fc2.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 1.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.2.17.mlp.fc2.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.2.17.ls2.gamma: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.3.18.norm1.weight: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.3.18.norm1.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.3.18.attn.qkv.weight: lr_multiplier: 0.531441, wd_multiplier: 1.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.3.18.attn.qkv.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.3.18.attn.proj.weight: lr_multiplier: 0.531441, wd_multiplier: 1.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.3.18.attn.proj.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.3.18.ls1.gamma: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.3.18.norm2.weight: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.3.18.norm2.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.3.18.mlp.fc1.weight: lr_multiplier: 0.531441, wd_multiplier: 1.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.3.18.mlp.fc1.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.3.18.mlp.fc2.weight: lr_multiplier: 0.531441, wd_multiplier: 1.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.3.18.mlp.fc2.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.3.18.ls2.gamma: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.3.19.norm1.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.3.19.norm1.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.3.19.attn.qkv.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 1.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.3.19.attn.qkv.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.3.19.attn.proj.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 1.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.3.19.attn.proj.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.3.19.ls1.gamma: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.3.19.norm2.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.3.19.norm2.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.3.19.mlp.fc1.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 1.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.3.19.mlp.fc1.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.3.19.mlp.fc2.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 1.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.3.19.mlp.fc2.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.3.19.ls2.gamma: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.3.20.norm1.weight: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.3.20.norm1.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.3.20.attn.qkv.weight: lr_multiplier: 0.6561, wd_multiplier: 1.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.3.20.attn.qkv.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.3.20.attn.proj.weight: lr_multiplier: 0.6561, wd_multiplier: 1.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.3.20.attn.proj.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.3.20.ls1.gamma: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.3.20.norm2.weight: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.3.20.norm2.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.3.20.mlp.fc1.weight: lr_multiplier: 0.6561, wd_multiplier: 1.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.3.20.mlp.fc1.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.3.20.mlp.fc2.weight: lr_multiplier: 0.6561, wd_multiplier: 1.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.3.20.mlp.fc2.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.3.20.ls2.gamma: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.3.21.norm1.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.3.21.norm1.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.3.21.attn.qkv.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 1.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.3.21.attn.qkv.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.3.21.attn.proj.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 1.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.3.21.attn.proj.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.3.21.ls1.gamma: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.3.21.norm2.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.3.21.norm2.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.3.21.mlp.fc1.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 1.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.3.21.mlp.fc1.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.3.21.mlp.fc2.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 1.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.3.21.mlp.fc2.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.3.21.ls2.gamma: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.3.22.norm1.weight: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.3.22.norm1.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.3.22.attn.qkv.weight: lr_multiplier: 0.81, wd_multiplier: 1.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.3.22.attn.qkv.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.3.22.attn.proj.weight: lr_multiplier: 0.81, wd_multiplier: 1.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.3.22.attn.proj.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.3.22.ls1.gamma: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.3.22.norm2.weight: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.3.22.norm2.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.3.22.mlp.fc1.weight: lr_multiplier: 0.81, wd_multiplier: 1.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.3.22.mlp.fc1.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.3.22.mlp.fc2.weight: lr_multiplier: 0.81, wd_multiplier: 1.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.3.22.mlp.fc2.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.3.22.ls2.gamma: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.3.23.norm1.weight: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.3.23.norm1.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.3.23.attn.qkv.weight: lr_multiplier: 0.9, wd_multiplier: 1.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.3.23.attn.qkv.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.3.23.attn.proj.weight: lr_multiplier: 0.9, wd_multiplier: 1.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.3.23.attn.proj.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.3.23.ls1.gamma: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.3.23.norm2.weight: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.3.23.norm2.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.3.23.mlp.fc1.weight: lr_multiplier: 0.9, wd_multiplier: 1.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.3.23.mlp.fc1.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.3.23.mlp.fc2.weight: lr_multiplier: 0.9, wd_multiplier: 1.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.3.23.mlp.fc2.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] blocks.3.23.ls2.gamma: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] norm.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] norm.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 ssl_meta_arch.py:380] fusing param groups
I20240910 13:54:50 2580 dinov2 param_groups.py:64] else code branch
I20240910 13:54:50 2580 dinov2 param_groups.py:87] mlp.0.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] mlp.0.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] mlp.2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] mlp.2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] mlp.4.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] mlp.4.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] last_layer.weight_g: lr_multiplier: 1.0, wd_multiplier: 1.0
I20240910 13:54:50 2580 dinov2 param_groups.py:87] last_layer.weight_v: lr_multiplier: 1.0, wd_multiplier: 1.0
I20240910 13:54:50 2580 dinov2 ssl_meta_arch.py:380] fusing param groups
I20240910 13:54:50 2580 dinov2 train.py:100] Schedulers ready.
I20240910 13:54:50 2580 dinov2 augmentations.py:34] ###################################
I20240910 13:54:50 2580 dinov2 augmentations.py:35] Using data augmentation parameters:
I20240910 13:54:50 2580 dinov2 augmentations.py:36] global_crops_scale: [0.32, 1.0]
I20240910 13:54:50 2580 dinov2 augmentations.py:37] local_crops_scale: [0.05, 0.32]
I20240910 13:54:50 2580 dinov2 augmentations.py:38] local_crops_number: 8
I20240910 13:54:50 2580 dinov2 augmentations.py:39] global_crops_size: 224
I20240910 13:54:50 2580 dinov2 augmentations.py:40] local_crops_size: 96
I20240910 13:54:50 2580 dinov2 augmentations.py:41] ###################################
I20240910 13:54:50 2580 dinov2 loaders.py:89] using dataset: "ImageNet:split=TRAIN:root=C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\datasets\ILSVRC\Data\CLS-LOC:extra=C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\datasets\ILSVRC\Data\CLS-LOC"
I20240910 13:54:50 2580 dinov2 loaders.py:94] # of dataset samples: 1,281,167
I20240910 13:54:50 2580 dinov2 loaders.py:127] sampler: sharded infinite
I20240910 13:54:50 2580 dinov2 loaders.py:211] using PyTorch data loader
I20240910 13:54:50 2580 dinov2 loaders.py:226] infinite data loader
I20240910 13:54:50 2580 dinov2 train.py:218] Starting training from iteration 0
W20240910 13:55:02 2580 xformers __init__.py:50] A matching Triton is not available, some optimizations will not be enabled.
Error caught was: No module named 'triton'
I20240910 13:55:34 3356 dinov2 config.py:59] git:
  sha: d43546fae6805ceb22618af8cf78469b61d913c7, status: has uncommitted changes, branch: main

I20240910 13:55:34 3356 dinov2 config.py:60] config_file: configs/train/vitl16_short.yaml
eval: 
eval_only: False
no_resume: False
opts: ['train.dataset_path=ImageNet:split=TRAIN:root=C:\\Users\\ISI_UTS\\Siladittya\\iclr2023\\inexpts\\datasets\\ILSVRC\\Data\\CLS-LOC:extra=C:\\Users\\ISI_UTS\\Siladittya\\iclr2023\\inexpts\\datasets\\ILSVRC\\Data\\CLS-LOC', 'train.output_dir=C:\\Users\\ISI_UTS\\Siladittya\\iclr2023\\inexpts\\dinov2_nosetup\\dinov2\\dinov2_nosetup\\dinov2']
output_dir: C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\dinov2_nosetup\dinov2\dinov2_nosetup\dinov2
I20240910 13:55:34 3356 dinov2 config.py:26] sqrt scaling learning rate; base: 0.004, new: 0.001
I20240910 13:55:34 3356 dinov2 config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
dino:
  loss_weight: 1.0
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
  koleo_loss_weight: 0.1
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  separate_head: false
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
train:
  batch_size_per_gpu: 64
  dataset_path: ImageNet:split=TRAIN:root=C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\datasets\ILSVRC\Data\CLS-LOC:extra=C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\datasets\ILSVRC\Data\CLS-LOC
  output_dir: C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\dinov2_nosetup\dinov2\dinov2_nosetup\dinov2
  saveckp_freq: 20
  seed: 0
  num_workers: 0
  OFFICIAL_EPOCH_LENGTH: 1250
  cache_dataset: true
  centering: centering
student:
  arch: vit_large
  patch_size: 16
  drop_path_rate: 0.3
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: mlp
  block_chunks: 4
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
  num_register_tokens: 0
  interpolate_antialias: false
  interpolate_offset: 0.1
teacher:
  momentum_teacher: 0.992
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 30
optim:
  epochs: 100
  weight_decay: 0.04
  weight_decay_end: 0.4
  base_lr: 0.004
  lr: 0.001
  warmup_epochs: 10
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 0.9
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 8
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 224
  local_crops_size: 96
evaluation:
  eval_period_iterations: 12500

I20240910 13:55:34 3356 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20240910 13:55:37 3356 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20240910 13:55:39 3356 dinov2 ssl_meta_arch.py:45] OPTIONS -- architecture : embed_dim: 1024
I20240910 13:55:39 3356 dinov2 ssl_meta_arch.py:60] OPTIONS -- DINO
I20240910 13:55:39 3356 dinov2 ssl_meta_arch.py:62] OPTIONS -- DINO -- loss_weight: 1.0
I20240910 13:55:39 3356 dinov2 ssl_meta_arch.py:63] OPTIONS -- DINO -- head_n_prototypes: 65536
I20240910 13:55:39 3356 dinov2 ssl_meta_arch.py:64] OPTIONS -- DINO -- head_bottleneck_dim: 256
I20240910 13:55:39 3356 dinov2 ssl_meta_arch.py:65] OPTIONS -- DINO -- head_hidden_dim: 2048
I20240910 13:55:39 3356 dinov2 ssl_meta_arch.py:77] OPTIONS -- DINO -- applying KOLEO regularization
I20240910 13:55:40 3356 dinov2 ssl_meta_arch.py:87] OPTIONS -- IBOT
I20240910 13:55:40 3356 dinov2 ssl_meta_arch.py:88] OPTIONS -- IBOT -- loss_weight: 1.0
I20240910 13:55:40 3356 dinov2 ssl_meta_arch.py:89] OPTIONS -- IBOT masking -- ibot_mask_ratio_tuple: [0.1, 0.5]
I20240910 13:55:40 3356 dinov2 ssl_meta_arch.py:90] OPTIONS -- IBOT masking -- ibot_mask_sample_probability: 0.5
I20240910 13:55:40 3356 dinov2 ssl_meta_arch.py:113] OPTIONS -- IBOT -- head shared with DINO
I20240910 13:55:40 3356 dinov2 ssl_meta_arch.py:123] Student and Teacher are built: they are both vit_large network.
I20240910 13:55:41 3356 dinov2 ssl_meta_arch.py:393] DISTRIBUTED FSDP -- preparing model for distributed training
W20240910 13:55:41 3356 py.warnings warnings.py:109] C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\lib\site-packages\torch\distributed\fsdp\_init_utils.py:295: UserWarning: FSDP is switching to use `NO_SHARD` instead of ShardingStrategy.SHARD_GRAD_OP since the world size is 1.
  warnings.warn(

I20240910 13:55:42 3356 dinov2 train.py:304] Model:
SSLMetaArch(
  (dino_loss): DINOLoss()
  (koleo_loss): KoLeoLoss(
    (pdist): PairwiseDistance()
  )
  (ibot_patch_loss): iBOTPatchLoss()
  (student): ModuleDict(
    (backbone): FullyShardedDataParallel(
      (_fsdp_wrapped_module): DinoVisionTransformer(
        (patch_embed): PatchEmbed(
          (proj): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))
          (norm): Identity()
        )
        (blocks): ModuleList(
          (0): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-5): 6 x NestedTensorBlock(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): DropPath()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): DropPath()
              )
            )
          )
          (1): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-5): 6 x Identity()
              (6-11): 6 x NestedTensorBlock(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): DropPath()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): DropPath()
              )
            )
          )
          (2): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-11): 12 x Identity()
              (12-17): 6 x NestedTensorBlock(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): DropPath()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): DropPath()
              )
            )
          )
          (3): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-17): 18 x Identity()
              (18-23): 6 x NestedTensorBlock(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): DropPath()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): DropPath()
              )
            )
          )
        )
        (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (head): Identity()
      )
    )
    (dino_head): FullyShardedDataParallel(
      (_fsdp_wrapped_module): DINOHead(
        (mlp): Sequential(
          (0): Linear(in_features=1024, out_features=2048, bias=True)
          (1): GELU(approximate='none')
          (2): Linear(in_features=2048, out_features=2048, bias=True)
          (3): GELU(approximate='none')
          (4): Linear(in_features=2048, out_features=256, bias=True)
        )
        (last_layer): Linear(in_features=256, out_features=65536, bias=False)
      )
    )
  )
  (teacher): ModuleDict(
    (backbone): FullyShardedDataParallel(
      (_fsdp_wrapped_module): DinoVisionTransformer(
        (patch_embed): PatchEmbed(
          (proj): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))
          (norm): Identity()
        )
        (blocks): ModuleList(
          (0): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-5): 6 x NestedTensorBlock(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): Identity()
              )
            )
          )
          (1): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-5): 6 x Identity()
              (6-11): 6 x NestedTensorBlock(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): Identity()
              )
            )
          )
          (2): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-11): 12 x Identity()
              (12-17): 6 x NestedTensorBlock(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): Identity()
              )
            )
          )
          (3): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-17): 18 x Identity()
              (18-23): 6 x NestedTensorBlock(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): Identity()
              )
            )
          )
        )
        (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (head): Identity()
      )
    )
    (dino_head): FullyShardedDataParallel(
      (_fsdp_wrapped_module): DINOHead(
        (mlp): Sequential(
          (0): Linear(in_features=1024, out_features=2048, bias=True)
          (1): GELU(approximate='none')
          (2): Linear(in_features=2048, out_features=2048, bias=True)
          (3): GELU(approximate='none')
          (4): Linear(in_features=2048, out_features=256, bias=True)
        )
        (last_layer): Linear(in_features=256, out_features=65536, bias=False)
      )
    )
  )
)
I20240910 13:55:42 3356 dinov2 param_groups.py:54] chunked fsdp
I20240910 13:55:42 3356 dinov2 param_groups.py:87] cls_token: lr_multiplier: 0.0717897987691853, wd_multiplier: 1.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] pos_embed: lr_multiplier: 0.0717897987691853, wd_multiplier: 1.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] mask_token: lr_multiplier: 0.0717897987691853, wd_multiplier: 1.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] patch_embed.proj.weight: lr_multiplier: 0.01435795975383706, wd_multiplier: 1.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] patch_embed.proj.bias: lr_multiplier: 0.01435795975383706, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.0.0.norm1.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.0.0.norm1.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.0.0.attn.qkv.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 1.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.0.0.attn.qkv.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.0.0.attn.proj.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 1.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.0.0.attn.proj.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.0.0.ls1.gamma: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.0.0.norm2.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.0.0.norm2.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.0.0.mlp.fc1.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 1.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.0.0.mlp.fc1.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.0.0.mlp.fc2.weight: lr_multiplier: 0.07976644307687256, wd_multiplier: 1.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.0.0.mlp.fc2.bias: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.0.0.ls2.gamma: lr_multiplier: 0.07976644307687256, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.0.1.norm1.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.0.1.norm1.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.0.1.attn.qkv.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 1.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.0.1.attn.qkv.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.0.1.attn.proj.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 1.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.0.1.attn.proj.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.0.1.ls1.gamma: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.0.1.norm2.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.0.1.norm2.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.0.1.mlp.fc1.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 1.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.0.1.mlp.fc1.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.0.1.mlp.fc2.weight: lr_multiplier: 0.08862938119652507, wd_multiplier: 1.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.0.1.mlp.fc2.bias: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.0.1.ls2.gamma: lr_multiplier: 0.08862938119652507, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.0.2.norm1.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.0.2.norm1.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.0.2.attn.qkv.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 1.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.0.2.attn.qkv.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.0.2.attn.proj.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 1.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.0.2.attn.proj.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.0.2.ls1.gamma: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.0.2.norm2.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.0.2.norm2.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.0.2.mlp.fc1.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 1.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.0.2.mlp.fc1.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.0.2.mlp.fc2.weight: lr_multiplier: 0.09847709021836118, wd_multiplier: 1.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.0.2.mlp.fc2.bias: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.0.2.ls2.gamma: lr_multiplier: 0.09847709021836118, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.0.3.norm1.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.0.3.norm1.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.0.3.attn.qkv.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 1.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.0.3.attn.qkv.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.0.3.attn.proj.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 1.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.0.3.attn.proj.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.0.3.ls1.gamma: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.0.3.norm2.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.0.3.norm2.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.0.3.mlp.fc1.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 1.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.0.3.mlp.fc1.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.0.3.mlp.fc2.weight: lr_multiplier: 0.10941898913151242, wd_multiplier: 1.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.0.3.mlp.fc2.bias: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.0.3.ls2.gamma: lr_multiplier: 0.10941898913151242, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.0.4.norm1.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.0.4.norm1.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.0.4.attn.qkv.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 1.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.0.4.attn.qkv.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.0.4.attn.proj.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 1.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.0.4.attn.proj.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.0.4.ls1.gamma: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.0.4.norm2.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.0.4.norm2.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.0.4.mlp.fc1.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 1.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.0.4.mlp.fc1.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.0.4.mlp.fc2.weight: lr_multiplier: 0.12157665459056935, wd_multiplier: 1.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.0.4.mlp.fc2.bias: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.0.4.ls2.gamma: lr_multiplier: 0.12157665459056935, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.0.5.norm1.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.0.5.norm1.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.0.5.attn.qkv.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 1.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.0.5.attn.qkv.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.0.5.attn.proj.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 1.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.0.5.attn.proj.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.0.5.ls1.gamma: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.0.5.norm2.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.0.5.norm2.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.0.5.mlp.fc1.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 1.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.0.5.mlp.fc1.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.0.5.mlp.fc2.weight: lr_multiplier: 0.13508517176729928, wd_multiplier: 1.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.0.5.mlp.fc2.bias: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.0.5.ls2.gamma: lr_multiplier: 0.13508517176729928, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.1.6.norm1.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.1.6.norm1.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.1.6.attn.qkv.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 1.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.1.6.attn.qkv.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.1.6.attn.proj.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 1.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.1.6.attn.proj.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.1.6.ls1.gamma: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.1.6.norm2.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.1.6.norm2.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.1.6.mlp.fc1.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 1.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.1.6.mlp.fc1.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.1.6.mlp.fc2.weight: lr_multiplier: 0.15009463529699918, wd_multiplier: 1.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.1.6.mlp.fc2.bias: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.1.6.ls2.gamma: lr_multiplier: 0.15009463529699918, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.1.7.norm1.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.1.7.norm1.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.1.7.attn.qkv.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 1.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.1.7.attn.qkv.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.1.7.attn.proj.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 1.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.1.7.attn.proj.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.1.7.ls1.gamma: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.1.7.norm2.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.1.7.norm2.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.1.7.mlp.fc1.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 1.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.1.7.mlp.fc1.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.1.7.mlp.fc2.weight: lr_multiplier: 0.16677181699666577, wd_multiplier: 1.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.1.7.mlp.fc2.bias: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.1.7.ls2.gamma: lr_multiplier: 0.16677181699666577, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.1.8.norm1.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.1.8.norm1.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.1.8.attn.qkv.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 1.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.1.8.attn.qkv.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.1.8.attn.proj.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 1.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.1.8.attn.proj.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.1.8.ls1.gamma: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.1.8.norm2.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.1.8.norm2.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.1.8.mlp.fc1.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 1.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.1.8.mlp.fc1.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.1.8.mlp.fc2.weight: lr_multiplier: 0.18530201888518416, wd_multiplier: 1.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.1.8.mlp.fc2.bias: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.1.8.ls2.gamma: lr_multiplier: 0.18530201888518416, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.1.9.norm1.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.1.9.norm1.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.1.9.attn.qkv.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 1.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.1.9.attn.qkv.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.1.9.attn.proj.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 1.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.1.9.attn.proj.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.1.9.ls1.gamma: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.1.9.norm2.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.1.9.norm2.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.1.9.mlp.fc1.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 1.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.1.9.mlp.fc1.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.1.9.mlp.fc2.weight: lr_multiplier: 0.20589113209464907, wd_multiplier: 1.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.1.9.mlp.fc2.bias: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.1.9.ls2.gamma: lr_multiplier: 0.20589113209464907, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.1.10.norm1.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.1.10.norm1.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.1.10.attn.qkv.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 1.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.1.10.attn.qkv.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.1.10.attn.proj.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 1.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.1.10.attn.proj.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.1.10.ls1.gamma: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.1.10.norm2.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.1.10.norm2.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.1.10.mlp.fc1.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 1.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.1.10.mlp.fc1.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.1.10.mlp.fc2.weight: lr_multiplier: 0.2287679245496101, wd_multiplier: 1.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.1.10.mlp.fc2.bias: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.1.10.ls2.gamma: lr_multiplier: 0.2287679245496101, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.1.11.norm1.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.1.11.norm1.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.1.11.attn.qkv.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 1.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.1.11.attn.qkv.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.1.11.attn.proj.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 1.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.1.11.attn.proj.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.1.11.ls1.gamma: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.1.11.norm2.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.1.11.norm2.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.1.11.mlp.fc1.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 1.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.1.11.mlp.fc1.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.1.11.mlp.fc2.weight: lr_multiplier: 0.2541865828329001, wd_multiplier: 1.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.1.11.mlp.fc2.bias: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.1.11.ls2.gamma: lr_multiplier: 0.2541865828329001, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.2.12.norm1.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.2.12.norm1.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.2.12.attn.qkv.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 1.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.2.12.attn.qkv.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.2.12.attn.proj.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 1.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.2.12.attn.proj.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.2.12.ls1.gamma: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.2.12.norm2.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.2.12.norm2.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.2.12.mlp.fc1.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 1.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.2.12.mlp.fc1.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.2.12.mlp.fc2.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 1.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.2.12.mlp.fc2.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.2.12.ls2.gamma: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.2.13.norm1.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.2.13.norm1.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.2.13.attn.qkv.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 1.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.2.13.attn.qkv.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.2.13.attn.proj.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 1.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.2.13.attn.proj.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.2.13.ls1.gamma: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.2.13.norm2.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.2.13.norm2.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.2.13.mlp.fc1.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 1.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.2.13.mlp.fc1.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.2.13.mlp.fc2.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 1.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.2.13.mlp.fc2.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.2.13.ls2.gamma: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.2.14.norm1.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.2.14.norm1.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.2.14.attn.qkv.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 1.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.2.14.attn.qkv.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.2.14.attn.proj.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 1.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.2.14.attn.proj.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.2.14.ls1.gamma: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.2.14.norm2.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.2.14.norm2.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.2.14.mlp.fc1.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 1.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.2.14.mlp.fc1.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.2.14.mlp.fc2.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 1.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.2.14.mlp.fc2.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.2.14.ls2.gamma: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.2.15.norm1.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.2.15.norm1.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.2.15.attn.qkv.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 1.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.2.15.attn.qkv.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.2.15.attn.proj.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 1.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.2.15.attn.proj.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.2.15.ls1.gamma: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.2.15.norm2.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.2.15.norm2.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.2.15.mlp.fc1.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 1.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.2.15.mlp.fc1.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.2.15.mlp.fc2.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 1.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.2.15.mlp.fc2.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.2.15.ls2.gamma: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.2.16.norm1.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.2.16.norm1.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.2.16.attn.qkv.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 1.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.2.16.attn.qkv.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.2.16.attn.proj.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 1.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.2.16.attn.proj.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.2.16.ls1.gamma: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.2.16.norm2.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.2.16.norm2.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.2.16.mlp.fc1.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 1.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.2.16.mlp.fc1.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.2.16.mlp.fc2.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 1.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.2.16.mlp.fc2.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.2.16.ls2.gamma: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.2.17.norm1.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.2.17.norm1.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.2.17.attn.qkv.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 1.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.2.17.attn.qkv.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.2.17.attn.proj.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 1.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.2.17.attn.proj.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.2.17.ls1.gamma: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.2.17.norm2.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.2.17.norm2.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.2.17.mlp.fc1.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 1.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.2.17.mlp.fc1.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.2.17.mlp.fc2.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 1.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.2.17.mlp.fc2.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.2.17.ls2.gamma: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.3.18.norm1.weight: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.3.18.norm1.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.3.18.attn.qkv.weight: lr_multiplier: 0.531441, wd_multiplier: 1.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.3.18.attn.qkv.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.3.18.attn.proj.weight: lr_multiplier: 0.531441, wd_multiplier: 1.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.3.18.attn.proj.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.3.18.ls1.gamma: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.3.18.norm2.weight: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.3.18.norm2.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.3.18.mlp.fc1.weight: lr_multiplier: 0.531441, wd_multiplier: 1.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.3.18.mlp.fc1.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.3.18.mlp.fc2.weight: lr_multiplier: 0.531441, wd_multiplier: 1.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.3.18.mlp.fc2.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.3.18.ls2.gamma: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.3.19.norm1.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.3.19.norm1.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.3.19.attn.qkv.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 1.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.3.19.attn.qkv.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.3.19.attn.proj.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 1.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.3.19.attn.proj.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.3.19.ls1.gamma: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.3.19.norm2.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.3.19.norm2.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.3.19.mlp.fc1.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 1.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.3.19.mlp.fc1.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.3.19.mlp.fc2.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 1.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.3.19.mlp.fc2.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.3.19.ls2.gamma: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.3.20.norm1.weight: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.3.20.norm1.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.3.20.attn.qkv.weight: lr_multiplier: 0.6561, wd_multiplier: 1.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.3.20.attn.qkv.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.3.20.attn.proj.weight: lr_multiplier: 0.6561, wd_multiplier: 1.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.3.20.attn.proj.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.3.20.ls1.gamma: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.3.20.norm2.weight: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.3.20.norm2.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.3.20.mlp.fc1.weight: lr_multiplier: 0.6561, wd_multiplier: 1.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.3.20.mlp.fc1.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.3.20.mlp.fc2.weight: lr_multiplier: 0.6561, wd_multiplier: 1.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.3.20.mlp.fc2.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.3.20.ls2.gamma: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.3.21.norm1.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.3.21.norm1.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.3.21.attn.qkv.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 1.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.3.21.attn.qkv.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.3.21.attn.proj.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 1.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.3.21.attn.proj.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.3.21.ls1.gamma: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.3.21.norm2.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.3.21.norm2.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.3.21.mlp.fc1.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 1.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.3.21.mlp.fc1.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.3.21.mlp.fc2.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 1.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.3.21.mlp.fc2.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.3.21.ls2.gamma: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.3.22.norm1.weight: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.3.22.norm1.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.3.22.attn.qkv.weight: lr_multiplier: 0.81, wd_multiplier: 1.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.3.22.attn.qkv.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.3.22.attn.proj.weight: lr_multiplier: 0.81, wd_multiplier: 1.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.3.22.attn.proj.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.3.22.ls1.gamma: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.3.22.norm2.weight: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.3.22.norm2.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.3.22.mlp.fc1.weight: lr_multiplier: 0.81, wd_multiplier: 1.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.3.22.mlp.fc1.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.3.22.mlp.fc2.weight: lr_multiplier: 0.81, wd_multiplier: 1.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.3.22.mlp.fc2.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.3.22.ls2.gamma: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.3.23.norm1.weight: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.3.23.norm1.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.3.23.attn.qkv.weight: lr_multiplier: 0.9, wd_multiplier: 1.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.3.23.attn.qkv.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.3.23.attn.proj.weight: lr_multiplier: 0.9, wd_multiplier: 1.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.3.23.attn.proj.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.3.23.ls1.gamma: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.3.23.norm2.weight: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.3.23.norm2.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.3.23.mlp.fc1.weight: lr_multiplier: 0.9, wd_multiplier: 1.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.3.23.mlp.fc1.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.3.23.mlp.fc2.weight: lr_multiplier: 0.9, wd_multiplier: 1.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.3.23.mlp.fc2.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] blocks.3.23.ls2.gamma: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] norm.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] norm.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 ssl_meta_arch.py:380] fusing param groups
I20240910 13:55:42 3356 dinov2 param_groups.py:64] else code branch
I20240910 13:55:42 3356 dinov2 param_groups.py:87] mlp.0.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] mlp.0.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] mlp.2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] mlp.2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] mlp.4.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] mlp.4.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] last_layer.weight_g: lr_multiplier: 1.0, wd_multiplier: 1.0
I20240910 13:55:42 3356 dinov2 param_groups.py:87] last_layer.weight_v: lr_multiplier: 1.0, wd_multiplier: 1.0
I20240910 13:55:42 3356 dinov2 ssl_meta_arch.py:380] fusing param groups
I20240910 13:55:42 3356 dinov2 train.py:100] Schedulers ready.
I20240910 13:55:42 3356 dinov2 augmentations.py:34] ###################################
I20240910 13:55:42 3356 dinov2 augmentations.py:35] Using data augmentation parameters:
I20240910 13:55:42 3356 dinov2 augmentations.py:36] global_crops_scale: [0.32, 1.0]
I20240910 13:55:42 3356 dinov2 augmentations.py:37] local_crops_scale: [0.05, 0.32]
I20240910 13:55:42 3356 dinov2 augmentations.py:38] local_crops_number: 8
I20240910 13:55:42 3356 dinov2 augmentations.py:39] global_crops_size: 224
I20240910 13:55:42 3356 dinov2 augmentations.py:40] local_crops_size: 96
I20240910 13:55:42 3356 dinov2 augmentations.py:41] ###################################
I20240910 13:55:42 3356 dinov2 loaders.py:89] using dataset: "ImageNet:split=TRAIN:root=C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\datasets\ILSVRC\Data\CLS-LOC:extra=C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\datasets\ILSVRC\Data\CLS-LOC"
I20240910 13:55:42 3356 dinov2 loaders.py:94] # of dataset samples: 1,281,167
I20240910 13:55:42 3356 dinov2 loaders.py:127] sampler: sharded infinite
I20240910 13:55:42 3356 dinov2 loaders.py:211] using PyTorch data loader
I20240910 13:55:42 3356 dinov2 loaders.py:226] infinite data loader
I20240910 13:55:42 3356 dinov2 train.py:218] Starting training from iteration 0
W20240910 13:55:56 3356 xformers __init__.py:50] A matching Triton is not available, some optimizations will not be enabled.
Error caught was: No module named 'triton'
I20240910 13:56:27 16808 dinov2 config.py:59] git:
  sha: d43546fae6805ceb22618af8cf78469b61d913c7, status: has uncommitted changes, branch: main

I20240910 13:56:27 16808 dinov2 config.py:60] config_file: configs/train/vitl16_short.yaml
eval: 
eval_only: False
no_resume: False
opts: ['train.dataset_path=ImageNet:split=TRAIN:root=C:\\Users\\ISI_UTS\\Siladittya\\iclr2023\\inexpts\\datasets\\ILSVRC\\Data\\CLS-LOC:extra=C:\\Users\\ISI_UTS\\Siladittya\\iclr2023\\inexpts\\datasets\\ILSVRC\\Data\\CLS-LOC', 'train.output_dir=C:\\Users\\ISI_UTS\\Siladittya\\iclr2023\\inexpts\\dinov2_nosetup\\dinov2\\dinov2_nosetup\\dinov2']
output_dir: C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\dinov2_nosetup\dinov2\dinov2_nosetup\dinov2
I20240910 13:56:27 16808 dinov2 config.py:26] sqrt scaling learning rate; base: 0.004, new: 0.001
I20240910 13:56:27 16808 dinov2 config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
dino:
  loss_weight: 1.0
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
  koleo_loss_weight: 0.1
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  separate_head: false
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
train:
  batch_size_per_gpu: 64
  dataset_path: ImageNet:split=TRAIN:root=C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\datasets\ILSVRC\Data\CLS-LOC:extra=C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\datasets\ILSVRC\Data\CLS-LOC
  output_dir: C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\dinov2_nosetup\dinov2\dinov2_nosetup\dinov2
  saveckp_freq: 20
  seed: 0
  num_workers: 0
  OFFICIAL_EPOCH_LENGTH: 1250
  cache_dataset: true
  centering: centering
student:
  arch: vit_small
  patch_size: 16
  drop_path_rate: 0.3
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: mlp
  block_chunks: 4
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
  num_register_tokens: 0
  interpolate_antialias: false
  interpolate_offset: 0.1
teacher:
  momentum_teacher: 0.992
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 30
optim:
  epochs: 100
  weight_decay: 0.04
  weight_decay_end: 0.4
  base_lr: 0.004
  lr: 0.001
  warmup_epochs: 10
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 0.9
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 8
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 224
  local_crops_size: 96
evaluation:
  eval_period_iterations: 12500

I20240910 13:56:27 16808 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20240910 13:56:27 16808 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20240910 13:56:27 16808 dinov2 ssl_meta_arch.py:45] OPTIONS -- architecture : embed_dim: 384
I20240910 13:56:27 16808 dinov2 ssl_meta_arch.py:60] OPTIONS -- DINO
I20240910 13:56:27 16808 dinov2 ssl_meta_arch.py:62] OPTIONS -- DINO -- loss_weight: 1.0
I20240910 13:56:27 16808 dinov2 ssl_meta_arch.py:63] OPTIONS -- DINO -- head_n_prototypes: 65536
I20240910 13:56:27 16808 dinov2 ssl_meta_arch.py:64] OPTIONS -- DINO -- head_bottleneck_dim: 256
I20240910 13:56:27 16808 dinov2 ssl_meta_arch.py:65] OPTIONS -- DINO -- head_hidden_dim: 2048
I20240910 13:56:27 16808 dinov2 ssl_meta_arch.py:77] OPTIONS -- DINO -- applying KOLEO regularization
I20240910 13:56:27 16808 dinov2 ssl_meta_arch.py:87] OPTIONS -- IBOT
I20240910 13:56:27 16808 dinov2 ssl_meta_arch.py:88] OPTIONS -- IBOT -- loss_weight: 1.0
I20240910 13:56:27 16808 dinov2 ssl_meta_arch.py:89] OPTIONS -- IBOT masking -- ibot_mask_ratio_tuple: [0.1, 0.5]
I20240910 13:56:27 16808 dinov2 ssl_meta_arch.py:90] OPTIONS -- IBOT masking -- ibot_mask_sample_probability: 0.5
I20240910 13:56:27 16808 dinov2 ssl_meta_arch.py:113] OPTIONS -- IBOT -- head shared with DINO
I20240910 13:56:27 16808 dinov2 ssl_meta_arch.py:123] Student and Teacher are built: they are both vit_small network.
I20240910 13:56:27 16808 dinov2 ssl_meta_arch.py:393] DISTRIBUTED FSDP -- preparing model for distributed training
W20240910 13:56:28 16808 py.warnings warnings.py:109] C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\lib\site-packages\torch\distributed\fsdp\_init_utils.py:295: UserWarning: FSDP is switching to use `NO_SHARD` instead of ShardingStrategy.SHARD_GRAD_OP since the world size is 1.
  warnings.warn(

I20240910 13:56:28 16808 dinov2 train.py:304] Model:
SSLMetaArch(
  (dino_loss): DINOLoss()
  (koleo_loss): KoLeoLoss(
    (pdist): PairwiseDistance()
  )
  (ibot_patch_loss): iBOTPatchLoss()
  (student): ModuleDict(
    (backbone): FullyShardedDataParallel(
      (_fsdp_wrapped_module): DinoVisionTransformer(
        (patch_embed): PatchEmbed(
          (proj): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))
          (norm): Identity()
        )
        (blocks): ModuleList(
          (0): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-2): 3 x NestedTensorBlock(
                (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=384, out_features=1152, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=384, out_features=384, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): DropPath()
                (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=384, out_features=1536, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=1536, out_features=384, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): DropPath()
              )
            )
          )
          (1): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-2): 3 x Identity()
              (3-5): 3 x NestedTensorBlock(
                (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=384, out_features=1152, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=384, out_features=384, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): DropPath()
                (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=384, out_features=1536, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=1536, out_features=384, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): DropPath()
              )
            )
          )
          (2): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-5): 6 x Identity()
              (6-8): 3 x NestedTensorBlock(
                (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=384, out_features=1152, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=384, out_features=384, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): DropPath()
                (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=384, out_features=1536, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=1536, out_features=384, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): DropPath()
              )
            )
          )
          (3): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-8): 9 x Identity()
              (9-11): 3 x NestedTensorBlock(
                (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=384, out_features=1152, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=384, out_features=384, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): DropPath()
                (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=384, out_features=1536, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=1536, out_features=384, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): DropPath()
              )
            )
          )
        )
        (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (head): Identity()
      )
    )
    (dino_head): FullyShardedDataParallel(
      (_fsdp_wrapped_module): DINOHead(
        (mlp): Sequential(
          (0): Linear(in_features=384, out_features=2048, bias=True)
          (1): GELU(approximate='none')
          (2): Linear(in_features=2048, out_features=2048, bias=True)
          (3): GELU(approximate='none')
          (4): Linear(in_features=2048, out_features=256, bias=True)
        )
        (last_layer): Linear(in_features=256, out_features=65536, bias=False)
      )
    )
  )
  (teacher): ModuleDict(
    (backbone): FullyShardedDataParallel(
      (_fsdp_wrapped_module): DinoVisionTransformer(
        (patch_embed): PatchEmbed(
          (proj): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))
          (norm): Identity()
        )
        (blocks): ModuleList(
          (0): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-2): 3 x NestedTensorBlock(
                (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=384, out_features=1152, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=384, out_features=384, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=384, out_features=1536, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=1536, out_features=384, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): Identity()
              )
            )
          )
          (1): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-2): 3 x Identity()
              (3-5): 3 x NestedTensorBlock(
                (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=384, out_features=1152, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=384, out_features=384, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=384, out_features=1536, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=1536, out_features=384, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): Identity()
              )
            )
          )
          (2): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-5): 6 x Identity()
              (6-8): 3 x NestedTensorBlock(
                (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=384, out_features=1152, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=384, out_features=384, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=384, out_features=1536, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=1536, out_features=384, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): Identity()
              )
            )
          )
          (3): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-8): 9 x Identity()
              (9-11): 3 x NestedTensorBlock(
                (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=384, out_features=1152, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=384, out_features=384, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=384, out_features=1536, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=1536, out_features=384, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): Identity()
              )
            )
          )
        )
        (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (head): Identity()
      )
    )
    (dino_head): FullyShardedDataParallel(
      (_fsdp_wrapped_module): DINOHead(
        (mlp): Sequential(
          (0): Linear(in_features=384, out_features=2048, bias=True)
          (1): GELU(approximate='none')
          (2): Linear(in_features=2048, out_features=2048, bias=True)
          (3): GELU(approximate='none')
          (4): Linear(in_features=2048, out_features=256, bias=True)
        )
        (last_layer): Linear(in_features=256, out_features=65536, bias=False)
      )
    )
  )
)
I20240910 13:56:28 16808 dinov2 param_groups.py:54] chunked fsdp
I20240910 13:56:28 16808 dinov2 param_groups.py:87] cls_token: lr_multiplier: 0.2541865828329001, wd_multiplier: 1.0
I20240910 13:56:28 16808 dinov2 param_groups.py:87] pos_embed: lr_multiplier: 0.2541865828329001, wd_multiplier: 1.0
I20240910 13:56:28 16808 dinov2 param_groups.py:87] mask_token: lr_multiplier: 0.2541865828329001, wd_multiplier: 1.0
I20240910 13:56:28 16808 dinov2 param_groups.py:87] patch_embed.proj.weight: lr_multiplier: 0.05083731656658002, wd_multiplier: 1.0
I20240910 13:56:28 16808 dinov2 param_groups.py:87] patch_embed.proj.bias: lr_multiplier: 0.05083731656658002, wd_multiplier: 0.0
I20240910 13:56:28 16808 dinov2 param_groups.py:87] blocks.0.0.norm1.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240910 13:56:28 16808 dinov2 param_groups.py:87] blocks.0.0.norm1.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240910 13:56:28 16808 dinov2 param_groups.py:87] blocks.0.0.attn.qkv.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 1.0
I20240910 13:56:28 16808 dinov2 param_groups.py:87] blocks.0.0.attn.qkv.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240910 13:56:28 16808 dinov2 param_groups.py:87] blocks.0.0.attn.proj.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 1.0
I20240910 13:56:28 16808 dinov2 param_groups.py:87] blocks.0.0.attn.proj.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240910 13:56:28 16808 dinov2 param_groups.py:87] blocks.0.0.ls1.gamma: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240910 13:56:28 16808 dinov2 param_groups.py:87] blocks.0.0.norm2.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240910 13:56:28 16808 dinov2 param_groups.py:87] blocks.0.0.norm2.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240910 13:56:28 16808 dinov2 param_groups.py:87] blocks.0.0.mlp.fc1.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 1.0
I20240910 13:56:28 16808 dinov2 param_groups.py:87] blocks.0.0.mlp.fc1.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240910 13:56:28 16808 dinov2 param_groups.py:87] blocks.0.0.mlp.fc2.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 1.0
I20240910 13:56:28 16808 dinov2 param_groups.py:87] blocks.0.0.mlp.fc2.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240910 13:56:28 16808 dinov2 param_groups.py:87] blocks.0.0.ls2.gamma: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240910 13:56:28 16808 dinov2 param_groups.py:87] blocks.0.1.norm1.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240910 13:56:28 16808 dinov2 param_groups.py:87] blocks.0.1.norm1.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240910 13:56:28 16808 dinov2 param_groups.py:87] blocks.0.1.attn.qkv.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 1.0
I20240910 13:56:28 16808 dinov2 param_groups.py:87] blocks.0.1.attn.qkv.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240910 13:56:28 16808 dinov2 param_groups.py:87] blocks.0.1.attn.proj.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 1.0
I20240910 13:56:28 16808 dinov2 param_groups.py:87] blocks.0.1.attn.proj.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240910 13:56:28 16808 dinov2 param_groups.py:87] blocks.0.1.ls1.gamma: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240910 13:56:28 16808 dinov2 param_groups.py:87] blocks.0.1.norm2.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240910 13:56:28 16808 dinov2 param_groups.py:87] blocks.0.1.norm2.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240910 13:56:28 16808 dinov2 param_groups.py:87] blocks.0.1.mlp.fc1.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 1.0
I20240910 13:56:28 16808 dinov2 param_groups.py:87] blocks.0.1.mlp.fc1.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240910 13:56:28 16808 dinov2 param_groups.py:87] blocks.0.1.mlp.fc2.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 1.0
I20240910 13:56:28 16808 dinov2 param_groups.py:87] blocks.0.1.mlp.fc2.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240910 13:56:28 16808 dinov2 param_groups.py:87] blocks.0.1.ls2.gamma: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240910 13:56:28 16808 dinov2 param_groups.py:87] blocks.0.2.norm1.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240910 13:56:28 16808 dinov2 param_groups.py:87] blocks.0.2.norm1.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240910 13:56:28 16808 dinov2 param_groups.py:87] blocks.0.2.attn.qkv.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 1.0
I20240910 13:56:28 16808 dinov2 param_groups.py:87] blocks.0.2.attn.qkv.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240910 13:56:28 16808 dinov2 param_groups.py:87] blocks.0.2.attn.proj.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 1.0
I20240910 13:56:28 16808 dinov2 param_groups.py:87] blocks.0.2.attn.proj.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240910 13:56:28 16808 dinov2 param_groups.py:87] blocks.0.2.ls1.gamma: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240910 13:56:28 16808 dinov2 param_groups.py:87] blocks.0.2.norm2.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240910 13:56:28 16808 dinov2 param_groups.py:87] blocks.0.2.norm2.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240910 13:56:28 16808 dinov2 param_groups.py:87] blocks.0.2.mlp.fc1.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 1.0
I20240910 13:56:28 16808 dinov2 param_groups.py:87] blocks.0.2.mlp.fc1.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240910 13:56:28 16808 dinov2 param_groups.py:87] blocks.0.2.mlp.fc2.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 1.0
I20240910 13:56:28 16808 dinov2 param_groups.py:87] blocks.0.2.mlp.fc2.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240910 13:56:28 16808 dinov2 param_groups.py:87] blocks.0.2.ls2.gamma: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240910 13:56:28 16808 dinov2 param_groups.py:87] blocks.1.3.norm1.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240910 13:56:28 16808 dinov2 param_groups.py:87] blocks.1.3.norm1.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240910 13:56:28 16808 dinov2 param_groups.py:87] blocks.1.3.attn.qkv.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 1.0
I20240910 13:56:28 16808 dinov2 param_groups.py:87] blocks.1.3.attn.qkv.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240910 13:56:28 16808 dinov2 param_groups.py:87] blocks.1.3.attn.proj.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 1.0
I20240910 13:56:28 16808 dinov2 param_groups.py:87] blocks.1.3.attn.proj.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240910 13:56:28 16808 dinov2 param_groups.py:87] blocks.1.3.ls1.gamma: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240910 13:56:28 16808 dinov2 param_groups.py:87] blocks.1.3.norm2.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240910 13:56:28 16808 dinov2 param_groups.py:87] blocks.1.3.norm2.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240910 13:56:28 16808 dinov2 param_groups.py:87] blocks.1.3.mlp.fc1.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 1.0
I20240910 13:56:28 16808 dinov2 param_groups.py:87] blocks.1.3.mlp.fc1.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240910 13:56:28 16808 dinov2 param_groups.py:87] blocks.1.3.mlp.fc2.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 1.0
I20240910 13:56:28 16808 dinov2 param_groups.py:87] blocks.1.3.mlp.fc2.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240910 13:56:28 16808 dinov2 param_groups.py:87] blocks.1.3.ls2.gamma: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240910 13:56:28 16808 dinov2 param_groups.py:87] blocks.1.4.norm1.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240910 13:56:28 16808 dinov2 param_groups.py:87] blocks.1.4.norm1.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240910 13:56:28 16808 dinov2 param_groups.py:87] blocks.1.4.attn.qkv.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 1.0
I20240910 13:56:28 16808 dinov2 param_groups.py:87] blocks.1.4.attn.qkv.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240910 13:56:28 16808 dinov2 param_groups.py:87] blocks.1.4.attn.proj.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 1.0
I20240910 13:56:28 16808 dinov2 param_groups.py:87] blocks.1.4.attn.proj.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240910 13:56:28 16808 dinov2 param_groups.py:87] blocks.1.4.ls1.gamma: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240910 13:56:28 16808 dinov2 param_groups.py:87] blocks.1.4.norm2.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240910 13:56:28 16808 dinov2 param_groups.py:87] blocks.1.4.norm2.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240910 13:56:28 16808 dinov2 param_groups.py:87] blocks.1.4.mlp.fc1.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 1.0
I20240910 13:56:28 16808 dinov2 param_groups.py:87] blocks.1.4.mlp.fc1.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240910 13:56:28 16808 dinov2 param_groups.py:87] blocks.1.4.mlp.fc2.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 1.0
I20240910 13:56:28 16808 dinov2 param_groups.py:87] blocks.1.4.mlp.fc2.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240910 13:56:28 16808 dinov2 param_groups.py:87] blocks.1.4.ls2.gamma: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240910 13:56:28 16808 dinov2 param_groups.py:87] blocks.1.5.norm1.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240910 13:56:28 16808 dinov2 param_groups.py:87] blocks.1.5.norm1.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240910 13:56:28 16808 dinov2 param_groups.py:87] blocks.1.5.attn.qkv.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 1.0
I20240910 13:56:28 16808 dinov2 param_groups.py:87] blocks.1.5.attn.qkv.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240910 13:56:28 16808 dinov2 param_groups.py:87] blocks.1.5.attn.proj.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 1.0
I20240910 13:56:28 16808 dinov2 param_groups.py:87] blocks.1.5.attn.proj.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240910 13:56:28 16808 dinov2 param_groups.py:87] blocks.1.5.ls1.gamma: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240910 13:56:28 16808 dinov2 param_groups.py:87] blocks.1.5.norm2.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240910 13:56:28 16808 dinov2 param_groups.py:87] blocks.1.5.norm2.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240910 13:56:28 16808 dinov2 param_groups.py:87] blocks.1.5.mlp.fc1.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 1.0
I20240910 13:56:28 16808 dinov2 param_groups.py:87] blocks.1.5.mlp.fc1.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240910 13:56:28 16808 dinov2 param_groups.py:87] blocks.1.5.mlp.fc2.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 1.0
I20240910 13:56:28 16808 dinov2 param_groups.py:87] blocks.1.5.mlp.fc2.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240910 13:56:28 16808 dinov2 param_groups.py:87] blocks.1.5.ls2.gamma: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240910 13:56:28 16808 dinov2 param_groups.py:87] blocks.2.6.norm1.weight: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240910 13:56:28 16808 dinov2 param_groups.py:87] blocks.2.6.norm1.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240910 13:56:28 16808 dinov2 param_groups.py:87] blocks.2.6.attn.qkv.weight: lr_multiplier: 0.531441, wd_multiplier: 1.0
I20240910 13:56:28 16808 dinov2 param_groups.py:87] blocks.2.6.attn.qkv.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240910 13:56:28 16808 dinov2 param_groups.py:87] blocks.2.6.attn.proj.weight: lr_multiplier: 0.531441, wd_multiplier: 1.0
I20240910 13:56:28 16808 dinov2 param_groups.py:87] blocks.2.6.attn.proj.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240910 13:56:28 16808 dinov2 param_groups.py:87] blocks.2.6.ls1.gamma: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240910 13:56:28 16808 dinov2 param_groups.py:87] blocks.2.6.norm2.weight: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240910 13:56:28 16808 dinov2 param_groups.py:87] blocks.2.6.norm2.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240910 13:56:28 16808 dinov2 param_groups.py:87] blocks.2.6.mlp.fc1.weight: lr_multiplier: 0.531441, wd_multiplier: 1.0
I20240910 13:56:28 16808 dinov2 param_groups.py:87] blocks.2.6.mlp.fc1.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240910 13:56:28 16808 dinov2 param_groups.py:87] blocks.2.6.mlp.fc2.weight: lr_multiplier: 0.531441, wd_multiplier: 1.0
I20240910 13:56:28 16808 dinov2 param_groups.py:87] blocks.2.6.mlp.fc2.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240910 13:56:28 16808 dinov2 param_groups.py:87] blocks.2.6.ls2.gamma: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240910 13:56:28 16808 dinov2 param_groups.py:87] blocks.2.7.norm1.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240910 13:56:28 16808 dinov2 param_groups.py:87] blocks.2.7.norm1.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240910 13:56:28 16808 dinov2 param_groups.py:87] blocks.2.7.attn.qkv.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 1.0
I20240910 13:56:28 16808 dinov2 param_groups.py:87] blocks.2.7.attn.qkv.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240910 13:56:28 16808 dinov2 param_groups.py:87] blocks.2.7.attn.proj.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 1.0
I20240910 13:56:28 16808 dinov2 param_groups.py:87] blocks.2.7.attn.proj.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240910 13:56:28 16808 dinov2 param_groups.py:87] blocks.2.7.ls1.gamma: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240910 13:56:28 16808 dinov2 param_groups.py:87] blocks.2.7.norm2.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240910 13:56:28 16808 dinov2 param_groups.py:87] blocks.2.7.norm2.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240910 13:56:28 16808 dinov2 param_groups.py:87] blocks.2.7.mlp.fc1.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 1.0
I20240910 13:56:28 16808 dinov2 param_groups.py:87] blocks.2.7.mlp.fc1.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240910 13:56:28 16808 dinov2 param_groups.py:87] blocks.2.7.mlp.fc2.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 1.0
I20240910 13:56:28 16808 dinov2 param_groups.py:87] blocks.2.7.mlp.fc2.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240910 13:56:28 16808 dinov2 param_groups.py:87] blocks.2.7.ls2.gamma: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240910 13:56:28 16808 dinov2 param_groups.py:87] blocks.2.8.norm1.weight: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240910 13:56:28 16808 dinov2 param_groups.py:87] blocks.2.8.norm1.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240910 13:56:28 16808 dinov2 param_groups.py:87] blocks.2.8.attn.qkv.weight: lr_multiplier: 0.6561, wd_multiplier: 1.0
I20240910 13:56:28 16808 dinov2 param_groups.py:87] blocks.2.8.attn.qkv.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240910 13:56:28 16808 dinov2 param_groups.py:87] blocks.2.8.attn.proj.weight: lr_multiplier: 0.6561, wd_multiplier: 1.0
I20240910 13:56:28 16808 dinov2 param_groups.py:87] blocks.2.8.attn.proj.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240910 13:56:28 16808 dinov2 param_groups.py:87] blocks.2.8.ls1.gamma: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240910 13:56:28 16808 dinov2 param_groups.py:87] blocks.2.8.norm2.weight: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240910 13:56:28 16808 dinov2 param_groups.py:87] blocks.2.8.norm2.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240910 13:56:28 16808 dinov2 param_groups.py:87] blocks.2.8.mlp.fc1.weight: lr_multiplier: 0.6561, wd_multiplier: 1.0
I20240910 13:56:28 16808 dinov2 param_groups.py:87] blocks.2.8.mlp.fc1.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240910 13:56:28 16808 dinov2 param_groups.py:87] blocks.2.8.mlp.fc2.weight: lr_multiplier: 0.6561, wd_multiplier: 1.0
I20240910 13:56:28 16808 dinov2 param_groups.py:87] blocks.2.8.mlp.fc2.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240910 13:56:28 16808 dinov2 param_groups.py:87] blocks.2.8.ls2.gamma: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240910 13:56:28 16808 dinov2 param_groups.py:87] blocks.3.9.norm1.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240910 13:56:28 16808 dinov2 param_groups.py:87] blocks.3.9.norm1.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240910 13:56:28 16808 dinov2 param_groups.py:87] blocks.3.9.attn.qkv.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 1.0
I20240910 13:56:28 16808 dinov2 param_groups.py:87] blocks.3.9.attn.qkv.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240910 13:56:28 16808 dinov2 param_groups.py:87] blocks.3.9.attn.proj.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 1.0
I20240910 13:56:28 16808 dinov2 param_groups.py:87] blocks.3.9.attn.proj.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240910 13:56:28 16808 dinov2 param_groups.py:87] blocks.3.9.ls1.gamma: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240910 13:56:28 16808 dinov2 param_groups.py:87] blocks.3.9.norm2.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240910 13:56:28 16808 dinov2 param_groups.py:87] blocks.3.9.norm2.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240910 13:56:28 16808 dinov2 param_groups.py:87] blocks.3.9.mlp.fc1.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 1.0
I20240910 13:56:28 16808 dinov2 param_groups.py:87] blocks.3.9.mlp.fc1.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240910 13:56:28 16808 dinov2 param_groups.py:87] blocks.3.9.mlp.fc2.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 1.0
I20240910 13:56:28 16808 dinov2 param_groups.py:87] blocks.3.9.mlp.fc2.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240910 13:56:28 16808 dinov2 param_groups.py:87] blocks.3.9.ls2.gamma: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240910 13:56:28 16808 dinov2 param_groups.py:87] blocks.3.10.norm1.weight: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240910 13:56:28 16808 dinov2 param_groups.py:87] blocks.3.10.norm1.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240910 13:56:28 16808 dinov2 param_groups.py:87] blocks.3.10.attn.qkv.weight: lr_multiplier: 0.81, wd_multiplier: 1.0
I20240910 13:56:28 16808 dinov2 param_groups.py:87] blocks.3.10.attn.qkv.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240910 13:56:28 16808 dinov2 param_groups.py:87] blocks.3.10.attn.proj.weight: lr_multiplier: 0.81, wd_multiplier: 1.0
I20240910 13:56:28 16808 dinov2 param_groups.py:87] blocks.3.10.attn.proj.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240910 13:56:28 16808 dinov2 param_groups.py:87] blocks.3.10.ls1.gamma: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240910 13:56:28 16808 dinov2 param_groups.py:87] blocks.3.10.norm2.weight: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240910 13:56:28 16808 dinov2 param_groups.py:87] blocks.3.10.norm2.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240910 13:56:28 16808 dinov2 param_groups.py:87] blocks.3.10.mlp.fc1.weight: lr_multiplier: 0.81, wd_multiplier: 1.0
I20240910 13:56:28 16808 dinov2 param_groups.py:87] blocks.3.10.mlp.fc1.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240910 13:56:28 16808 dinov2 param_groups.py:87] blocks.3.10.mlp.fc2.weight: lr_multiplier: 0.81, wd_multiplier: 1.0
I20240910 13:56:28 16808 dinov2 param_groups.py:87] blocks.3.10.mlp.fc2.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240910 13:56:28 16808 dinov2 param_groups.py:87] blocks.3.10.ls2.gamma: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240910 13:56:28 16808 dinov2 param_groups.py:87] blocks.3.11.norm1.weight: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240910 13:56:28 16808 dinov2 param_groups.py:87] blocks.3.11.norm1.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240910 13:56:28 16808 dinov2 param_groups.py:87] blocks.3.11.attn.qkv.weight: lr_multiplier: 0.9, wd_multiplier: 1.0
I20240910 13:56:28 16808 dinov2 param_groups.py:87] blocks.3.11.attn.qkv.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240910 13:56:28 16808 dinov2 param_groups.py:87] blocks.3.11.attn.proj.weight: lr_multiplier: 0.9, wd_multiplier: 1.0
I20240910 13:56:28 16808 dinov2 param_groups.py:87] blocks.3.11.attn.proj.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240910 13:56:28 16808 dinov2 param_groups.py:87] blocks.3.11.ls1.gamma: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240910 13:56:28 16808 dinov2 param_groups.py:87] blocks.3.11.norm2.weight: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240910 13:56:28 16808 dinov2 param_groups.py:87] blocks.3.11.norm2.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240910 13:56:28 16808 dinov2 param_groups.py:87] blocks.3.11.mlp.fc1.weight: lr_multiplier: 0.9, wd_multiplier: 1.0
I20240910 13:56:28 16808 dinov2 param_groups.py:87] blocks.3.11.mlp.fc1.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240910 13:56:28 16808 dinov2 param_groups.py:87] blocks.3.11.mlp.fc2.weight: lr_multiplier: 0.9, wd_multiplier: 1.0
I20240910 13:56:28 16808 dinov2 param_groups.py:87] blocks.3.11.mlp.fc2.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240910 13:56:28 16808 dinov2 param_groups.py:87] blocks.3.11.ls2.gamma: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240910 13:56:28 16808 dinov2 param_groups.py:87] norm.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20240910 13:56:28 16808 dinov2 param_groups.py:87] norm.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20240910 13:56:28 16808 dinov2 ssl_meta_arch.py:380] fusing param groups
I20240910 13:56:28 16808 dinov2 param_groups.py:64] else code branch
I20240910 13:56:28 16808 dinov2 param_groups.py:87] mlp.0.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20240910 13:56:28 16808 dinov2 param_groups.py:87] mlp.0.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20240910 13:56:28 16808 dinov2 param_groups.py:87] mlp.2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20240910 13:56:28 16808 dinov2 param_groups.py:87] mlp.2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20240910 13:56:28 16808 dinov2 param_groups.py:87] mlp.4.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20240910 13:56:28 16808 dinov2 param_groups.py:87] mlp.4.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20240910 13:56:28 16808 dinov2 param_groups.py:87] last_layer.weight_g: lr_multiplier: 1.0, wd_multiplier: 1.0
I20240910 13:56:28 16808 dinov2 param_groups.py:87] last_layer.weight_v: lr_multiplier: 1.0, wd_multiplier: 1.0
I20240910 13:56:28 16808 dinov2 ssl_meta_arch.py:380] fusing param groups
I20240910 13:56:28 16808 dinov2 train.py:100] Schedulers ready.
I20240910 13:56:28 16808 dinov2 augmentations.py:34] ###################################
I20240910 13:56:28 16808 dinov2 augmentations.py:35] Using data augmentation parameters:
I20240910 13:56:28 16808 dinov2 augmentations.py:36] global_crops_scale: [0.32, 1.0]
I20240910 13:56:28 16808 dinov2 augmentations.py:37] local_crops_scale: [0.05, 0.32]
I20240910 13:56:28 16808 dinov2 augmentations.py:38] local_crops_number: 8
I20240910 13:56:28 16808 dinov2 augmentations.py:39] global_crops_size: 224
I20240910 13:56:28 16808 dinov2 augmentations.py:40] local_crops_size: 96
I20240910 13:56:28 16808 dinov2 augmentations.py:41] ###################################
I20240910 13:56:28 16808 dinov2 loaders.py:89] using dataset: "ImageNet:split=TRAIN:root=C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\datasets\ILSVRC\Data\CLS-LOC:extra=C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\datasets\ILSVRC\Data\CLS-LOC"
I20240910 13:56:28 16808 dinov2 loaders.py:94] # of dataset samples: 1,281,167
I20240910 13:56:28 16808 dinov2 loaders.py:127] sampler: sharded infinite
I20240910 13:56:28 16808 dinov2 loaders.py:211] using PyTorch data loader
I20240910 13:56:28 16808 dinov2 loaders.py:226] infinite data loader
I20240910 13:56:28 16808 dinov2 train.py:218] Starting training from iteration 0
W20240910 13:56:40 16808 xformers __init__.py:50] A matching Triton is not available, some optimizations will not be enabled.
Error caught was: No module named 'triton'
I20240910 14:02:47 11804 dinov2 config.py:59] git:
  sha: d43546fae6805ceb22618af8cf78469b61d913c7, status: has uncommitted changes, branch: main

I20240910 14:02:47 11804 dinov2 config.py:60] config_file: configs/train/vitl16_short.yaml
eval: 
eval_only: False
no_resume: False
opts: ['train.dataset_path=ImageNet:split=TRAIN:root=C:\\Users\\ISI_UTS\\Siladittya\\iclr2023\\inexpts\\datasets\\ILSVRC\\Data\\CLS-LOC:extra=C:\\Users\\ISI_UTS\\Siladittya\\iclr2023\\inexpts\\datasets\\ILSVRC\\Data\\CLS-LOC', 'train.output_dir=C:\\Users\\ISI_UTS\\Siladittya\\iclr2023\\inexpts\\dinov2_nosetup\\dinov2\\dinov2_nosetup\\dinov2']
output_dir: C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\dinov2_nosetup\dinov2\dinov2_nosetup\dinov2
I20240910 14:02:47 11804 dinov2 config.py:26] sqrt scaling learning rate; base: 0.004, new: 0.001
I20240910 14:02:47 11804 dinov2 config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
dino:
  loss_weight: 1.0
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
  koleo_loss_weight: 0.1
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  separate_head: false
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
train:
  batch_size_per_gpu: 64
  dataset_path: ImageNet:split=TRAIN:root=C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\datasets\ILSVRC\Data\CLS-LOC:extra=C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\datasets\ILSVRC\Data\CLS-LOC
  output_dir: C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\dinov2_nosetup\dinov2\dinov2_nosetup\dinov2
  saveckp_freq: 20
  seed: 0
  num_workers: 0
  OFFICIAL_EPOCH_LENGTH: 1250
  cache_dataset: true
  centering: centering
student:
  arch: vit_small
  patch_size: 16
  drop_path_rate: 0.3
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: mlp
  block_chunks: 4
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
  num_register_tokens: 0
  interpolate_antialias: false
  interpolate_offset: 0.1
teacher:
  momentum_teacher: 0.992
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 30
optim:
  epochs: 100
  weight_decay: 0.04
  weight_decay_end: 0.4
  base_lr: 0.004
  lr: 0.001
  warmup_epochs: 10
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 0.9
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 8
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 224
  local_crops_size: 96
evaluation:
  eval_period_iterations: 12500

I20240910 14:02:47 11804 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20240910 14:02:48 11804 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20240910 14:02:48 11804 dinov2 ssl_meta_arch.py:45] OPTIONS -- architecture : embed_dim: 384
I20240910 14:02:48 11804 dinov2 ssl_meta_arch.py:60] OPTIONS -- DINO
I20240910 14:02:48 11804 dinov2 ssl_meta_arch.py:62] OPTIONS -- DINO -- loss_weight: 1.0
I20240910 14:02:48 11804 dinov2 ssl_meta_arch.py:63] OPTIONS -- DINO -- head_n_prototypes: 65536
I20240910 14:02:48 11804 dinov2 ssl_meta_arch.py:64] OPTIONS -- DINO -- head_bottleneck_dim: 256
I20240910 14:02:48 11804 dinov2 ssl_meta_arch.py:65] OPTIONS -- DINO -- head_hidden_dim: 2048
I20240910 14:02:48 11804 dinov2 ssl_meta_arch.py:77] OPTIONS -- DINO -- applying KOLEO regularization
I20240910 14:02:48 11804 dinov2 ssl_meta_arch.py:87] OPTIONS -- IBOT
I20240910 14:02:48 11804 dinov2 ssl_meta_arch.py:88] OPTIONS -- IBOT -- loss_weight: 1.0
I20240910 14:02:48 11804 dinov2 ssl_meta_arch.py:89] OPTIONS -- IBOT masking -- ibot_mask_ratio_tuple: [0.1, 0.5]
I20240910 14:02:48 11804 dinov2 ssl_meta_arch.py:90] OPTIONS -- IBOT masking -- ibot_mask_sample_probability: 0.5
I20240910 14:02:48 11804 dinov2 ssl_meta_arch.py:113] OPTIONS -- IBOT -- head shared with DINO
I20240910 14:02:48 11804 dinov2 ssl_meta_arch.py:123] Student and Teacher are built: they are both vit_small network.
I20240910 14:02:48 11804 dinov2 ssl_meta_arch.py:393] DISTRIBUTED FSDP -- preparing model for distributed training
W20240910 14:02:49 11804 py.warnings warnings.py:109] C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\lib\site-packages\torch\distributed\fsdp\_init_utils.py:295: UserWarning: FSDP is switching to use `NO_SHARD` instead of ShardingStrategy.SHARD_GRAD_OP since the world size is 1.
  warnings.warn(

I20240910 14:02:49 11804 dinov2 train.py:304] Model:
SSLMetaArch(
  (dino_loss): DINOLoss()
  (koleo_loss): KoLeoLoss(
    (pdist): PairwiseDistance()
  )
  (ibot_patch_loss): iBOTPatchLoss()
  (student): ModuleDict(
    (backbone): FullyShardedDataParallel(
      (_fsdp_wrapped_module): DinoVisionTransformer(
        (patch_embed): PatchEmbed(
          (proj): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))
          (norm): Identity()
        )
        (blocks): ModuleList(
          (0): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-2): 3 x NestedTensorBlock(
                (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=384, out_features=1152, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=384, out_features=384, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): DropPath()
                (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=384, out_features=1536, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=1536, out_features=384, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): DropPath()
              )
            )
          )
          (1): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-2): 3 x Identity()
              (3-5): 3 x NestedTensorBlock(
                (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=384, out_features=1152, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=384, out_features=384, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): DropPath()
                (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=384, out_features=1536, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=1536, out_features=384, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): DropPath()
              )
            )
          )
          (2): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-5): 6 x Identity()
              (6-8): 3 x NestedTensorBlock(
                (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=384, out_features=1152, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=384, out_features=384, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): DropPath()
                (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=384, out_features=1536, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=1536, out_features=384, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): DropPath()
              )
            )
          )
          (3): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-8): 9 x Identity()
              (9-11): 3 x NestedTensorBlock(
                (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=384, out_features=1152, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=384, out_features=384, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): DropPath()
                (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=384, out_features=1536, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=1536, out_features=384, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): DropPath()
              )
            )
          )
        )
        (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (head): Identity()
      )
    )
    (dino_head): FullyShardedDataParallel(
      (_fsdp_wrapped_module): DINOHead(
        (mlp): Sequential(
          (0): Linear(in_features=384, out_features=2048, bias=True)
          (1): GELU(approximate='none')
          (2): Linear(in_features=2048, out_features=2048, bias=True)
          (3): GELU(approximate='none')
          (4): Linear(in_features=2048, out_features=256, bias=True)
        )
        (last_layer): Linear(in_features=256, out_features=65536, bias=False)
      )
    )
  )
  (teacher): ModuleDict(
    (backbone): FullyShardedDataParallel(
      (_fsdp_wrapped_module): DinoVisionTransformer(
        (patch_embed): PatchEmbed(
          (proj): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))
          (norm): Identity()
        )
        (blocks): ModuleList(
          (0): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-2): 3 x NestedTensorBlock(
                (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=384, out_features=1152, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=384, out_features=384, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=384, out_features=1536, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=1536, out_features=384, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): Identity()
              )
            )
          )
          (1): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-2): 3 x Identity()
              (3-5): 3 x NestedTensorBlock(
                (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=384, out_features=1152, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=384, out_features=384, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=384, out_features=1536, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=1536, out_features=384, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): Identity()
              )
            )
          )
          (2): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-5): 6 x Identity()
              (6-8): 3 x NestedTensorBlock(
                (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=384, out_features=1152, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=384, out_features=384, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=384, out_features=1536, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=1536, out_features=384, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): Identity()
              )
            )
          )
          (3): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-8): 9 x Identity()
              (9-11): 3 x NestedTensorBlock(
                (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=384, out_features=1152, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=384, out_features=384, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=384, out_features=1536, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=1536, out_features=384, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): Identity()
              )
            )
          )
        )
        (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (head): Identity()
      )
    )
    (dino_head): FullyShardedDataParallel(
      (_fsdp_wrapped_module): DINOHead(
        (mlp): Sequential(
          (0): Linear(in_features=384, out_features=2048, bias=True)
          (1): GELU(approximate='none')
          (2): Linear(in_features=2048, out_features=2048, bias=True)
          (3): GELU(approximate='none')
          (4): Linear(in_features=2048, out_features=256, bias=True)
        )
        (last_layer): Linear(in_features=256, out_features=65536, bias=False)
      )
    )
  )
)
I20240910 14:02:49 11804 dinov2 param_groups.py:54] chunked fsdp
I20240910 14:02:49 11804 dinov2 param_groups.py:87] cls_token: lr_multiplier: 0.2541865828329001, wd_multiplier: 1.0
I20240910 14:02:49 11804 dinov2 param_groups.py:87] pos_embed: lr_multiplier: 0.2541865828329001, wd_multiplier: 1.0
I20240910 14:02:49 11804 dinov2 param_groups.py:87] mask_token: lr_multiplier: 0.2541865828329001, wd_multiplier: 1.0
I20240910 14:02:49 11804 dinov2 param_groups.py:87] patch_embed.proj.weight: lr_multiplier: 0.05083731656658002, wd_multiplier: 1.0
I20240910 14:02:49 11804 dinov2 param_groups.py:87] patch_embed.proj.bias: lr_multiplier: 0.05083731656658002, wd_multiplier: 0.0
I20240910 14:02:49 11804 dinov2 param_groups.py:87] blocks.0.0.norm1.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240910 14:02:49 11804 dinov2 param_groups.py:87] blocks.0.0.norm1.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240910 14:02:49 11804 dinov2 param_groups.py:87] blocks.0.0.attn.qkv.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 1.0
I20240910 14:02:49 11804 dinov2 param_groups.py:87] blocks.0.0.attn.qkv.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240910 14:02:49 11804 dinov2 param_groups.py:87] blocks.0.0.attn.proj.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 1.0
I20240910 14:02:49 11804 dinov2 param_groups.py:87] blocks.0.0.attn.proj.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240910 14:02:49 11804 dinov2 param_groups.py:87] blocks.0.0.ls1.gamma: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240910 14:02:49 11804 dinov2 param_groups.py:87] blocks.0.0.norm2.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240910 14:02:49 11804 dinov2 param_groups.py:87] blocks.0.0.norm2.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240910 14:02:49 11804 dinov2 param_groups.py:87] blocks.0.0.mlp.fc1.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 1.0
I20240910 14:02:49 11804 dinov2 param_groups.py:87] blocks.0.0.mlp.fc1.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240910 14:02:49 11804 dinov2 param_groups.py:87] blocks.0.0.mlp.fc2.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 1.0
I20240910 14:02:49 11804 dinov2 param_groups.py:87] blocks.0.0.mlp.fc2.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240910 14:02:49 11804 dinov2 param_groups.py:87] blocks.0.0.ls2.gamma: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20240910 14:02:49 11804 dinov2 param_groups.py:87] blocks.0.1.norm1.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240910 14:02:49 11804 dinov2 param_groups.py:87] blocks.0.1.norm1.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240910 14:02:49 11804 dinov2 param_groups.py:87] blocks.0.1.attn.qkv.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 1.0
I20240910 14:02:49 11804 dinov2 param_groups.py:87] blocks.0.1.attn.qkv.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240910 14:02:49 11804 dinov2 param_groups.py:87] blocks.0.1.attn.proj.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 1.0
I20240910 14:02:49 11804 dinov2 param_groups.py:87] blocks.0.1.attn.proj.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240910 14:02:49 11804 dinov2 param_groups.py:87] blocks.0.1.ls1.gamma: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240910 14:02:49 11804 dinov2 param_groups.py:87] blocks.0.1.norm2.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240910 14:02:49 11804 dinov2 param_groups.py:87] blocks.0.1.norm2.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240910 14:02:49 11804 dinov2 param_groups.py:87] blocks.0.1.mlp.fc1.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 1.0
I20240910 14:02:49 11804 dinov2 param_groups.py:87] blocks.0.1.mlp.fc1.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240910 14:02:49 11804 dinov2 param_groups.py:87] blocks.0.1.mlp.fc2.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 1.0
I20240910 14:02:49 11804 dinov2 param_groups.py:87] blocks.0.1.mlp.fc2.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240910 14:02:49 11804 dinov2 param_groups.py:87] blocks.0.1.ls2.gamma: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20240910 14:02:49 11804 dinov2 param_groups.py:87] blocks.0.2.norm1.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240910 14:02:49 11804 dinov2 param_groups.py:87] blocks.0.2.norm1.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240910 14:02:49 11804 dinov2 param_groups.py:87] blocks.0.2.attn.qkv.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 1.0
I20240910 14:02:49 11804 dinov2 param_groups.py:87] blocks.0.2.attn.qkv.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240910 14:02:49 11804 dinov2 param_groups.py:87] blocks.0.2.attn.proj.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 1.0
I20240910 14:02:49 11804 dinov2 param_groups.py:87] blocks.0.2.attn.proj.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240910 14:02:49 11804 dinov2 param_groups.py:87] blocks.0.2.ls1.gamma: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240910 14:02:49 11804 dinov2 param_groups.py:87] blocks.0.2.norm2.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240910 14:02:49 11804 dinov2 param_groups.py:87] blocks.0.2.norm2.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240910 14:02:49 11804 dinov2 param_groups.py:87] blocks.0.2.mlp.fc1.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 1.0
I20240910 14:02:49 11804 dinov2 param_groups.py:87] blocks.0.2.mlp.fc1.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240910 14:02:49 11804 dinov2 param_groups.py:87] blocks.0.2.mlp.fc2.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 1.0
I20240910 14:02:49 11804 dinov2 param_groups.py:87] blocks.0.2.mlp.fc2.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240910 14:02:49 11804 dinov2 param_groups.py:87] blocks.0.2.ls2.gamma: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20240910 14:02:49 11804 dinov2 param_groups.py:87] blocks.1.3.norm1.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240910 14:02:49 11804 dinov2 param_groups.py:87] blocks.1.3.norm1.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240910 14:02:49 11804 dinov2 param_groups.py:87] blocks.1.3.attn.qkv.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 1.0
I20240910 14:02:49 11804 dinov2 param_groups.py:87] blocks.1.3.attn.qkv.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240910 14:02:49 11804 dinov2 param_groups.py:87] blocks.1.3.attn.proj.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 1.0
I20240910 14:02:49 11804 dinov2 param_groups.py:87] blocks.1.3.attn.proj.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240910 14:02:49 11804 dinov2 param_groups.py:87] blocks.1.3.ls1.gamma: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240910 14:02:49 11804 dinov2 param_groups.py:87] blocks.1.3.norm2.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240910 14:02:49 11804 dinov2 param_groups.py:87] blocks.1.3.norm2.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240910 14:02:49 11804 dinov2 param_groups.py:87] blocks.1.3.mlp.fc1.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 1.0
I20240910 14:02:49 11804 dinov2 param_groups.py:87] blocks.1.3.mlp.fc1.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240910 14:02:49 11804 dinov2 param_groups.py:87] blocks.1.3.mlp.fc2.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 1.0
I20240910 14:02:49 11804 dinov2 param_groups.py:87] blocks.1.3.mlp.fc2.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240910 14:02:49 11804 dinov2 param_groups.py:87] blocks.1.3.ls2.gamma: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20240910 14:02:49 11804 dinov2 param_groups.py:87] blocks.1.4.norm1.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240910 14:02:49 11804 dinov2 param_groups.py:87] blocks.1.4.norm1.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240910 14:02:49 11804 dinov2 param_groups.py:87] blocks.1.4.attn.qkv.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 1.0
I20240910 14:02:49 11804 dinov2 param_groups.py:87] blocks.1.4.attn.qkv.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240910 14:02:49 11804 dinov2 param_groups.py:87] blocks.1.4.attn.proj.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 1.0
I20240910 14:02:49 11804 dinov2 param_groups.py:87] blocks.1.4.attn.proj.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240910 14:02:49 11804 dinov2 param_groups.py:87] blocks.1.4.ls1.gamma: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240910 14:02:49 11804 dinov2 param_groups.py:87] blocks.1.4.norm2.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240910 14:02:49 11804 dinov2 param_groups.py:87] blocks.1.4.norm2.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240910 14:02:49 11804 dinov2 param_groups.py:87] blocks.1.4.mlp.fc1.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 1.0
I20240910 14:02:49 11804 dinov2 param_groups.py:87] blocks.1.4.mlp.fc1.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240910 14:02:49 11804 dinov2 param_groups.py:87] blocks.1.4.mlp.fc2.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 1.0
I20240910 14:02:49 11804 dinov2 param_groups.py:87] blocks.1.4.mlp.fc2.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240910 14:02:49 11804 dinov2 param_groups.py:87] blocks.1.4.ls2.gamma: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20240910 14:02:49 11804 dinov2 param_groups.py:87] blocks.1.5.norm1.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240910 14:02:49 11804 dinov2 param_groups.py:87] blocks.1.5.norm1.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240910 14:02:49 11804 dinov2 param_groups.py:87] blocks.1.5.attn.qkv.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 1.0
I20240910 14:02:49 11804 dinov2 param_groups.py:87] blocks.1.5.attn.qkv.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240910 14:02:49 11804 dinov2 param_groups.py:87] blocks.1.5.attn.proj.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 1.0
I20240910 14:02:49 11804 dinov2 param_groups.py:87] blocks.1.5.attn.proj.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240910 14:02:49 11804 dinov2 param_groups.py:87] blocks.1.5.ls1.gamma: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240910 14:02:49 11804 dinov2 param_groups.py:87] blocks.1.5.norm2.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240910 14:02:49 11804 dinov2 param_groups.py:87] blocks.1.5.norm2.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240910 14:02:49 11804 dinov2 param_groups.py:87] blocks.1.5.mlp.fc1.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 1.0
I20240910 14:02:49 11804 dinov2 param_groups.py:87] blocks.1.5.mlp.fc1.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240910 14:02:49 11804 dinov2 param_groups.py:87] blocks.1.5.mlp.fc2.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 1.0
I20240910 14:02:49 11804 dinov2 param_groups.py:87] blocks.1.5.mlp.fc2.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240910 14:02:49 11804 dinov2 param_groups.py:87] blocks.1.5.ls2.gamma: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20240910 14:02:49 11804 dinov2 param_groups.py:87] blocks.2.6.norm1.weight: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240910 14:02:49 11804 dinov2 param_groups.py:87] blocks.2.6.norm1.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240910 14:02:49 11804 dinov2 param_groups.py:87] blocks.2.6.attn.qkv.weight: lr_multiplier: 0.531441, wd_multiplier: 1.0
I20240910 14:02:49 11804 dinov2 param_groups.py:87] blocks.2.6.attn.qkv.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240910 14:02:49 11804 dinov2 param_groups.py:87] blocks.2.6.attn.proj.weight: lr_multiplier: 0.531441, wd_multiplier: 1.0
I20240910 14:02:49 11804 dinov2 param_groups.py:87] blocks.2.6.attn.proj.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240910 14:02:49 11804 dinov2 param_groups.py:87] blocks.2.6.ls1.gamma: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240910 14:02:49 11804 dinov2 param_groups.py:87] blocks.2.6.norm2.weight: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240910 14:02:49 11804 dinov2 param_groups.py:87] blocks.2.6.norm2.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240910 14:02:49 11804 dinov2 param_groups.py:87] blocks.2.6.mlp.fc1.weight: lr_multiplier: 0.531441, wd_multiplier: 1.0
I20240910 14:02:49 11804 dinov2 param_groups.py:87] blocks.2.6.mlp.fc1.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240910 14:02:49 11804 dinov2 param_groups.py:87] blocks.2.6.mlp.fc2.weight: lr_multiplier: 0.531441, wd_multiplier: 1.0
I20240910 14:02:49 11804 dinov2 param_groups.py:87] blocks.2.6.mlp.fc2.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240910 14:02:49 11804 dinov2 param_groups.py:87] blocks.2.6.ls2.gamma: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20240910 14:02:49 11804 dinov2 param_groups.py:87] blocks.2.7.norm1.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240910 14:02:49 11804 dinov2 param_groups.py:87] blocks.2.7.norm1.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240910 14:02:49 11804 dinov2 param_groups.py:87] blocks.2.7.attn.qkv.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 1.0
I20240910 14:02:49 11804 dinov2 param_groups.py:87] blocks.2.7.attn.qkv.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240910 14:02:49 11804 dinov2 param_groups.py:87] blocks.2.7.attn.proj.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 1.0
I20240910 14:02:49 11804 dinov2 param_groups.py:87] blocks.2.7.attn.proj.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240910 14:02:49 11804 dinov2 param_groups.py:87] blocks.2.7.ls1.gamma: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240910 14:02:49 11804 dinov2 param_groups.py:87] blocks.2.7.norm2.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240910 14:02:49 11804 dinov2 param_groups.py:87] blocks.2.7.norm2.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240910 14:02:49 11804 dinov2 param_groups.py:87] blocks.2.7.mlp.fc1.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 1.0
I20240910 14:02:49 11804 dinov2 param_groups.py:87] blocks.2.7.mlp.fc1.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240910 14:02:49 11804 dinov2 param_groups.py:87] blocks.2.7.mlp.fc2.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 1.0
I20240910 14:02:49 11804 dinov2 param_groups.py:87] blocks.2.7.mlp.fc2.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240910 14:02:49 11804 dinov2 param_groups.py:87] blocks.2.7.ls2.gamma: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20240910 14:02:49 11804 dinov2 param_groups.py:87] blocks.2.8.norm1.weight: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240910 14:02:49 11804 dinov2 param_groups.py:87] blocks.2.8.norm1.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240910 14:02:49 11804 dinov2 param_groups.py:87] blocks.2.8.attn.qkv.weight: lr_multiplier: 0.6561, wd_multiplier: 1.0
I20240910 14:02:49 11804 dinov2 param_groups.py:87] blocks.2.8.attn.qkv.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240910 14:02:49 11804 dinov2 param_groups.py:87] blocks.2.8.attn.proj.weight: lr_multiplier: 0.6561, wd_multiplier: 1.0
I20240910 14:02:49 11804 dinov2 param_groups.py:87] blocks.2.8.attn.proj.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240910 14:02:49 11804 dinov2 param_groups.py:87] blocks.2.8.ls1.gamma: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240910 14:02:49 11804 dinov2 param_groups.py:87] blocks.2.8.norm2.weight: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240910 14:02:49 11804 dinov2 param_groups.py:87] blocks.2.8.norm2.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240910 14:02:49 11804 dinov2 param_groups.py:87] blocks.2.8.mlp.fc1.weight: lr_multiplier: 0.6561, wd_multiplier: 1.0
I20240910 14:02:49 11804 dinov2 param_groups.py:87] blocks.2.8.mlp.fc1.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240910 14:02:49 11804 dinov2 param_groups.py:87] blocks.2.8.mlp.fc2.weight: lr_multiplier: 0.6561, wd_multiplier: 1.0
I20240910 14:02:49 11804 dinov2 param_groups.py:87] blocks.2.8.mlp.fc2.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240910 14:02:49 11804 dinov2 param_groups.py:87] blocks.2.8.ls2.gamma: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20240910 14:02:49 11804 dinov2 param_groups.py:87] blocks.3.9.norm1.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240910 14:02:49 11804 dinov2 param_groups.py:87] blocks.3.9.norm1.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240910 14:02:49 11804 dinov2 param_groups.py:87] blocks.3.9.attn.qkv.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 1.0
I20240910 14:02:49 11804 dinov2 param_groups.py:87] blocks.3.9.attn.qkv.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240910 14:02:49 11804 dinov2 param_groups.py:87] blocks.3.9.attn.proj.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 1.0
I20240910 14:02:49 11804 dinov2 param_groups.py:87] blocks.3.9.attn.proj.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240910 14:02:49 11804 dinov2 param_groups.py:87] blocks.3.9.ls1.gamma: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240910 14:02:49 11804 dinov2 param_groups.py:87] blocks.3.9.norm2.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240910 14:02:49 11804 dinov2 param_groups.py:87] blocks.3.9.norm2.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240910 14:02:49 11804 dinov2 param_groups.py:87] blocks.3.9.mlp.fc1.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 1.0
I20240910 14:02:49 11804 dinov2 param_groups.py:87] blocks.3.9.mlp.fc1.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240910 14:02:49 11804 dinov2 param_groups.py:87] blocks.3.9.mlp.fc2.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 1.0
I20240910 14:02:49 11804 dinov2 param_groups.py:87] blocks.3.9.mlp.fc2.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240910 14:02:49 11804 dinov2 param_groups.py:87] blocks.3.9.ls2.gamma: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20240910 14:02:49 11804 dinov2 param_groups.py:87] blocks.3.10.norm1.weight: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240910 14:02:49 11804 dinov2 param_groups.py:87] blocks.3.10.norm1.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240910 14:02:49 11804 dinov2 param_groups.py:87] blocks.3.10.attn.qkv.weight: lr_multiplier: 0.81, wd_multiplier: 1.0
I20240910 14:02:49 11804 dinov2 param_groups.py:87] blocks.3.10.attn.qkv.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240910 14:02:49 11804 dinov2 param_groups.py:87] blocks.3.10.attn.proj.weight: lr_multiplier: 0.81, wd_multiplier: 1.0
I20240910 14:02:49 11804 dinov2 param_groups.py:87] blocks.3.10.attn.proj.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240910 14:02:49 11804 dinov2 param_groups.py:87] blocks.3.10.ls1.gamma: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240910 14:02:49 11804 dinov2 param_groups.py:87] blocks.3.10.norm2.weight: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240910 14:02:49 11804 dinov2 param_groups.py:87] blocks.3.10.norm2.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240910 14:02:49 11804 dinov2 param_groups.py:87] blocks.3.10.mlp.fc1.weight: lr_multiplier: 0.81, wd_multiplier: 1.0
I20240910 14:02:49 11804 dinov2 param_groups.py:87] blocks.3.10.mlp.fc1.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240910 14:02:49 11804 dinov2 param_groups.py:87] blocks.3.10.mlp.fc2.weight: lr_multiplier: 0.81, wd_multiplier: 1.0
I20240910 14:02:49 11804 dinov2 param_groups.py:87] blocks.3.10.mlp.fc2.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240910 14:02:49 11804 dinov2 param_groups.py:87] blocks.3.10.ls2.gamma: lr_multiplier: 0.81, wd_multiplier: 0.0
I20240910 14:02:49 11804 dinov2 param_groups.py:87] blocks.3.11.norm1.weight: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240910 14:02:49 11804 dinov2 param_groups.py:87] blocks.3.11.norm1.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240910 14:02:49 11804 dinov2 param_groups.py:87] blocks.3.11.attn.qkv.weight: lr_multiplier: 0.9, wd_multiplier: 1.0
I20240910 14:02:49 11804 dinov2 param_groups.py:87] blocks.3.11.attn.qkv.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240910 14:02:49 11804 dinov2 param_groups.py:87] blocks.3.11.attn.proj.weight: lr_multiplier: 0.9, wd_multiplier: 1.0
I20240910 14:02:49 11804 dinov2 param_groups.py:87] blocks.3.11.attn.proj.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240910 14:02:49 11804 dinov2 param_groups.py:87] blocks.3.11.ls1.gamma: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240910 14:02:49 11804 dinov2 param_groups.py:87] blocks.3.11.norm2.weight: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240910 14:02:49 11804 dinov2 param_groups.py:87] blocks.3.11.norm2.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240910 14:02:49 11804 dinov2 param_groups.py:87] blocks.3.11.mlp.fc1.weight: lr_multiplier: 0.9, wd_multiplier: 1.0
I20240910 14:02:49 11804 dinov2 param_groups.py:87] blocks.3.11.mlp.fc1.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240910 14:02:49 11804 dinov2 param_groups.py:87] blocks.3.11.mlp.fc2.weight: lr_multiplier: 0.9, wd_multiplier: 1.0
I20240910 14:02:49 11804 dinov2 param_groups.py:87] blocks.3.11.mlp.fc2.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240910 14:02:49 11804 dinov2 param_groups.py:87] blocks.3.11.ls2.gamma: lr_multiplier: 0.9, wd_multiplier: 0.0
I20240910 14:02:49 11804 dinov2 param_groups.py:87] norm.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20240910 14:02:49 11804 dinov2 param_groups.py:87] norm.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20240910 14:02:49 11804 dinov2 ssl_meta_arch.py:380] fusing param groups
I20240910 14:02:49 11804 dinov2 param_groups.py:64] else code branch
I20240910 14:02:49 11804 dinov2 param_groups.py:87] mlp.0.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20240910 14:02:49 11804 dinov2 param_groups.py:87] mlp.0.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20240910 14:02:49 11804 dinov2 param_groups.py:87] mlp.2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20240910 14:02:49 11804 dinov2 param_groups.py:87] mlp.2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20240910 14:02:49 11804 dinov2 param_groups.py:87] mlp.4.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20240910 14:02:49 11804 dinov2 param_groups.py:87] mlp.4.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20240910 14:02:49 11804 dinov2 param_groups.py:87] last_layer.weight_g: lr_multiplier: 1.0, wd_multiplier: 1.0
I20240910 14:02:49 11804 dinov2 param_groups.py:87] last_layer.weight_v: lr_multiplier: 1.0, wd_multiplier: 1.0
I20240910 14:02:49 11804 dinov2 ssl_meta_arch.py:380] fusing param groups
I20240910 14:02:49 11804 dinov2 train.py:100] Schedulers ready.
I20240910 14:02:49 11804 dinov2 augmentations.py:34] ###################################
I20240910 14:02:49 11804 dinov2 augmentations.py:35] Using data augmentation parameters:
I20240910 14:02:49 11804 dinov2 augmentations.py:36] global_crops_scale: [0.32, 1.0]
I20240910 14:02:49 11804 dinov2 augmentations.py:37] local_crops_scale: [0.05, 0.32]
I20240910 14:02:49 11804 dinov2 augmentations.py:38] local_crops_number: 8
I20240910 14:02:49 11804 dinov2 augmentations.py:39] global_crops_size: 224
I20240910 14:02:49 11804 dinov2 augmentations.py:40] local_crops_size: 96
I20240910 14:02:49 11804 dinov2 augmentations.py:41] ###################################
I20240910 14:02:49 11804 dinov2 loaders.py:89] using dataset: "ImageNet:split=TRAIN:root=C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\datasets\ILSVRC\Data\CLS-LOC:extra=C:\Users\ISI_UTS\Siladittya\iclr2023\inexpts\datasets\ILSVRC\Data\CLS-LOC"
I20240910 14:02:49 11804 dinov2 loaders.py:94] # of dataset samples: 1,281,167
I20240910 14:02:49 11804 dinov2 loaders.py:127] sampler: sharded infinite
I20240910 14:02:49 11804 dinov2 loaders.py:211] using PyTorch data loader
I20240910 14:02:49 11804 dinov2 loaders.py:226] infinite data loader
I20240910 14:02:49 11804 dinov2 train.py:218] Starting training from iteration 0
W20240910 14:03:01 11804 xformers __init__.py:50] A matching Triton is not available, some optimizations will not be enabled.
Error caught was: No module named 'triton'
I20240910 14:03:03 11804 dinov2 helpers.py:102] Training  [     0/125000]  eta: 20 days, 2:14:40  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.9323 (13.9323)  dino_local_crops_loss: 9.1854 (9.1854)  dino_global_crops_loss: 1.1482 (1.1482)  koleo_loss: 0.8076 (0.8076)  ibot_loss: 2.7911 (2.7911)  time: 13.888644  data: 10.472613  max mem: 9533
I20240910 14:03:34 11804 dinov2 helpers.py:102] Training  [    10/125000]  eta: 5 days, 22:21:13  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 14.3273 (14.2929)  dino_local_crops_loss: 9.5243 (9.4977)  dino_global_crops_loss: 1.1905 (1.1872)  koleo_loss: 0.8076 (0.8091)  ibot_loss: 2.7986 (2.7989)  time: 4.100118  data: 3.377401  max mem: 9882
I20240910 14:04:07 11804 dinov2 helpers.py:102] Training  [    20/125000]  eta: 5 days, 8:43:44  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 14.5608 (14.4787)  dino_local_crops_loss: 9.7298 (9.6673)  dino_global_crops_loss: 1.2162 (1.2084)  koleo_loss: 0.8062 (0.7995)  ibot_loss: 2.8047 (2.8034)  time: 3.198960  data: 2.734329  max mem: 9888
I20240910 14:04:39 11804 dinov2 helpers.py:102] Training  [    30/125000]  eta: 5 days, 3:02:41  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 14.7352 (14.5718)  dino_local_crops_loss: 9.9251 (9.7642)  dino_global_crops_loss: 1.2406 (1.2205)  koleo_loss: 0.7686 (0.7809)  ibot_loss: 2.8106 (2.8062)  time: 3.238977  data: 2.764535  max mem: 9888
I20240910 14:05:12 11804 dinov2 helpers.py:102] Training  [    40/125000]  eta: 5 days, 1:04:37  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 14.7622 (14.6160)  dino_local_crops_loss: 9.9914 (9.8227)  dino_global_crops_loss: 1.2489 (1.2278)  koleo_loss: 0.7134 (0.7576)  ibot_loss: 2.8125 (2.8079)  time: 3.257284  data: 2.776039  max mem: 9888
I20240910 14:05:45 11804 dinov2 helpers.py:102] Training  [    50/125000]  eta: 4 days, 23:48:27  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 14.7308 (14.6364)  dino_local_crops_loss: 10.0083 (9.8593)  dino_global_crops_loss: 1.2510 (1.2324)  koleo_loss: 0.6592 (0.7359)  ibot_loss: 2.8131 (2.8088)  time: 3.308160  data: 2.827197  max mem: 9888
I20240910 14:06:18 11804 dinov2 helpers.py:102] Training  [    60/125000]  eta: 4 days, 22:59:44  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 14.6867 (14.6411)  dino_local_crops_loss: 10.0056 (9.8822)  dino_global_crops_loss: 1.2507 (1.2353)  koleo_loss: 0.6221 (0.7141)  ibot_loss: 2.8130 (2.8095)  time: 3.306927  data: 2.833056  max mem: 9888
I20240910 14:06:52 11804 dinov2 helpers.py:102] Training  [    70/125000]  eta: 4 days, 22:29:44  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 14.6281 (14.6365)  dino_local_crops_loss: 9.9908 (9.8963)  dino_global_crops_loss: 1.2488 (1.2370)  koleo_loss: 0.5757 (0.6932)  ibot_loss: 2.8128 (2.8099)  time: 3.319585  data: 2.850691  max mem: 9888
I20240910 14:07:25 11804 dinov2 helpers.py:102] Training  [    80/125000]  eta: 4 days, 22:04:10  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 14.5744 (14.6255)  dino_local_crops_loss: 9.9717 (9.9046)  dino_global_crops_loss: 1.2465 (1.2381)  koleo_loss: 0.5439 (0.6726)  ibot_loss: 2.8126 (2.8103)  time: 3.322848  data: 2.852075  max mem: 9888
I20240910 14:07:57 11804 dinov2 helpers.py:102] Training  [    90/125000]  eta: 4 days, 21:30:37  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 14.5191 (14.6120)  dino_local_crops_loss: 9.9541 (9.9093)  dino_global_crops_loss: 1.2443 (1.2387)  koleo_loss: 0.5093 (0.6535)  ibot_loss: 2.8123 (2.8105)  time: 3.287887  data: 2.816022  max mem: 9888
I20240910 14:08:31 11804 dinov2 helpers.py:102] Training  [   100/125000]  eta: 4 days, 21:19:58  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 14.4756 (14.5959)  dino_local_crops_loss: 9.9396 (9.9118)  dino_global_crops_loss: 1.2425 (1.2390)  koleo_loss: 0.4819 (0.6345)  ibot_loss: 2.8118 (2.8106)  time: 3.298107  data: 2.825950  max mem: 9888
I20240910 14:09:04 11804 dinov2 helpers.py:102] Training  [   110/125000]  eta: 4 days, 21:03:53  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 14.4147 (14.5790)  dino_local_crops_loss: 9.9286 (9.9130)  dino_global_crops_loss: 1.2411 (1.2391)  koleo_loss: 0.4368 (0.6163)  ibot_loss: 2.8114 (2.8107)  time: 3.318449  data: 2.846318  max mem: 9888
I20240910 14:09:38 11804 dinov2 helpers.py:102] Training  [   120/125000]  eta: 4 days, 21:04:18  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 14.3812 (14.5619)  dino_local_crops_loss: 9.9201 (9.9133)  dino_global_crops_loss: 1.2400 (1.2392)  koleo_loss: 0.4175 (0.5989)  ibot_loss: 2.8103 (2.8106)  time: 3.339647  data: 2.874269  max mem: 9888
I20240910 14:10:11 11804 dinov2 helpers.py:102] Training  [   130/125000]  eta: 4 days, 20:57:29  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 14.3720 (14.5474)  dino_local_crops_loss: 9.9127 (9.9130)  dino_global_crops_loss: 1.2391 (1.2391)  koleo_loss: 0.4128 (0.5848)  ibot_loss: 2.8091 (2.8105)  time: 3.357865  data: 2.878595  max mem: 9888
I20240910 14:10:44 11804 dinov2 helpers.py:102] Training  [   140/125000]  eta: 4 days, 20:51:54  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 14.3509 (14.5314)  dino_local_crops_loss: 9.9056 (9.9122)  dino_global_crops_loss: 1.2382 (1.2390)  koleo_loss: 0.3992 (0.5698)  ibot_loss: 2.8088 (2.8103)  time: 3.336693  data: 2.858288  max mem: 9888
I20240910 14:11:17 11804 dinov2 helpers.py:102] Training  [   150/125000]  eta: 4 days, 20:44:38  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 14.3086 (14.5166)  dino_local_crops_loss: 9.8984 (9.9111)  dino_global_crops_loss: 1.2373 (1.2389)  koleo_loss: 0.3706 (0.5566)  ibot_loss: 2.8078 (2.8100)  time: 3.329339  data: 2.861685  max mem: 9888
I20240910 14:11:51 11804 dinov2 helpers.py:102] Training  [   160/125000]  eta: 4 days, 20:45:50  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 14.3005 (14.5020)  dino_local_crops_loss: 9.8904 (9.9096)  dino_global_crops_loss: 1.2363 (1.2387)  koleo_loss: 0.3608 (0.5440)  ibot_loss: 2.8053 (2.8097)  time: 3.350344  data: 2.876303  max mem: 9888
I20240910 14:12:24 11804 dinov2 helpers.py:102] Training  [   170/125000]  eta: 4 days, 20:39:32  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 14.2627 (14.4868)  dino_local_crops_loss: 9.8817 (9.9077)  dino_global_crops_loss: 1.2352 (1.2385)  koleo_loss: 0.3411 (0.5314)  ibot_loss: 2.8035 (2.8093)  time: 3.349891  data: 2.877546  max mem: 9888
I20240910 14:12:58 11804 dinov2 helpers.py:102] Training  [   180/125000]  eta: 4 days, 20:39:54  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 14.2264 (14.4715)  dino_local_crops_loss: 9.8722 (9.9055)  dino_global_crops_loss: 1.2340 (1.2382)  koleo_loss: 0.3215 (0.5191)  ibot_loss: 2.8004 (2.8087)  time: 3.346212  data: 2.855234  max mem: 9888
I20240910 14:13:31 11804 dinov2 helpers.py:102] Training  [   190/125000]  eta: 4 days, 20:30:48  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 14.1857 (14.4556)  dino_local_crops_loss: 9.8623 (9.9030)  dino_global_crops_loss: 1.2328 (1.2379)  koleo_loss: 0.2935 (0.5068)  ibot_loss: 2.7973 (2.8080)  time: 3.329466  data: 2.845593  max mem: 9888
I20240910 14:14:05 11804 dinov2 helpers.py:102] Training  [   200/125000]  eta: 4 days, 20:29:42  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 14.1408 (14.4388)  dino_local_crops_loss: 9.8520 (9.9002)  dino_global_crops_loss: 1.2314 (1.2375)  koleo_loss: 0.2676 (0.4942)  ibot_loss: 2.7921 (2.8070)  time: 3.320914  data: 2.842136  max mem: 9888
I20240910 14:14:38 11804 dinov2 helpers.py:102] Training  [   210/125000]  eta: 4 days, 20:27:26  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 14.0851 (14.4211)  dino_local_crops_loss: 9.8398 (9.8971)  dino_global_crops_loss: 1.2299 (1.2371)  koleo_loss: 0.2363 (0.4813)  ibot_loss: 2.7797 (2.8056)  time: 3.349251  data: 2.864518  max mem: 9888
I20240910 14:15:12 11804 dinov2 helpers.py:102] Training  [   220/125000]  eta: 4 days, 20:26:32  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 14.0361 (14.4021)  dino_local_crops_loss: 9.8273 (9.8935)  dino_global_crops_loss: 1.2284 (1.2367)  koleo_loss: 0.2058 (0.4680)  ibot_loss: 2.7715 (2.8038)  time: 3.349557  data: 2.884524  max mem: 9888
I20240910 14:15:45 11804 dinov2 helpers.py:102] Training  [   230/125000]  eta: 4 days, 20:27:22  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.9636 (14.3818)  dino_local_crops_loss: 9.8105 (9.8898)  dino_global_crops_loss: 1.2262 (1.2362)  koleo_loss: 0.1644 (0.4542)  ibot_loss: 2.7613 (2.8016)  time: 3.365451  data: 2.888649  max mem: 9888
I20240910 14:16:20 11804 dinov2 helpers.py:102] Training  [   240/125000]  eta: 4 days, 20:33:53  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.8995 (14.3610)  dino_local_crops_loss: 9.8031 (9.8861)  dino_global_crops_loss: 1.2252 (1.2357)  koleo_loss: 0.1267 (0.4398)  ibot_loss: 2.7481 (2.7993)  time: 3.408572  data: 2.918752  max mem: 9888
I20240910 14:16:53 11804 dinov2 helpers.py:102] Training  [   250/125000]  eta: 4 days, 20:31:08  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.8616 (14.3408)  dino_local_crops_loss: 9.8055 (9.8832)  dino_global_crops_loss: 1.2256 (1.2354)  koleo_loss: 0.0867 (0.4253)  ibot_loss: 2.7404 (2.7969)  time: 3.389664  data: 2.909662  max mem: 9888
I20240910 14:17:27 11804 dinov2 helpers.py:102] Training  [   260/125000]  eta: 4 days, 20:33:48  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.8559 (14.3223)  dino_local_crops_loss: 9.8187 (9.8812)  dino_global_crops_loss: 1.2271 (1.2351)  koleo_loss: 0.0626 (0.4112)  ibot_loss: 2.7437 (2.7948)  time: 3.370016  data: 2.906618  max mem: 9888
I20240910 14:18:01 11804 dinov2 helpers.py:102] Training  [   270/125000]  eta: 4 days, 20:35:50  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.8628 (14.3055)  dino_local_crops_loss: 9.8418 (9.8804)  dino_global_crops_loss: 1.2304 (1.2350)  koleo_loss: 0.0464 (0.3976)  ibot_loss: 2.7362 (2.7925)  time: 3.400380  data: 2.933061  max mem: 9888
I20240910 14:18:34 11804 dinov2 helpers.py:102] Training  [   280/125000]  eta: 4 days, 20:31:48  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.8701 (14.2905)  dino_local_crops_loss: 9.8668 (9.8804)  dino_global_crops_loss: 1.2328 (1.2350)  koleo_loss: 0.0378 (0.3847)  ibot_loss: 2.7325 (2.7904)  time: 3.358074  data: 2.878646  max mem: 9888
I20240910 14:19:08 11804 dinov2 helpers.py:102] Training  [   290/125000]  eta: 4 days, 20:30:24  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.8862 (14.2769)  dino_local_crops_loss: 9.8906 (9.8811)  dino_global_crops_loss: 1.2361 (1.2351)  koleo_loss: 0.0316 (0.3725)  ibot_loss: 2.7325 (2.7882)  time: 3.335123  data: 2.856588  max mem: 9888
I20240910 14:19:42 11804 dinov2 helpers.py:102] Training  [   300/125000]  eta: 4 days, 20:33:48  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.8968 (14.2645)  dino_local_crops_loss: 9.9057 (9.8822)  dino_global_crops_loss: 1.2386 (1.2352)  koleo_loss: 0.0281 (0.3609)  ibot_loss: 2.7230 (2.7862)  time: 3.386249  data: 2.894527  max mem: 9888
I20240910 14:20:15 11804 dinov2 helpers.py:102] Training  [   310/125000]  eta: 4 days, 20:28:45  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.9084 (14.2532)  dino_local_crops_loss: 9.9199 (9.8836)  dino_global_crops_loss: 1.2397 (1.2354)  koleo_loss: 0.0239 (0.3501)  ibot_loss: 2.7217 (2.7841)  time: 3.359285  data: 2.863524  max mem: 9888
I20240910 14:20:49 11804 dinov2 helpers.py:102] Training  [   320/125000]  eta: 4 days, 20:33:08  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.9068 (14.2425)  dino_local_crops_loss: 9.9290 (9.8851)  dino_global_crops_loss: 1.2409 (1.2356)  koleo_loss: 0.0226 (0.3399)  ibot_loss: 2.7176 (2.7820)  time: 3.368551  data: 2.886000  max mem: 9888
I20240910 14:21:24 11804 dinov2 helpers.py:102] Training  [   330/125000]  eta: 4 days, 20:35:57  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.9119 (14.2327)  dino_local_crops_loss: 9.9348 (9.8866)  dino_global_crops_loss: 1.2419 (1.2358)  koleo_loss: 0.0193 (0.3301)  ibot_loss: 2.7169 (2.7801)  time: 3.429169  data: 2.936870  max mem: 9888
I20240910 14:21:57 11804 dinov2 helpers.py:102] Training  [   340/125000]  eta: 4 days, 20:31:36  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.9122 (14.2233)  dino_local_crops_loss: 9.9379 (9.8882)  dino_global_crops_loss: 1.2423 (1.2360)  koleo_loss: 0.0169 (0.3209)  ibot_loss: 2.7133 (2.7781)  time: 3.362050  data: 2.879102  max mem: 9888
I20240910 14:22:30 11804 dinov2 helpers.py:102] Training  [   350/125000]  eta: 4 days, 20:30:14  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.9099 (14.2145)  dino_local_crops_loss: 9.9404 (9.8897)  dino_global_crops_loss: 1.2425 (1.2362)  koleo_loss: 0.0160 (0.3122)  ibot_loss: 2.7109 (2.7763)  time: 3.328093  data: 2.868841  max mem: 9888
I20240910 14:23:03 11804 dinov2 helpers.py:102] Training  [   360/125000]  eta: 4 days, 20:27:15  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.9099 (14.2061)  dino_local_crops_loss: 9.9423 (9.8912)  dino_global_crops_loss: 1.2428 (1.2364)  koleo_loss: 0.0146 (0.3040)  ibot_loss: 2.7106 (2.7745)  time: 3.336981  data: 2.868204  max mem: 9888
I20240910 14:23:37 11804 dinov2 helpers.py:102] Training  [   370/125000]  eta: 4 days, 20:28:52  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.9123 (14.1982)  dino_local_crops_loss: 9.9434 (9.8926)  dino_global_crops_loss: 1.2428 (1.2365)  koleo_loss: 0.0129 (0.2961)  ibot_loss: 2.7115 (2.7729)  time: 3.362522  data: 2.877512  max mem: 9888
I20240910 14:24:12 11804 dinov2 helpers.py:102] Training  [   380/125000]  eta: 4 days, 20:32:12  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.9148 (14.1905)  dino_local_crops_loss: 9.9435 (9.8939)  dino_global_crops_loss: 1.2430 (1.2367)  koleo_loss: 0.0117 (0.2886)  ibot_loss: 2.7179 (2.7712)  time: 3.419344  data: 2.933028  max mem: 9888
I20240910 14:24:46 11804 dinov2 helpers.py:102] Training  [   390/125000]  eta: 4 days, 20:32:27  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.9102 (14.1833)  dino_local_crops_loss: 9.9428 (9.8952)  dino_global_crops_loss: 1.2428 (1.2369)  koleo_loss: 0.0101 (0.2815)  ibot_loss: 2.7155 (2.7698)  time: 3.408960  data: 2.938613  max mem: 9888
I20240910 14:25:20 11804 dinov2 helpers.py:102] Training  [   400/125000]  eta: 4 days, 20:35:36  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.8964 (14.1761)  dino_local_crops_loss: 9.9391 (9.8962)  dino_global_crops_loss: 1.2425 (1.2370)  koleo_loss: 0.0091 (0.2747)  ibot_loss: 2.7078 (2.7682)  time: 3.410089  data: 2.934062  max mem: 9888
I20240910 14:25:54 11804 dinov2 helpers.py:102] Training  [   410/125000]  eta: 4 days, 20:35:26  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.8966 (14.1694)  dino_local_crops_loss: 9.9369 (9.8972)  dino_global_crops_loss: 1.2424 (1.2371)  koleo_loss: 0.0075 (0.2682)  ibot_loss: 2.7090 (2.7669)  time: 3.407524  data: 2.915780  max mem: 9888
I20240910 14:26:28 11804 dinov2 helpers.py:102] Training  [   420/125000]  eta: 4 days, 20:35:24  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.8966 (14.1628)  dino_local_crops_loss: 9.9336 (9.8980)  dino_global_crops_loss: 1.2419 (1.2372)  koleo_loss: 0.0071 (0.2620)  ibot_loss: 2.7146 (2.7656)  time: 3.378021  data: 2.888670  max mem: 9888
I20240910 14:27:01 11804 dinov2 helpers.py:102] Training  [   430/125000]  eta: 4 days, 20:34:25  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.8828 (14.1562)  dino_local_crops_loss: 9.9258 (9.8986)  dino_global_crops_loss: 1.2412 (1.2373)  koleo_loss: 0.0070 (0.2560)  ibot_loss: 2.7075 (2.7642)  time: 3.370044  data: 2.889354  max mem: 9888
I20240910 14:27:35 11804 dinov2 helpers.py:102] Training  [   440/125000]  eta: 4 days, 20:33:23  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.8752 (14.1496)  dino_local_crops_loss: 9.9144 (9.8988)  dino_global_crops_loss: 1.2398 (1.2374)  koleo_loss: 0.0070 (0.2504)  ibot_loss: 2.7065 (2.7630)  time: 3.359629  data: 2.895762  max mem: 9888
I20240910 14:28:08 11804 dinov2 helpers.py:102] Training  [   450/125000]  eta: 4 days, 20:31:25  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.8516 (14.1429)  dino_local_crops_loss: 9.9024 (9.8988)  dino_global_crops_loss: 1.2384 (1.2374)  koleo_loss: 0.0064 (0.2450)  ibot_loss: 2.7099 (2.7618)  time: 3.348503  data: 2.877107  max mem: 9888
I20240910 14:28:42 11804 dinov2 helpers.py:102] Training  [   460/125000]  eta: 4 days, 20:32:03  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.8362 (14.1359)  dino_local_crops_loss: 9.8856 (9.8982)  dino_global_crops_loss: 1.2359 (1.2373)  koleo_loss: 0.0063 (0.2398)  ibot_loss: 2.7074 (2.7606)  time: 3.366337  data: 2.875765  max mem: 9888
I20240910 14:29:16 11804 dinov2 helpers.py:102] Training  [   470/125000]  eta: 4 days, 20:31:03  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.8048 (14.1287)  dino_local_crops_loss: 9.8585 (9.8971)  dino_global_crops_loss: 1.2324 (1.2372)  koleo_loss: 0.0067 (0.2349)  ibot_loss: 2.7070 (2.7595)  time: 3.376580  data: 2.891963  max mem: 9888
I20240910 14:29:49 11804 dinov2 helpers.py:102] Training  [   480/125000]  eta: 4 days, 20:29:22  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.7802 (14.1212)  dino_local_crops_loss: 9.8335 (9.8956)  dino_global_crops_loss: 1.2306 (1.2370)  koleo_loss: 0.0074 (0.2301)  ibot_loss: 2.7065 (2.7584)  time: 3.350585  data: 2.876277  max mem: 9888
I20240910 14:30:23 11804 dinov2 helpers.py:102] Training  [   490/125000]  eta: 4 days, 20:27:32  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.7554 (14.1134)  dino_local_crops_loss: 9.8086 (9.8937)  dino_global_crops_loss: 1.2273 (1.2368)  koleo_loss: 0.0065 (0.2256)  ibot_loss: 2.7071 (2.7574)  time: 3.339923  data: 2.862938  max mem: 9888
I20240910 14:30:58 11804 dinov2 helpers.py:102] Training  [   500/125000]  eta: 4 days, 20:34:49  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.7304 (14.1056)  dino_local_crops_loss: 9.7968 (9.8915)  dino_global_crops_loss: 1.2264 (1.2365)  koleo_loss: 0.0058 (0.2212)  ibot_loss: 2.7056 (2.7563)  time: 3.447126  data: 2.964322  max mem: 9888
I20240910 14:31:33 11804 dinov2 helpers.py:102] Training  [   510/125000]  eta: 4 days, 20:40:27  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.7099 (14.0978)  dino_local_crops_loss: 9.7817 (9.8892)  dino_global_crops_loss: 1.2216 (1.2362)  koleo_loss: 0.0058 (0.2170)  ibot_loss: 2.7050 (2.7554)  time: 3.540122  data: 3.064506  max mem: 9888
I20240910 14:32:08 11804 dinov2 helpers.py:102] Training  [   520/125000]  eta: 4 days, 20:42:24  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.7099 (14.0905)  dino_local_crops_loss: 9.7795 (9.8872)  dino_global_crops_loss: 1.2225 (1.2360)  koleo_loss: 0.0047 (0.2129)  ibot_loss: 2.7057 (2.7544)  time: 3.480400  data: 3.011463  max mem: 9888
I20240910 14:32:42 11804 dinov2 helpers.py:102] Training  [   530/125000]  eta: 4 days, 20:45:00  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.7110 (14.0831)  dino_local_crops_loss: 9.7772 (9.8850)  dino_global_crops_loss: 1.2227 (1.2357)  koleo_loss: 0.0039 (0.2090)  ibot_loss: 2.7045 (2.7535)  time: 3.446727  data: 2.963047  max mem: 9888
I20240910 14:33:17 11804 dinov2 helpers.py:102] Training  [   540/125000]  eta: 4 days, 20:49:16  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.7029 (14.0759)  dino_local_crops_loss: 9.7638 (9.8828)  dino_global_crops_loss: 1.2210 (1.2355)  koleo_loss: 0.0046 (0.2052)  ibot_loss: 2.6988 (2.7525)  time: 3.479369  data: 2.990117  max mem: 9888
I20240910 14:33:52 11804 dinov2 helpers.py:102] Training  [   550/125000]  eta: 4 days, 20:52:10  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.6906 (14.0688)  dino_local_crops_loss: 9.7645 (9.8806)  dino_global_crops_loss: 1.2206 (1.2352)  koleo_loss: 0.0052 (0.2016)  ibot_loss: 2.6969 (2.7514)  time: 3.486912  data: 3.003607  max mem: 9888
I20240910 14:34:26 11804 dinov2 helpers.py:102] Training  [   560/125000]  eta: 4 days, 20:53:08  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.6638 (14.0614)  dino_local_crops_loss: 9.7436 (9.8781)  dino_global_crops_loss: 1.2168 (1.2348)  koleo_loss: 0.0048 (0.1981)  ibot_loss: 2.6976 (2.7505)  time: 3.446613  data: 2.977245  max mem: 9888
I20240910 14:35:01 11804 dinov2 helpers.py:102] Training  [   570/125000]  eta: 4 days, 20:56:17  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.6449 (14.0539)  dino_local_crops_loss: 9.7362 (9.8753)  dino_global_crops_loss: 1.2154 (1.2345)  koleo_loss: 0.0043 (0.1947)  ibot_loss: 2.6972 (2.7495)  time: 3.452918  data: 2.981375  max mem: 9888
I20240910 14:35:34 11804 dinov2 helpers.py:102] Training  [   580/125000]  eta: 4 days, 20:54:01  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.6243 (14.0460)  dino_local_crops_loss: 9.7125 (9.8722)  dino_global_crops_loss: 1.2151 (1.2341)  koleo_loss: 0.0043 (0.1914)  ibot_loss: 2.6911 (2.7483)  time: 3.409496  data: 2.930595  max mem: 9888
I20240910 14:36:08 11804 dinov2 helpers.py:102] Training  [   590/125000]  eta: 4 days, 20:52:41  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.5954 (14.0385)  dino_local_crops_loss: 9.7073 (9.8694)  dino_global_crops_loss: 1.2146 (1.2337)  koleo_loss: 0.0046 (0.1882)  ibot_loss: 2.6774 (2.7471)  time: 3.347963  data: 2.896733  max mem: 9888
I20240910 14:36:43 11804 dinov2 helpers.py:102] Training  [   600/125000]  eta: 4 days, 20:56:52  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.5767 (14.0302)  dino_local_crops_loss: 9.6923 (9.8664)  dino_global_crops_loss: 1.2106 (1.2333)  koleo_loss: 0.0039 (0.1852)  ibot_loss: 2.6566 (2.7453)  time: 3.440119  data: 2.981791  max mem: 9888
I20240910 14:37:19 11804 dinov2 helpers.py:102] Training  [   610/125000]  eta: 4 days, 21:02:09  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.5109 (14.0208)  dino_local_crops_loss: 9.6782 (9.8632)  dino_global_crops_loss: 1.2075 (1.2330)  koleo_loss: 0.0039 (0.1822)  ibot_loss: 2.6257 (2.7424)  time: 3.538128  data: 3.058757  max mem: 9888
I20240910 14:37:52 11804 dinov2 helpers.py:102] Training  [   620/125000]  eta: 4 days, 21:00:04  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.4193 (14.0105)  dino_local_crops_loss: 9.6714 (9.8603)  dino_global_crops_loss: 1.2072 (1.2326)  koleo_loss: 0.0035 (0.1793)  ibot_loss: 2.5372 (2.7383)  time: 3.449203  data: 2.979331  max mem: 9888
I20240910 14:38:26 11804 dinov2 helpers.py:102] Training  [   630/125000]  eta: 4 days, 20:58:20  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9920 (0.9920)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 64.0000 (64.0000)  total_loss: 13.3061 (13.9983)  dino_local_crops_loss: 9.6589 (9.8570)  dino_global_crops_loss: 1.2045 (1.2321)  koleo_loss: 0.0034 (0.1766)  ibot_loss: 2.4601 (2.7327)  time: 3.346311  data: 2.869817  max mem: 9888
